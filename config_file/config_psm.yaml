defaults:
- config_psm_schema
- backbone_config: graphormer
# - schedule: polynomial #'cosine', 'reduce_on_plateau','polynomial'
# - data_module: default
- _self_



backbone: graphormer
##########################
# DistributedTrainConfig
local_rank:  -1
world_size: 1
node_rank: 0
rank: 0
pipeline_model_parallel_size: 0
tensor_model_parallel_size: 1
deepspeed_config: ""
dist_backend: "nccl"
# TrainerConfig
seed: 46
fp16: False
auto_cast: False
bf16: False
grad_scaler_init: 1.0
gradient_accumulation_steps: 1
max_tokens: 2048
train_batch_size: 1
val_batch_size: 1
val_batch_interval: 0
val_batch_log_interval: 1000
val_epoch_interval: 1
save_dir: "./checkpoints"
save_batch_interval: 0
save_epoch_interval: 1
log_interval: 10000
strategy: "Single"
pp_partition_layer_name: ""
pp_part_list: null
cpu: False
ifresume: False
load_ckpt: False
freeze_param_list: ""
unfreeze_param_list: ""
finetune_from_checkpoint_dir: null
finetune_from_checkpoint_id: null
# dataloader strategy
dynamic_loader: False
ifstack: False
gradient_clipping: 1.0
total_num_steps: 1000
warmup_num_steps: 60
warmup_factor: 0.06
warmup_lr: 1e-6
warmup_num_epochs: 10
max_lr: 0.0001
init_lr: 8e-5
min_lr: 8e-6
weight_decay: 0.0
total_num_epochs: 100
# wandb
wandb: False
wandb_team: ""
wandb_group: ""
wandb_project: ""
wandb_run_name: ""
# adam
beta1: 0.9
beta2: 0.999
eps: 1e-8
# early stopping
early_stopping: False
early_stopping_patience: 10
early_stopping_metric: "valid_loss"
early_stopping_mode: "min"
# compile CUDA kernels with torch.compile
compile: False
# validate
calculate_metrics: False
# offload parameters to CPU/NVMe if Zero optimizer is used
zero_offload: False
zero_offload_dir: "./"
# profiler
profiling: False
prof_dir: "./prof"
ptensorboard: False
# debugger
debug: False
############################
# PSMConfig
model_type: "psm"
seq_masking_method: "transformerM"
add_rope: True
num_residues: 32
max_num_aa: 1024
encoder_pair_embed_dim: 64
task: "mae"
sample_mode: False
train_data_path: ""
valid_data_path: ""
data_path: ""
data_path_list: ""
dataset_name_list: ""
dataset_split_raito: ""
lamb_pde: 0.01
# for PBC
pbc_expanded_token_cutoff: 512
pbc_expanded_num_cell_per_direction: 5
pbc_expanded_distance_cutoff: 20.0
pbc_use_local_attention: True
pbc_multigraph_cutoff: 5.0
diff_init_lattice_size: 10.0
diff_init_lattice_size_factor: 2.859496852322873
# for diffusion
diffusion_sampling: "ode"
diffusion_noise_std: 1.0
ddim_eta: 0.0
ddim_steps: 50
clean_sample_ratio: 0.5
diffusion_training_loss: "L1"
diffusion_time_step_encoder_type: "POSITIONAL"
# for equivariant part
equivar_vec_init: "ZERO_CENTERED_POS"
# for 2D information
use_2d_atom_features: False
use_2d_bond_features: False
num_classes: 1
encoder_attention_heads: 32
encoder_ffn_embed_dim: 768
encoder_embed_dim: 768
encoder_layers: 24
num_pred_attn_layer: 4
num_3d_bias_kernel: 128
max_length: 1024
multi_hop_max_dist: 5
droppath_prob: 0.0
act_dropout: 0.0
attn_dropout: 0.0
dropout: 0.0
sandwich_ln: True
noise_scale: 0.2
mask_ratio: 0.5
d_tilde: 1.0
pbc_cutoff: 40.0
dataset_names: ""
loadcheck_path: ""
add_3d: False
no_2d: False
ft: False
infer: False
use_pbc: False
# TM and ddpm params
transformer_m_pretrain: True
mode_prob: "0.6,0.2,0.2"
num_timesteps: 1000
ddpm_beta_start: 0.0001
ddpm_beta_end: 0.02
ddpm_schedule: "linear"
noise_mode: "const"
num_edges: 1536
num_atom_features: 5120
###########################################################################################
####### THESE are from graphormer_base_architecture, is confict, follow upper default vaule.
attention_dropout: 0.0
share_encoder_input_output_embed: False
encoder_learned_pos: False
no_token_positional_embeddings: False
num_segments: 2
sentence_class_num: 2
sent_loss: False
apply_bert_init: False
activation_fn: "relu"
pooler_activation_fn: "tanh"
encoder_normalize_before: False
atom_loss_coeff: 1.0
pos_loss_coeff: 1.0
y_2d_loss_coeff: 1.0
max_positions: 512
num_atoms: 4608
num_in_degree: 512
num_out_degree: 512
num_spatial: 512
num_edge_dis: 128
edge_type: "multi_hop"
layerdrop: 0.0
apply_graphormer_init: True
pre_layernorm: False
