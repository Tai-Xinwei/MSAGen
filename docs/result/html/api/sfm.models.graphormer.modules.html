<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>sfm.models.graphormer.modules package &mdash; A4SFramework ai4science documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->

        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=c1325062"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="sfm.models.llama2 package" href="sfm.models.llama2.html" />
    <link rel="prev" title="sfm.models.graphormer package" href="sfm.models.graphormer.html" />
</head>

<body class="wy-body-for-nav">
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >



          <a href="../index.html" class="icon icon-home">
            A4SFramework
          </a>
              <div class="version">
                0.0.5
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../readme.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../userfriendly.html">User-friendly Interface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed.html">Large Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modelzoo.html">Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datapipeline.html">Data Pipeline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../trainargs.html">Training Args</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="modules.html">sfm</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="sfm.html">sfm package</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="sfm.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="sfm.criterions.html">sfm.criterions package</a></li>
<li class="toctree-l4"><a class="reference internal" href="sfm.data.html">sfm.data package</a></li>
<li class="toctree-l4"><a class="reference internal" href="sfm.logging.html">sfm.logging package</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="sfm.models.html">sfm.models package</a></li>
<li class="toctree-l4"><a class="reference internal" href="sfm.modules.html">sfm.modules package</a></li>
<li class="toctree-l4"><a class="reference internal" href="sfm.pipeline.html">sfm.pipeline package</a></li>
<li class="toctree-l4"><a class="reference internal" href="sfm.tasks.html">sfm.tasks package</a></li>
<li class="toctree-l4"><a class="reference internal" href="sfm.utils.html">sfm.utils package</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="sfm.html#module-sfm">Module contents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">A4SFramework</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="modules.html">sfm</a></li>
          <li class="breadcrumb-item"><a href="sfm.html">sfm package</a></li>
          <li class="breadcrumb-item"><a href="sfm.models.html">sfm.models package</a></li>
          <li class="breadcrumb-item"><a href="sfm.models.graphormer.html">sfm.models.graphormer package</a></li>
      <li class="breadcrumb-item active">sfm.models.graphormer.modules package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/api/sfm.models.graphormer.modules.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">

  <section id="sfm-models-graphormer-modules-package">
<h1>sfm.models.graphormer.modules package<a class="headerlink" href="#sfm-models-graphormer-modules-package" title="Link to this heading"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading"></a></h2>
</section>
<section id="module-sfm.models.graphormer.modules.UnifiedDecoder">
<span id="sfm-models-graphormer-modules-unifieddecoder-module"></span><h2>sfm.models.graphormer.modules.UnifiedDecoder module<a class="headerlink" href="#module-sfm.models.graphormer.modules.UnifiedDecoder" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EncoderLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">EncoderLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ffn_embedding_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eQi_choice</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'original'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gbf_args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EncoderLayer" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EncoderLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vec</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias_iself</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias_i2e</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias_eself</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias_e2i</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_unit</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gbf_args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EncoderLayer.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EncoderLayer.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EncoderLayer.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EncoderLayer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EncoderLayer.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.Equivariant2InvariantAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">Equivariant2InvariantAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eQi_choice</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gbf_args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.Equivariant2InvariantAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.Equivariant2InvariantAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vec</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_unit</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gbf_args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.Equivariant2InvariantAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.Equivariant2InvariantAttention.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.Equivariant2InvariantAttention.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.Equivariant2InvariantAttention.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.Equivariant2InvariantAttention.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">EquivariantAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key_padding_mask</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantAttention.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantAttention.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantAttention.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantAttention.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">EquivariantLayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">normalized_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">elementwise_linear</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Rotationally-equivariant Vector Layer Normalization
Expects inputs with shape (N, n, d), where N is batch size, n is vector dimension, d is width/number of vectors.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.covariance">
<span class="sig-name descname"><span class="pre">covariance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.covariance" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.elementwise_linear">
<span class="sig-name descname"><span class="pre">elementwise_linear</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.elementwise_linear" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.eps">
<span class="sig-name descname"><span class="pre">eps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.eps" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.extra_repr" title="Link to this definition"></a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.mean_center">
<span class="sig-name descname"><span class="pre">mean_center</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.mean_center" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.normalized_shape">
<span class="sig-name descname"><span class="pre">normalized_shape</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.normalized_shape" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.symsqrtinv">
<span class="sig-name descname"><span class="pre">symsqrtinv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">matrix</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm.symsqrtinv" title="Link to this definition"></a></dt>
<dd><p>Compute the inverse square root of a positive definite matrix.
Based on <a class="reference external" href="https://github.com/pytorch/pytorch/issues/25481">https://github.com/pytorch/pytorch/issues/25481</a></p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantSelfAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">EquivariantSelfAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantSelfAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantSelfAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">vec</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantSelfAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantSelfAttention.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantSelfAttention.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantSelfAttention.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantSelfAttention.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantVectorOutput">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">EquivariantVectorOutput</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">768</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'silu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantVectorOutput" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantVectorOutput.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantVectorOutput.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantVectorOutput.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantVectorOutput.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.EquivariantVectorOutput.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.EquivariantVectorOutput.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.GatedEquivariantBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">GatedEquivariantBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intermediate_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'silu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scalar_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.GatedEquivariantBlock" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Gated Equivariant Block as defined in Schütt et al. (2021):
Equivariant message passing for the prediction of tensorial properties and molecular spectra</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.GatedEquivariantBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.GatedEquivariantBlock.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.GatedEquivariantBlock.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.GatedEquivariantBlock.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.GaussianLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">GaussianLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">K</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_types</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.GaussianLayer" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.GaussianLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_types</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.GaussianLayer.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.GaussianLayer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.GaussianLayer.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.Invariant2EquivariantAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">Invariant2EquivariantAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.Invariant2EquivariantAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.Invariant2EquivariantAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vec</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.Invariant2EquivariantAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.Invariant2EquivariantAttention.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.Invariant2EquivariantAttention.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.Invariant2EquivariantAttention.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.Invariant2EquivariantAttention.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.InvariantAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">InvariantAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.InvariantAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.InvariantAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">key_padding_mask</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.InvariantAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.InvariantAttention.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.InvariantAttention.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.InvariantAttention.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.InvariantAttention.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.InvariantSelfAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">InvariantSelfAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.InvariantSelfAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.InvariantSelfAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.InvariantSelfAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.InvariantSelfAttention.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.InvariantSelfAttention.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.InvariantSelfAttention.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.InvariantSelfAttention.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.NodeGaussianLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">NodeGaussianLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">K</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_types</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1024</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.NodeGaussianLayer" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.NodeGaussianLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_types</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.NodeGaussianLayer.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.NodeGaussianLayer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.NodeGaussianLayer.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.NodeTaskHead">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">NodeTaskHead</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.NodeTaskHead" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.NodeTaskHead.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta_pos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.NodeTaskHead.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.NodeTaskHead.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.NodeTaskHead.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.NonLinear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">NonLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.NonLinear" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.NonLinear.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.NonLinear.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.NonLinear.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.NonLinear.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.UnifiedDecoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">UnifiedDecoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_pred_attn_layer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_attention_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ffn_embedding_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_3d_bias_kernel</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_edges</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_atoms</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.UnifiedDecoder" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.UnifiedDecoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batched_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding_mask</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_pos</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.UnifiedDecoder.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.UnifiedDecoder.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.UnifiedDecoder.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.UnifiedDecoder.gelu">
<span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.UnifiedDecoder.</span></span><span class="sig-name descname"><span class="pre">gelu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#sfm.models.graphormer.modules.UnifiedDecoder.gelu" title="Link to this definition"></a></dt>
<dd></dd></dl>

</section>
<section id="module-sfm.models.graphormer.modules.graphormer_embedding">
<span id="sfm-models-graphormer-modules-graphormer-embedding-module"></span><h2>sfm.models.graphormer.modules.graphormer_embedding module<a class="headerlink" href="#module-sfm.models.graphormer.modules.graphormer_embedding" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_embedding.GraphormerEmbeddingMP">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_embedding.</span></span><span class="sig-name descname"><span class="pre">GraphormerEmbeddingMP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">graphormer_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mp_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze_embeddings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_trans_layers_to_freeze</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">export</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_noise</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qn_block_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_embedding.GraphormerEmbeddingMP" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="sfm.modules.html#sfm.modules.sfmmodule.SFMModule" title="sfm.modules.sfmmodule.SFMModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">SFMModule</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_embedding.GraphormerEmbeddingMP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_batchdata</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_embedding.GraphormerEmbeddingMP.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_embedding.GraphormerEmbeddingMP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_embedding.GraphormerEmbeddingMP.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-sfm.models.graphormer.modules.graphormer_layers">
<span id="sfm-models-graphormer-modules-graphormer-layers-module"></span><h2>sfm.models.graphormer.modules.graphormer_layers module<a class="headerlink" href="#module-sfm.models.graphormer.modules.graphormer_layers" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.CosineCutoff">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">CosineCutoff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cutoff_lower</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutoff_upper</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.CosineCutoff" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.CosineCutoff.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">distances</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.CosineCutoff.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.CosineCutoff.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.CosineCutoff.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.Distance">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">Distance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cutoff_lower</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutoff_upper</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_num_neighbors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_vecs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.Distance" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.Distance.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pos</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.Distance.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.Distance.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.Distance.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">EquivariantLayerNorm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">normalized_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">elementwise_linear</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Rotationally-equivariant Vector Layer Normalization
Expects inputs with shape (N, n, d), where N is batch size, n is vector dimension, d is width/number of vectors.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.covariance">
<span class="sig-name descname"><span class="pre">covariance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.covariance" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.elementwise_linear">
<span class="sig-name descname"><span class="pre">elementwise_linear</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.elementwise_linear" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.eps">
<span class="sig-name descname"><span class="pre">eps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.eps" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.extra_repr" title="Link to this definition"></a></dt>
<dd><p>Set the extra representation of the module</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.mean_center">
<span class="sig-name descname"><span class="pre">mean_center</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.mean_center" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.normalized_shape">
<span class="sig-name descname"><span class="pre">normalized_shape</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.normalized_shape" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.symsqrtinv">
<span class="sig-name descname"><span class="pre">symsqrtinv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">matrix</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm.symsqrtinv" title="Link to this definition"></a></dt>
<dd><p>Compute the inverse square root of a positive definite matrix.
Based on <a class="reference external" href="https://github.com/pytorch/pytorch/issues/25481">https://github.com/pytorch/pytorch/issues/25481</a></p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantMultiHeadAttention">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">EquivariantMultiHeadAttention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_channels=256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_rbf=64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance_influence='both'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads=8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation=&lt;class</span> <span class="pre">'torch.nn.modules.activation.SiLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_activation='silu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutoff_lower=0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutoff_upper=5.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_tilde=1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantMultiHeadAttention" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">MessagePassing</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantMultiHeadAttention.aggregate">
<span class="sig-name descname"><span class="pre">aggregate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">features</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ptr</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantMultiHeadAttention.aggregate" title="Link to this definition"></a></dt>
<dd><p>Aggregates messages from neighbors as
<span class="math notranslate nohighlight">\(\bigoplus_{j \in \mathcal{N}(i)}\)</span>.</p>
<p>Takes in the output of message computation as first argument and any
argument which was initially passed to <code class="xref py py-meth docutils literal notranslate"><span class="pre">propagate()</span></code>.</p>
<p>By default, this function will delegate its call to the underlying
<code class="xref py py-class docutils literal notranslate"><span class="pre">Aggregation</span></code> module to reduce messages
as specified in <code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code> by the <code class="xref py py-obj docutils literal notranslate"><span class="pre">aggr</span></code> argument.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantMultiHeadAttention.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vec</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_index</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r_ij</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">f_ij</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ij</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantMultiHeadAttention.forward" title="Link to this definition"></a></dt>
<dd><p>Runs the forward pass of the module.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantMultiHeadAttention.message">
<span class="sig-name descname"><span class="pre">message</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q_i</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k_j</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v_j</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">vec_j</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dk</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dv</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">r_ij</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ij</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantMultiHeadAttention.message" title="Link to this definition"></a></dt>
<dd><p>Constructs messages from node <span class="math notranslate nohighlight">\(j\)</span> to node <span class="math notranslate nohighlight">\(i\)</span>
in analogy to <span class="math notranslate nohighlight">\(\phi_{\mathbf{\Theta}}\)</span> for each edge in
<code class="xref py py-obj docutils literal notranslate"><span class="pre">edge_index</span></code>.
This function can take any argument as input which was initially
passed to <code class="xref py py-meth docutils literal notranslate"><span class="pre">propagate()</span></code>.
Furthermore, tensors passed to <code class="xref py py-meth docutils literal notranslate"><span class="pre">propagate()</span></code> can be mapped to the
respective nodes <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> by appending <code class="xref py py-obj docutils literal notranslate"><span class="pre">_i</span></code> or
<code class="xref py py-obj docutils literal notranslate"><span class="pre">_j</span></code> to the variable name, <em>.e.g.</em> <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_i</span></code> and <code class="xref py py-obj docutils literal notranslate"><span class="pre">x_j</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantMultiHeadAttention.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantMultiHeadAttention.reset_parameters" title="Link to this definition"></a></dt>
<dd><p>Resets all learnable parameters of the module.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantMultiHeadAttention.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantMultiHeadAttention.update" title="Link to this definition"></a></dt>
<dd><p>Updates node embeddings in analogy to
<span class="math notranslate nohighlight">\(\gamma_{\mathbf{\Theta}}\)</span> for each node
<span class="math notranslate nohighlight">\(i \in \mathcal{V}\)</span>.
Takes in the output of aggregation as first argument and any argument
which was initially passed to <code class="xref py py-meth docutils literal notranslate"><span class="pre">propagate()</span></code>.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantVectorOutput">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">EquivariantVectorOutput</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">768</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'silu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantVectorOutput" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantVectorOutput.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantVectorOutput.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantVectorOutput.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantVectorOutput.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.EquivariantVectorOutput.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.EquivariantVectorOutput.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.ExpNormalSmearing">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">ExpNormalSmearing</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cutoff_lower</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cutoff_upper</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_rbf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainable</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.ExpNormalSmearing" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.ExpNormalSmearing.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dist</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.ExpNormalSmearing.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.ExpNormalSmearing.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.ExpNormalSmearing.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.ExpNormalSmearing.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.ExpNormalSmearing.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.GatedEquivariantBlock">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">GatedEquivariantBlock</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intermediate_channels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'silu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scalar_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.GatedEquivariantBlock" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Gated Equivariant Block as defined in Schütt et al. (2021):
Equivariant message passing for the prediction of tensorial properties and molecular spectra</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.GatedEquivariantBlock.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">v</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.GatedEquivariantBlock.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.GatedEquivariantBlock.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.GatedEquivariantBlock.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.GatedEquivariantBlock.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.GatedEquivariantBlock.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.GaussianLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">GaussianLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">K</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_types</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1536</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.GaussianLayer" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.GaussianLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_types</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.GaussianLayer.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.GaussianLayer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.GaussianLayer.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.Graph3DBias">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">Graph3DBias</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_edges</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_kernel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_share_rpe</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.Graph3DBias" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Compute 3D attention bias according to the position information for each head.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.Graph3DBias.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batched_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.Graph3DBias.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.Graph3DBias.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.Graph3DBias.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.GraphAttnBias">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">GraphAttnBias</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_atoms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_edges</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_spatial</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_edge_dis</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_hop_max_dist</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_2d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.GraphAttnBias" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Compute attention bias for each head.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.GraphAttnBias.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batched_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_2d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.GraphAttnBias.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.GraphAttnBias.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.GraphAttnBias.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.GraphNodeFeature">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">GraphNodeFeature</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_atoms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_in_degree</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_out_degree</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_2d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.GraphNodeFeature" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Compute node features for each node in the graph.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.GraphNodeFeature.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batched_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_2d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.GraphNodeFeature.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.GraphNodeFeature.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.GraphNodeFeature.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.NodeTaskHead">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">NodeTaskHead</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.NodeTaskHead" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.NodeTaskHead.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">query</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta_pos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.NodeTaskHead.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.NodeTaskHead.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.NodeTaskHead.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.NonLinear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">NonLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.NonLinear" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.NonLinear.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.NonLinear.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.NonLinear.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.NonLinear.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.RobertaClassificationHead">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">RobertaClassificationHead</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inner_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_classes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooler_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.RobertaClassificationHead" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Head for sentence-level classification tasks.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.RobertaClassificationHead.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">features</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.RobertaClassificationHead.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.RobertaClassificationHead.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.RobertaClassificationHead.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers.init_params">
<span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers.</span></span><span class="sig-name descname"><span class="pre">init_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers.init_params" title="Link to this definition"></a></dt>
<dd></dd></dl>

</section>
<section id="module-sfm.models.graphormer.modules.graphormer_layers_diff">
<span id="sfm-models-graphormer-modules-graphormer-layers-diff-module"></span><h2>sfm.models.graphormer.modules.graphormer_layers_diff module<a class="headerlink" href="#module-sfm.models.graphormer.modules.graphormer_layers_diff" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_diff.Graph3DBiasDiff">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_diff.</span></span><span class="sig-name descname"><span class="pre">Graph3DBiasDiff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_edges</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_kernel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_share_rpe</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_diff.Graph3DBiasDiff" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sfm.models.graphormer.modules.graphormer_layers.Graph3DBias" title="sfm.models.graphormer.modules.graphormer_layers.Graph3DBias"><code class="xref py py-class docutils literal notranslate"><span class="pre">Graph3DBias</span></code></a></p>
<p>Compute 3D attention bias according to the position information for each head.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_diff.Graph3DBiasDiff.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batched_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_diff.Graph3DBiasDiff.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_diff.Graph3DBiasDiff.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_diff.Graph3DBiasDiff.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_diff.GraphAttnBiasDiff">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_diff.</span></span><span class="sig-name descname"><span class="pre">GraphAttnBiasDiff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_atoms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_edges</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_spatial</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_edge_dis</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_hop_max_dist</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_2d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_diff.GraphAttnBiasDiff" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sfm.models.graphormer.modules.graphormer_layers.GraphAttnBias" title="sfm.models.graphormer.modules.graphormer_layers.GraphAttnBias"><code class="xref py py-class docutils literal notranslate"><span class="pre">GraphAttnBias</span></code></a></p>
<p>Compute attention bias for each head.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_diff.GraphAttnBiasDiff.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batched_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_2d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_diff.GraphAttnBiasDiff.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_diff.GraphAttnBiasDiff.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_diff.GraphAttnBiasDiff.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_diff.GraphNodeFeatureDiff">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_diff.</span></span><span class="sig-name descname"><span class="pre">GraphNodeFeatureDiff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_atoms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_in_degree</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_out_degree</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_2d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">t_timesteps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1010</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_embedding_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'positional'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_embedding_mlp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_diff.GraphNodeFeatureDiff" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sfm.models.graphormer.modules.graphormer_layers.GraphNodeFeature" title="sfm.models.graphormer.modules.graphormer_layers.GraphNodeFeature"><code class="xref py py-class docutils literal notranslate"><span class="pre">GraphNodeFeature</span></code></a></p>
<p>Compute node features for each node in the graph.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_diff.GraphNodeFeatureDiff.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batched_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">t</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask_2d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_diff.GraphNodeFeatureDiff.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_diff.GraphNodeFeatureDiff.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_diff.GraphNodeFeatureDiff.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-sfm.models.graphormer.modules.graphormer_layers_mp">
<span id="sfm-models-graphormer-modules-graphormer-layers-mp-module"></span><h2>sfm.models.graphormer.modules.graphormer_layers_mp module<a class="headerlink" href="#module-sfm.models.graphormer.modules.graphormer_layers_mp" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.GaussianLayerMP">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_mp.</span></span><span class="sig-name descname"><span class="pre">GaussianLayerMP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">K</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_types</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1536</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.GaussianLayerMP" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="sfm.modules.html#sfm.modules.sfmmodule.SFMModule" title="sfm.modules.sfmmodule.SFMModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">SFMModule</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.GaussianLayerMP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_types</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.GaussianLayerMP.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.GaussianLayerMP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.GaussianLayerMP.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.Graph3DBiasMP">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_mp.</span></span><span class="sig-name descname"><span class="pre">Graph3DBiasMP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mp_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_edges</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_kernel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_share_rpe</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.Graph3DBiasMP" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="sfm.modules.html#sfm.modules.sfmmodule.SFMModule" title="sfm.modules.sfmmodule.SFMModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">SFMModule</span></code></a></p>
<p>Compute 3D attention bias according to the position information for each head.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.Graph3DBiasMP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_tuple</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.Graph3DBiasMP.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.Graph3DBiasMP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.Graph3DBiasMP.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.GraphAttnBiasMP">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_mp.</span></span><span class="sig-name descname"><span class="pre">GraphAttnBiasMP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mp_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_atoms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_edges</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_spatial</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_edge_dis</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_hop_max_dist</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_2d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.GraphAttnBiasMP" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="sfm.modules.html#sfm.modules.sfmmodule.SFMModule" title="sfm.modules.sfmmodule.SFMModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">SFMModule</span></code></a></p>
<p>Compute attention bias for each head.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.GraphAttnBiasMP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_tuple</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.GraphAttnBiasMP.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.GraphAttnBiasMP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.GraphAttnBiasMP.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.GraphNodeFeatureMP">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_mp.</span></span><span class="sig-name descname"><span class="pre">GraphNodeFeatureMP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mp_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_atoms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_in_degree</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_out_degree</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_2d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.GraphNodeFeatureMP" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="sfm.modules.html#sfm.modules.sfmmodule.SFMModule" title="sfm.modules.sfmmodule.SFMModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">SFMModule</span></code></a></p>
<p>Compute node features for each node in the graph.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.GraphNodeFeatureMP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_tuple</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.GraphNodeFeatureMP.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.GraphNodeFeatureMP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.GraphNodeFeatureMP.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.NonLinearMP">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_mp.</span></span><span class="sig-name descname"><span class="pre">NonLinearMP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_expert_tensor_parallelism</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">moe</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.NonLinearMP" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="sfm.modules.html#sfm.modules.sfmmodule.SFMModule" title="sfm.modules.sfmmodule.SFMModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">SFMModule</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.NonLinearMP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.NonLinearMP.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.NonLinearMP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.NonLinearMP.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_mp.init_params">
<span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_mp.</span></span><span class="sig-name descname"><span class="pre">init_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_mp.init_params" title="Link to this definition"></a></dt>
<dd></dd></dl>

</section>
<section id="module-sfm.models.graphormer.modules.graphormer_layers_pp">
<span id="sfm-models-graphormer-modules-graphormer-layers-pp-module"></span><h2>sfm.models.graphormer.modules.graphormer_layers_pp module<a class="headerlink" href="#module-sfm.models.graphormer.modules.graphormer_layers_pp" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.GaussianLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_pp.</span></span><span class="sig-name descname"><span class="pre">GaussianLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">K</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_types</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1536</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.GaussianLayer" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.GaussianLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_types</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.GaussianLayer.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.GaussianLayer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.GaussianLayer.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.Graph3DBiasPipe">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_pp.</span></span><span class="sig-name descname"><span class="pre">Graph3DBiasPipe</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_edges</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_kernel</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_share_rpe</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.Graph3DBiasPipe" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sfm.models.graphormer.modules.graphormer_layers.Graph3DBias" title="sfm.models.graphormer.modules.graphormer_layers.Graph3DBias"><code class="xref py py-class docutils literal notranslate"><span class="pre">Graph3DBias</span></code></a></p>
<p>Compute 3D attention bias according to the position information for each head.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.Graph3DBiasPipe.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_tuple</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.Graph3DBiasPipe.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.Graph3DBiasPipe.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.Graph3DBiasPipe.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.GraphAttnBiasPipe">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_pp.</span></span><span class="sig-name descname"><span class="pre">GraphAttnBiasPipe</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_atoms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_edges</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_spatial</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_edge_dis</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">edge_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_hop_max_dist</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_2d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.GraphAttnBiasPipe" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sfm.models.graphormer.modules.graphormer_layers.GraphAttnBias" title="sfm.models.graphormer.modules.graphormer_layers.GraphAttnBias"><code class="xref py py-class docutils literal notranslate"><span class="pre">GraphAttnBias</span></code></a></p>
<p>Compute attention bias for each head.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.GraphAttnBiasPipe.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_tuple</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.GraphAttnBiasPipe.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.GraphAttnBiasPipe.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.GraphAttnBiasPipe.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.GraphNodeFeaturePipe">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_pp.</span></span><span class="sig-name descname"><span class="pre">GraphNodeFeaturePipe</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_atoms</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_in_degree</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_out_degree</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">no_2d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.GraphNodeFeaturePipe" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sfm.models.graphormer.modules.graphormer_layers.GraphNodeFeature" title="sfm.models.graphormer.modules.graphormer_layers.GraphNodeFeature"><code class="xref py py-class docutils literal notranslate"><span class="pre">GraphNodeFeature</span></code></a></p>
<p>Compute node features for each node in the graph.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.GraphNodeFeaturePipe.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_tuple</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.GraphNodeFeaturePipe.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.GraphNodeFeaturePipe.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.GraphNodeFeaturePipe.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_pp.</span></span><span class="sig-name descname"><span class="pre">NodeTaskHeadPipe</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sfm.models.graphormer.modules.graphormer_layers.NodeTaskHead" title="sfm.models.graphormer.modules.graphormer_layers.NodeTaskHead"><code class="xref py py-class docutils literal notranslate"><span class="pre">NodeTaskHead</span></code></a></p>
<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe.force_proj1">
<span class="sig-name descname"><span class="pre">force_proj1</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe.force_proj1" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_tuple</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe.k_proj">
<span class="sig-name descname"><span class="pre">k_proj</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe.k_proj" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe.q_proj">
<span class="sig-name descname"><span class="pre">q_proj</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe.q_proj" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe.v_proj">
<span class="sig-name descname"><span class="pre">v_proj</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe.v_proj" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.NonLinear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_pp.</span></span><span class="sig-name descname"><span class="pre">NonLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.NonLinear" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.NonLinear.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.NonLinear.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.NonLinear.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.NonLinear.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.RobertaClassificationHead">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_pp.</span></span><span class="sig-name descname"><span class="pre">RobertaClassificationHead</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inner_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_classes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooler_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.RobertaClassificationHead" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Head for sentence-level classification tasks.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.RobertaClassificationHead.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">features</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.RobertaClassificationHead.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.RobertaClassificationHead.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.RobertaClassificationHead.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_layers_pp.init_params">
<span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_layers_pp.</span></span><span class="sig-name descname"><span class="pre">init_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_layers_pp.init_params" title="Link to this definition"></a></dt>
<dd></dd></dl>

</section>
<section id="module-sfm.models.graphormer.modules.graphormer_sentence_encoder">
<span id="sfm-models-graphormer-modules-graphormer-sentence-encoder-module"></span><h2>sfm.models.graphormer.modules.graphormer_sentence_encoder module<a class="headerlink" href="#module-sfm.models.graphormer.modules.graphormer_sentence_encoder" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder.GraphormerSentenceEncoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_sentence_encoder.</span></span><span class="sig-name descname"><span class="pre">GraphormerSentenceEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">graphormer_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">embed_scale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freeze_embeddings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_trans_layers_to_freeze</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">export</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_noise</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qn_block_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder.GraphormerSentenceEncoder" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Implementation for a Bi-directional Transformer based Sentence Encoder used
in BERT/XLM style pre-trained models.</p>
<p>This first computes the token embedding using the token embedding matrix,
position embeddings (if specified) and segment embeddings
(if specified). After applying the specified number of
TransformerEncoderLayers, it outputs all the internal states of the
encoder as well as the final representation associated with the first
token (usually CLS token).</p>
<dl class="simple">
<dt>Input:</dt><dd><ul class="simple">
<li><p>tokens: B x T matrix representing sentences</p></li>
<li><p>segment_labels: B x T matrix representing segment label for tokens</p></li>
</ul>
</dd>
<dt>Output:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>a tuple of the following:</dt><dd><ul>
<li><p>a list of internal model states used to compute the
predictions where each tensor has shape T x B x C</p></li>
<li><p>sentence representation associated with first input token
in format B x C.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder.GraphormerSentenceEncoder.build_transformer_sentence_encoder_layer">
<span class="sig-name descname"><span class="pre">build_transformer_sentence_encoder_layer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ffn_embedding_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_attention_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_fn</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">export</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_noise</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qn_block_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sandwich_ln</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">droppath_prob</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nl</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder.GraphormerSentenceEncoder.build_transformer_sentence_encoder_layer" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder.GraphormerSentenceEncoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batched_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">perturb</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">segment_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_state_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_embeddings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder.GraphormerSentenceEncoder.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder.GraphormerSentenceEncoder.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder.GraphormerSentenceEncoder.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder.NodeDecoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_sentence_encoder.</span></span><span class="sig-name descname"><span class="pre">NodeDecoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">768</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_attention_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_state_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder.NodeDecoder" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder.NodeDecoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta_pos</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inner_states</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder.NodeDecoder.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder.NodeDecoder.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder.NodeDecoder.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder.init_bert_params">
<span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_sentence_encoder.</span></span><span class="sig-name descname"><span class="pre">init_bert_params</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder.init_bert_params" title="Link to this definition"></a></dt>
<dd><p>Initialize the weights specific to the BERT Model.
This overrides the default initializations depending on the specified arguments.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>If normal_init_linear_weights is set then weights of linear
layer will be initialized using the normal distribution and
bais will be set to the specified value.</p></li>
<li><p>If normal_init_embed_weights is set then weights of embedding
layer will be initialized using the normal distribution.</p></li>
<li><p>If normal_init_proj_weights is set then weights of
in_project_weight for MultiHeadAttention initialized using
the normal distribution (to be validated).</p></li>
</ol>
</div></blockquote>
</dd></dl>

</section>
<section id="module-sfm.models.graphormer.modules.graphormer_sentence_encoder_TMdiff">
<span id="sfm-models-graphormer-modules-graphormer-sentence-encoder-tmdiff-module"></span><h2>sfm.models.graphormer.modules.graphormer_sentence_encoder_TMdiff module<a class="headerlink" href="#module-sfm.models.graphormer.modules.graphormer_sentence_encoder_TMdiff" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_TMdiff.GraphormerSentenceEncoderDiff">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_sentence_encoder_TMdiff.</span></span><span class="sig-name descname"><span class="pre">GraphormerSentenceEncoderDiff</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">graphormer_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer_m_pretrain</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode_prob</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'0.6,0.2,0.2'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_TMdiff.GraphormerSentenceEncoderDiff" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder.GraphormerSentenceEncoder" title="sfm.models.graphormer.modules.graphormer_sentence_encoder.GraphormerSentenceEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">GraphormerSentenceEncoder</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_TMdiff.GraphormerSentenceEncoderDiff.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batched_data</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">perturb</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">segment_labels</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">last_state_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_embeddings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Tensor</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Tensor</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_TMdiff.GraphormerSentenceEncoderDiff.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_TMdiff.GraphormerSentenceEncoderDiff.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_TMdiff.GraphormerSentenceEncoderDiff.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-sfm.models.graphormer.modules.graphormer_sentence_encoder_layer">
<span id="sfm-models-graphormer-modules-graphormer-sentence-encoder-layer-module"></span><h2>sfm.models.graphormer.modules.graphormer_sentence_encoder_layer module<a class="headerlink" href="#module-sfm.models.graphormer.modules.graphormer_sentence_encoder_layer" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.</span></span><span class="sig-name descname"><span class="pre">GraphormerSentenceEncoderLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">768</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ffn_embedding_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3072</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_attention_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">export</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_noise</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qn_block_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sandwich_ln</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">droppath_prob</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nl</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">self_attn_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<p>Implements a Graphormer Encoder Layer used in BERT/XLM style pre-trained
models.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer.build_fc1">
<span class="sig-name descname"><span class="pre">build_fc1</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_noise</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qn_block_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer.build_fc1" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer.build_fc2">
<span class="sig-name descname"><span class="pre">build_fc2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_noise</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qn_block_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer.build_fc2" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer.build_self_attention">
<span class="sig-name descname"><span class="pre">build_self_attention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_attention_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">self_attention</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_noise</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qn_block_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer.build_self_attention" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">self_attn_bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">self_attn_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">self_attn_padding_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer.forward" title="Link to this definition"></a></dt>
<dd><p>LayerNorm is applied either before or after the self-attention/ffn
modules similar to the original Transformer implementation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer_PP">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.</span></span><span class="sig-name descname"><span class="pre">GraphormerSentenceEncoderLayer_PP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embedding_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">768</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ffn_embedding_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">3072</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_attention_heads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_dropout</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">export</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_noise</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qn_block_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Callable</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sandwich_ln</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">droppath_prob</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nl</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">self_attn_mask</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer_PP" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer" title="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">GraphormerSentenceEncoderLayer</span></code></a></p>
<p>Implements a Graphormer Encoder Layer used in BERT/XLM style pre-trained
models.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer_PP.config">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer_PP.config" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer_PP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_tuple</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer_PP.forward" title="Link to this definition"></a></dt>
<dd><p>LayerNorm is applied either before or after the self-attention/ffn
modules similar to the original Transformer implementation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer_PP.tensors_decode">
<span class="sig-name descname"><span class="pre">tensors_decode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">output</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shape_tensor</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer_PP.tensors_decode" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer_PP.tensors_encode">
<span class="sig-name descname"><span class="pre">tensors_encode</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">self_attn_bias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta_pos</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer_PP.tensors_encode" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer_PP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer_PP.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP">
<span id="sfm-models-graphormer-modules-graphormer-sentence-encoder-layer-mp-module"></span><h2>sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP module<a class="headerlink" href="#module-sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.</span></span><span class="sig-name descname"><span class="pre">GraphormerSentenceEncoderLayerMP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">graphormer_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mp_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nl</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="sfm.modules.html#sfm.modules.sfmmodule.SFMModule" title="sfm.modules.sfmmodule.SFMModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">SFMModule</span></code></a></p>
<p>Implements a Graphormer Encoder Layer used in BERT/XLM style pre-trained
models.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.build_fc1">
<span class="sig-name descname"><span class="pre">build_fc1</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_noise</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qn_block_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.build_fc1" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.build_fc1_TP">
<span class="sig-name descname"><span class="pre">build_fc1_TP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.build_fc1_TP" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.build_fc2">
<span class="sig-name descname"><span class="pre">build_fc2</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">q_noise</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qn_block_size</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.build_fc2" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.build_fc2_TP">
<span class="sig-name descname"><span class="pre">build_fc2_TP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.build_fc2_TP" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.build_self_attention_TP">
<span class="sig-name descname"><span class="pre">build_self_attention_TP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embed_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_attention_heads</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_tilde</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.build_self_attention_TP" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_tuple</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.forward" title="Link to this definition"></a></dt>
<dd><p>LayerNorm is applied either before or after the self-attention/ffn
modules similar to the original Transformer implementation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.reset_parameters">
<span class="sig-name descname"><span class="pre">reset_parameters</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.reset_parameters" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-sfm.models.graphormer.modules.graphormer_sentence_encoder_mp">
<span id="sfm-models-graphormer-modules-graphormer-sentence-encoder-mp-module"></span><h2>sfm.models.graphormer.modules.graphormer_sentence_encoder_mp module<a class="headerlink" href="#module-sfm.models.graphormer.modules.graphormer_sentence_encoder_mp" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_mp.GraphormerEncoderMP">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_sentence_encoder_mp.</span></span><span class="sig-name descname"><span class="pre">GraphormerEncoderMP</span></span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_mp.GraphormerEncoderMP" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_mp.GraphormerEncoderMP.to_layers">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">to_layers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">graphormer_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mp_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_id</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">load_ckpt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ckp_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">[]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_mp.GraphormerEncoderMP.to_layers" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_sentence_encoder_mp.GraphormerEncoderMP.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_sentence_encoder_mp.GraphormerEncoderMP.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-sfm.models.graphormer.modules.graphormer_timestep_encoder">
<span id="sfm-models-graphormer-modules-graphormer-timestep-encoder-module"></span><h2>sfm.models.graphormer.modules.graphormer_timestep_encoder module<a class="headerlink" href="#module-sfm.models.graphormer.modules.graphormer_timestep_encoder" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_timestep_encoder.SinusoidalPositionEmbeddings">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_timestep_encoder.</span></span><span class="sig-name descname"><span class="pre">SinusoidalPositionEmbeddings</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_period</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_timestep_encoder.SinusoidalPositionEmbeddings" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_timestep_encoder.SinusoidalPositionEmbeddings.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">time</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_timestep_encoder.SinusoidalPositionEmbeddings.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_timestep_encoder.SinusoidalPositionEmbeddings.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_timestep_encoder.SinusoidalPositionEmbeddings.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_timestep_encoder.TimeStepEncoder">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">sfm.models.graphormer.modules.graphormer_timestep_encoder.</span></span><span class="sig-name descname"><span class="pre">TimeStepEncoder</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_timesteps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep_emb_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">timestep_emb_type</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_timestep_encoder.TimeStepEncoder" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_timestep_encoder.TimeStepEncoder.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">timesteps</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_timestep_encoder.TimeStepEncoder.forward" title="Link to this definition"></a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sfm.models.graphormer.modules.graphormer_timestep_encoder.TimeStepEncoder.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">bool</span></em><a class="headerlink" href="#sfm.models.graphormer.modules.graphormer_timestep_encoder.TimeStepEncoder.training" title="Link to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-sfm.models.graphormer.modules">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-sfm.models.graphormer.modules" title="Link to this heading"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="sfm.models.graphormer.html" class="btn btn-neutral float-left" title="sfm.models.graphormer package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="sfm.models.llama2.html" class="btn btn-neutral float-right" title="sfm.models.llama2 package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, MSR A4S team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

</body>
</html>
