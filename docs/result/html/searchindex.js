Search.setIndex({"docnames": ["api/modules", "api/sfm", "api/sfm.criterions", "api/sfm.data", "api/sfm.data.dec_data", "api/sfm.data.mol_data", "api/sfm.data.prot_data", "api/sfm.data.sci_data", "api/sfm.data.tamgent2", "api/sfm.logging", "api/sfm.models", "api/sfm.models.decoder", "api/sfm.models.decoder.deepfuse", "api/sfm.models.generalist", "api/sfm.models.generalist.modules", "api/sfm.models.graphormer", "api/sfm.models.graphormer.modules", "api/sfm.models.llama2", "api/sfm.models.pfm", "api/sfm.models.pfm.modules", "api/sfm.models.scigpt", "api/sfm.models.tamgent", "api/sfm.modules", "api/sfm.pipeline", "api/sfm.pipeline.accelerator", "api/sfm.pipeline.generalist", "api/sfm.tasks", "api/sfm.tasks.decoder", "api/sfm.tasks.generalist", "api/sfm.tasks.graphormer", "api/sfm.tasks.pfm", "api/sfm.tasks.scigpt", "api/sfm.tasks.tamgent", "api/sfm.utils", "api/sfm.utils.optim", "datapipeline", "distributed", "index", "installation", "modelzoo", "readme", "trainargs", "userfriendly"], "filenames": ["api/modules.rst", "api/sfm.rst", "api/sfm.criterions.rst", "api/sfm.data.rst", "api/sfm.data.dec_data.rst", "api/sfm.data.mol_data.rst", "api/sfm.data.prot_data.rst", "api/sfm.data.sci_data.rst", "api/sfm.data.tamgent2.rst", "api/sfm.logging.rst", "api/sfm.models.rst", "api/sfm.models.decoder.rst", "api/sfm.models.decoder.deepfuse.rst", "api/sfm.models.generalist.rst", "api/sfm.models.generalist.modules.rst", "api/sfm.models.graphormer.rst", "api/sfm.models.graphormer.modules.rst", "api/sfm.models.llama2.rst", "api/sfm.models.pfm.rst", "api/sfm.models.pfm.modules.rst", "api/sfm.models.scigpt.rst", "api/sfm.models.tamgent.rst", "api/sfm.modules.rst", "api/sfm.pipeline.rst", "api/sfm.pipeline.accelerator.rst", "api/sfm.pipeline.generalist.rst", "api/sfm.tasks.rst", "api/sfm.tasks.decoder.rst", "api/sfm.tasks.generalist.rst", "api/sfm.tasks.graphormer.rst", "api/sfm.tasks.pfm.rst", "api/sfm.tasks.scigpt.rst", "api/sfm.tasks.tamgent.rst", "api/sfm.utils.rst", "api/sfm.utils.optim.rst", "datapipeline.rst", "distributed.rst", "index.rst", "installation.rst", "modelzoo.rst", "readme.rst", "trainargs.rst", "userfriendly.rst"], "titles": ["sfm", "sfm package", "sfm.criterions package", "sfm.data package", "sfm.data.dec_data package", "sfm.data.mol_data package", "sfm.data.prot_data package", "sfm.data.sci_data package", "sfm.data.tamgent2 package", "sfm.logging package", "sfm.models package", "sfm.models.decoder package", "sfm.models.decoder.deepfuse package", "sfm.models.generalist package", "sfm.models.generalist.modules package", "sfm.models.graphormer package", "sfm.models.graphormer.modules package", "sfm.models.llama2 package", "sfm.models.pfm package", "sfm.models.pfm.modules package", "sfm.models.scigpt package", "sfm.models.tamgent package", "sfm.modules package", "sfm.pipeline package", "sfm.pipeline.accelerator package", "sfm.pipeline.generalist package", "sfm.tasks package", "sfm.tasks.decoder package", "sfm.tasks.generalist package", "sfm.tasks.graphormer package", "sfm.tasks.pfm package", "sfm.tasks.scigpt package", "sfm.tasks.tamgent package", "sfm.utils package", "sfm.utils.optim package", "Data Pipeline", "Large Distributed Training", "Welcome to A4SFramework\u2019s documentation!", "Installation Guide", "Model Zoo", "Overview", "Training Args", "User-friendly Interface"], "terms": {"packag": [0, 37, 40], "subpackag": 0, "criterion": [0, 1], "submodul": [0, 1, 10, 11, 23, 26], "autoregress": [0, 1], "modul": [0, 37, 39], "copilotloss": [0, 1], "l1ft": [0, 1], "mae2d": [0, 1], "mae3d": [0, 1], "mae3ddiff": [0, 1], "content": 0, "data": [0, 1, 12, 15, 18, 21, 24, 33, 34, 37, 39, 40, 41, 42], "data_util": [0, 1], "data_utils_fast": [0, 1], "dataset": [0, 1, 24, 33, 35, 41, 42], "dynamics_load": [0, 1], "molecul": [0, 1, 14, 28], "sampler": [0, 1, 5, 37], "text": [0, 1, 4, 5, 6, 8, 12, 14, 21, 39], "log": [0, 1, 24, 33, 34, 36, 41], "logger": [0, 1], "model": [0, 1, 22, 23, 27, 33, 34, 35, 37, 40, 41, 42], "fairseqdropout": [0, 1], "flashatt_bk": [0, 1], "droppath": [0, 1], "fused_layernorm": [0, 1], "get_activation_fn": [0, 1], "layer_norm": [0, 1], "mem_eff_attn": [0, 1], "multihead_attent": [0, 1], "parallelattentionbia": [0, 1], "partial_grad_emb": [0, 1], "quant_nois": [0, 1], "rotary_embed": [0, 1], "sfmmodul": [0, 1, 14, 16, 17, 39], "pipelin": [0, 1, 6, 12, 20, 21, 22, 33, 36, 37, 39, 40, 41, 42], "task": [0, 1, 10, 15, 16, 18, 35, 39, 42], "util": [0, 1, 3, 36, 42], "fairseqdataset": [0, 1, 3], "layerdropmodulelist": [0, 1], "ppengin": [0, 1], "arg_util": [0, 1], "barrier": [0, 1, 23, 24], "cli_util": [0, 1, 42], "copilot_modul": [0, 1], "defaultdsconfig": [0, 1], "dist_util": [0, 1], "env_init": [0, 1], "get_paranum": [0, 1], "jload": [0, 1], "move_to_devic": [0, 1], "mypipelineparallelgrid": [0, 1], "mypp_modul": [0, 1], "peft": [0, 1], "pipelinemod": [0, 1], "pretrained_layer_spec": [0, 1, 36], "science_token": [0, 1], "autoregressivecriterion": [1, 2], "forward": [1, 2, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39, 42], "train": [1, 2, 5, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 28, 33, 34, 35, 37, 39, 40], "copilotcriterion": [1, 2], "copilotcriterionsnummp": [1, 2], "copilotcriterionsnumpp": [1, 2], "copilotcriterionspp": [1, 2], "binarycriterion": [1, 2], "l1criterion": [1, 2], "l1criterionspp": [1, 2], "mae2d_criterion": [1, 2], "mae3dcriterion": [1, 2], "mae3dcriterionspp": [1, 2], "tensors_decod": [1, 2, 15, 16], "proteinmae3dcriterion": [1, 2], "diffmae3dcriterion": [1, 2], "dec_data": [1, 3], "sfmdectoken": [1, 3], "mol_data": [1, 3, 42], "algo": [1, 3], "collat": [1, 3, 4, 7, 8, 33], "molftdataapi": [1, 3], "moltext_dataset": [1, 3], "moltoken": [1, 3], "tdc": [1, 3], "wrapper": [1, 3, 33], "xyz2smil": [1, 3], "prot_data": [1, 3, 35], "process": [1, 3, 5, 12, 13, 24, 33, 35, 39, 41, 42], "sequence_mask": [1, 3], "spatial_nois": [1, 3], "structure2lmdb": [1, 3], "vocalubari": [1, 3], "sci_data": [1, 3, 42], "tamgent2": [1, 3, 10, 21, 39], "token": [1, 3, 4, 5, 6, 7, 12, 13, 14, 15, 16, 18, 21, 28, 33, 35, 39], "batch_by_s": [1, 3, 33], "collect_filt": [1, 3], "batch_by_size_fn": [1, 3], "batch_by_size_vec": [1, 3], "batch_fixed_shapes_fast": [1, 3], "batch": [1, 3, 4, 6, 7, 8, 12, 13, 16, 17, 19, 21, 22, 23, 24, 33, 34, 37, 39, 41, 42], "batch_siz": [1, 3, 4, 5, 8, 11, 12, 13, 21, 26, 28, 39], "foundationmodeldataset": [1, 3, 4, 6, 24, 35], "get_batch_shap": [1, 3, 33], "num_token": [1, 3, 6, 7, 33, 35], "num_tokens_vec": [1, 3, 6, 33], "inmemoryfoundationmodeldataset": [1, 3, 5, 7, 8], "lmdbfoundationmodeldataset": [1, 3], "dynamicbatchsampl": [1, 3], "is_batch_ful": [1, 3], "dynamicdistributedsampl": [1, 3], "set_epoch": [1, 3, 33], "sort_dataset": [1, 3], "from_smil": [1, 3], "from_xyz_and_bond_index": [1, 3], "mol2graph": [1, 3, 5], "weighteddistributedsampl": [1, 3, 35], "mixedtext": [1, 3], "from_json": [1, 3], "from_raw_text": [1, 3], "metriclogg": [1, 9], "console_log_filt": [1, 9], "dataclass_to_dict": [1, 9], "get_logg": [1, 9], "decod": [1, 10, 14, 17, 21, 26, 33, 36, 37], "generalist": [1, 10, 23, 26, 36, 37], "generalist_config": [1, 10], "graphormer_llama": [1, 10, 36, 39], "graphorm": [1, 10, 26, 36, 37, 38, 42], "graphormer_config": [1, 10, 14, 16, 39], "graphormerdiff": [1, 10], "llama2": [1, 10, 12, 36, 37], "convert_llamaweight2pp": [1, 10], "llama2mp_config": [1, 10], "llama_modul": [1, 10], "llama_modules_3dmp": [1, 10, 39], "pfm": [1, 10, 26, 35], "pfm_config": [1, 10, 19], "pfmmodel": [1, 10], "scigpt": [1, 10, 26, 37, 42], "config": [1, 2, 6, 7, 10, 11, 13, 14, 15, 16, 17, 21, 22, 33, 36, 39, 42], "tamgent": [1, 10, 12, 26, 37], "qformer": [1, 10, 13, 14], "schedul": [1, 10, 12, 13, 15, 18, 20, 24, 33, 34, 39, 42], "make_generation_fast_": [1, 22], "flashatt": [1, 22], "prepare_for_onnx_export_": [1, 22], "reset_paramet": [1, 15, 16, 18, 19, 22], "init_to_zero": [1, 22], "extra_repr": [1, 15, 16, 18, 19, 22], "fusedlayernorm": [1, 22], "elementwise_affin": [1, 22], "ep": [1, 12, 15, 16, 18, 19, 20, 21, 22, 23, 24, 34, 39, 41], "normalized_shap": [1, 15, 16, 18, 19, 22], "test_layer_norm": [1, 22], "deprecation_warn": [1, 22], "gelu": [1, 12, 15, 16, 17, 18, 19, 22, 39], "gelu_accur": [1, 22], "get_available_activation_fn": [1, 22], "relu_squar": [1, 22], "fp32layernorm": [1, 22], "layernorm": [1, 16, 19, 22], "memeffattn": [1, 22], "apply_sparse_mask": [1, 22], "multiheadattent": [1, 16, 22], "coreattentionbia": [1, 22], "tpmultiheadattent": [1, 22], "repeat_kv": [1, 22], "partialgradembed": [1, 22], "rotaryembed": [1, 22], "apply_rotary_pos_emb": [1, 22], "rotate_half": [1, 22], "auto_partition_load_state_dict": [1, 10, 17, 22, 39], "acceler": [1, 12, 20, 21, 23, 35, 39, 41, 42], "dataclass": [1, 9, 12, 20, 21, 23, 39, 41, 42], "fp16_scaler": [1, 23], "pipeline_modul": [1, 23], "trainer": [1, 23, 37, 40], "graphormerllama_3dtrain": [1, 23], "gen_dec_deepfus": [1, 26], "train_dec_deepfus": [1, 26], "eval_generalist_metr": [1, 26], "ft3d_graphormer_llama_inst": [1, 26], "ft_graphormer_llama_inst": [1, 26], "test_generalist": [1, 26], "test_generalist_pp": [1, 26], "ft_graphorm": [1, 26, 36], "pretrain_graphorm": [1, 26, 36, 42], "pretrain_pfm": [1, 26, 35], "pretrain_scigpt": [1, 26, 42], "finetune_tamgent2": [1, 26], "optim": [1, 12, 13, 15, 18, 20, 21, 24, 33, 35, 37, 39, 41, 42], "adam": [1, 33], "set_lr": [1, 33], "epochlisten": [1, 33], "can_reuse_epoch_itr_across_epoch": [1, 33], "attr": [1, 33], "collate_token": [1, 33], "filter_indices_by_s": [1, 3, 6, 33], "ordered_indic": [1, 33], "prefetch": [1, 33], "size": [1, 3, 6, 14, 16, 17, 19, 22, 24, 33, 39, 41], "supports_fetch_outside_dataload": [1, 33], "supports_prefetch": [1, 33], "fairseqiterabledataset": [1, 33], "sfmpipeengin": [1, 33], "dtype_to_id": [1, 33], "id_to_dtyp": [1, 33], "allreduce_gradi": [1, 33], "backward": [1, 23, 24, 33], "eval_batch": [1, 33], "fast_load_checkpoint": [1, 33], "is_first_stag": [1, 33], "is_gradient_accumulation_boundari": [1, 33], "is_last_stag": [1, 33], "load_checkpoint": [1, 23, 24, 33], "load_module_state_dict": [1, 33], "log_for_devic": [1, 33], "mem_statu": [1, 33], "module_state_dict": [1, 33], "reset_activation_shap": [1, 33], "set_batch_fn": [1, 33], "set_dataiter": [1, 33], "set_dataload": [1, 33], "set_has_attention_mask": [1, 33], "set_train_batch_s": [1, 33], "step": [1, 21, 23, 24, 33, 34, 41], "tput_log": [1, 33], "train_batch": [1, 33, 34], "initi": [1, 14, 16, 21, 24, 33, 36, 41, 42], "is_even": [1, 33], "extraargsprovid": [1, 33], "add_dataclass_to_pars": [1, 33], "argument_exist": [1, 33], "from_arg": [1, 33], "is_collect": [1, 33], "is_enum_typ": [1, 33], "make_enum_pras": [1, 33], "unwarp_opt": [1, 33], "check": [1, 33], "cli": [1, 33, 37], "copilotmodul": [1, 33], "layerspec": [1, 33, 36], "build": [1, 33, 36], "pipelineerror": [1, 33], "tiedlayerspec": [1, 33], "is_master_nod": [1, 33], "set_env": [1, 33], "count_paranum": [1, 33], "jdump": [1, 33], "outputmixin": [1, 33], "get": [1, 33], "iget": [1, 33], "item": [1, 5, 6, 33], "kei": [1, 14, 17, 21, 22, 33, 39], "pipedataparalleltopologi": [1, 33], "pipemodeldataparalleltopologi": [1, 33], "processtopologi": [1, 33], "filter_match": [1, 33], "get_axis_comm_list": [1, 33], "get_axis_list": [1, 33], "get_axis_nam": [1, 33], "get_coord": [1, 33], "get_dim": [1, 33], "get_rank": [1, 33], "get_rank_repr": [1, 33], "world_siz": [1, 12, 20, 21, 23, 24, 33, 39, 41], "get_data_parallel_group": [1, 33], "get_data_parallel_id": [1, 33], "get_data_parallel_rank": [1, 33], "get_data_parallel_world_s": [1, 33], "get_global_rank": [1, 33], "get_model_parallel_group": [1, 33], "get_model_parallel_rank": [1, 33], "get_model_parallel_world_s": [1, 33], "get_pipe_parallel_group": [1, 33], "get_pipe_parallel_rank": [1, 33], "get_pipe_parallel_world_s": [1, 33], "get_slice_parallel_group": [1, 33], "get_slice_parallel_rank": [1, 33], "get_slice_parallel_world_s": [1, 33], "get_stage_id": [1, 33], "stage_to_glob": [1, 33], "topologi": [1, 24, 33], "pipelinemodul": [1, 24, 33], "allreduce_tied_weight_gradi": [1, 33], "ckpt_layer_path": [1, 33], "ckpt_layer_path_list": [1, 33], "ckpt_prefix": [1, 33], "fast_load_state_dir": [1, 33], "get_tied_weights_and_group": [1, 33], "load_state_dir": [1, 33], "mpu": [1, 33], "num_pipeline_stag": [1, 33], "partit": [1, 17, 22, 33, 39], "save_state_dict": [1, 33], "set_checkpoint_interv": [1, 33], "stage_own": [1, 33], "partition_by_lay": [1, 33], "create_peft_model": [1, 33, 36], "llamadecoderlayertest": [1, 33], "dict_forward": [1, 33], "tensor_forward": [1, 33], "check_grad_requir": [1, 33], "convert2list": [1, 33], "pipemod": [1, 33], "pipemodegradcheck": [1, 33], "tuple2input": [1, 33], "pretrainedlayerspec": [1, 33, 36], "load_pretrain": [1, 33, 36], "partition_load_pretrain": [1, 33, 36], "resize_token_embed": [1, 10, 13, 14, 17, 33, 36], "tiedpretrainedlayerspec": [1, 33], "load_ckp_tied_modul": [1, 33], "class": [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 28, 33, 34, 35, 36, 39, 41, 42], "reduct": [2, 21, 33], "mean": [2, 21, 33], "base": [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 28, 33, 34, 35, 36, 39, 41, 42], "output": [2, 12, 14, 15, 16, 18, 21, 33, 39], "label": [2, 5, 14, 15, 16, 18, 21, 33, 39], "defin": [2, 6, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 34, 39, 42], "comput": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 34, 35, 39], "perform": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 33, 34, 39, 41, 42], "everi": [2, 4, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "call": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 33, 39, 42], "should": [2, 3, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "overridden": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "all": [2, 3, 6, 7, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "subclass": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "although": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "recip": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "pass": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39, 42], "need": [2, 6, 7, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 35, 36, 39], "within": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 33, 39, 41], "thi": [2, 3, 4, 6, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 33, 34, 35, 38, 39, 41, 42], "function": [2, 3, 5, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 34, 39, 42], "one": [2, 3, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39, 41], "instanc": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "afterward": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "instead": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "sinc": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "former": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "take": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39, 42], "care": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "run": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 34, 38, 39], "regist": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "hook": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "while": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "latter": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "silent": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "ignor": [2, 3, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "them": [2, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39, 41], "bool": [2, 3, 4, 5, 6, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 33, 34, 35, 39, 41, 42], "vocab_s": [2, 10, 11, 12, 13, 14, 17, 20, 21, 39], "32001": [2, 17, 39], "data_mean": [2, 15, 18, 39], "0": [2, 3, 4, 5, 6, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 33, 34, 35, 39, 41], "data_std": [2, 15, 18, 39], "1": [2, 3, 4, 5, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 33, 34, 36, 38, 39, 41], "batch_data": [2, 15, 18, 20, 24, 39], "dict": [2, 3, 5, 6, 7, 9, 12, 13, 20, 24, 25, 28, 33, 34, 35, 36, 39, 42], "logit": [2, 11, 12, 15, 17, 18, 21, 39], "tensor": [2, 4, 5, 6, 7, 12, 13, 14, 16, 17, 19, 21, 22, 24, 33, 36, 39, 40, 41], "tupl": [2, 3, 5, 6, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 33, 34, 35, 39, 42], "arg": [2, 3, 5, 6, 7, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 36, 37, 39, 40, 42], "datasetmean": 2, "datasetstd": 2, "target": [2, 5, 33], "node_output": 2, "value_tensor": 2, "shape_tensor": [2, 16], "mask_po": [2, 16, 19], "mask_aa": [2, 19], "y_pred": 2, "convert_tokens_to_str": [3, 4, 7, 8], "mixedtokendata": [3, 4, 12, 39], "from_tupl": [3, 4, 11, 12], "label_seq": [3, 4], "non_padding_mask": [3, 4], "pad_idx": [3, 4, 33], "to_tupl": [3, 4, 11, 12], "token_seq": [3, 4], "token_type_mask": [3, 4, 11, 12], "mixedtokendataset": [3, 4], "entity_marker_from_entity_id_to_text_id": [3, 4], "from_jsonl": [3, 4], "from_text_to_mol": [3, 4], "init_tokenzi": [3, 4], "pad_sequ": [3, 4], "textspan": [3, 4], "type": [3, 4, 7, 12, 19, 33, 34], "tokentyp": [3, 4, 12], "entiti": [3, 4, 12, 39], "all_shortest_path_count": [3, 5], "all_shortest_path_count_custom": [3, 5], "floyd_warshal": [3, 5], "gen_edge_input": [3, 5], "gen_edge_input_with_nod": [3, 5], "get_all_edg": [3, 5], "collator_3d": [3, 5], "collator_3d_pp": [3, 5], "collator_copilot": [3, 5], "collator_copilot_multi_mol": [3, 5], "collator_copilot_multi_mol_pp": [3, 5], "collator_ft": [3, 5], "pad_1d_unsqueez": [3, 5, 6, 7], "pad_2d_unsqueez": [3, 5, 6], "pad_3d_unsqueez": [3, 5], "pad_attn_bias_unsqueez": [3, 5], "pad_edge_type_unsqueez": [3, 5], "pad_pos_unsqueez": [3, 5], "pad_spatial_pos_unsqueez": [3, 5], "batcheddatadataset": [3, 5, 6, 7], "collaterft": [3, 5], "dsdataload": [3, 5], "pcqpreprocesseddata": [3, 5], "setup": [3, 5], "preprocesssmil": [3, 5], "processed_file_nam": [3, 5], "targetdataset": [3, 5], "data_prefetch": [3, 5], "next": [3, 5, 21, 33], "preload": [3, 5], "mydataload": [3, 5], "drop_last": [3, 5, 24, 35], "num_work": [3, 5, 33], "pin_memori": [3, 5], "pin_memory_devic": [3, 5], "prefetch_factor": [3, 5], "timeout": [3, 5], "smile2data": [3, 5], "datacollatorforsuperviseddataset": [3, 5], "add_mfm": [3, 5], "pad_token_id": [3, 5, 10, 14, 17, 20], "use_pp": [3, 5], "supervisedmoleculenetdataset": [3, 5], "supervisedprocesseddata": [3, 5], "supervisedprocesseddatawithsmil": [3, 5], "batch_collater_for_graphorm": [3, 5], "preprocess": [3, 5, 21, 24, 39, 42], "preprocess_item": [3, 5], "preprocess_mask": [3, 5], "preprocess_moleculenet": [3, 5], "smiles2graph_removeh": [3, 5], "split_text_and_mol": [3, 5], "tdcdataset": [3, 5], "get_rdk_descriptor": [3, 5], "smiles2graph": [3, 5], "smiles2graphpo": [3, 5], "mypygpcqm4mdataset": [3, 5], "download": [3, 5, 21], "mypygpcqm4mposdataset": [3, 5], "pm6fulllmdbdataset": [3, 5], "get_idx_split": [3, 5], "get_keys_list": [3, 5], "init_env_list": [3, 5], "msg_dir": [3, 5], "processed_dir": [3, 5], "raw_dir": [3, 5], "raw_file_nam": [3, 5], "smiles_db_dir": [3, 5], "pygpcqm4mv2dataset": [3, 5], "pygpcqm4mv2posdataset": [3, 5], "read_xyz_fil": [3, 5], "xyz_to_smil": [3, 5], "collate_fn": [3, 5, 6, 7, 33], "pad_2d_square_unsqueez": [3, 6], "proteinlmdbdataset": [3, 6, 35], "set_default_arg": [3, 6], "split_dataset": [3, 6], "angle_help": [3, 6], "bstr2obj": [3, 6], "obj2bstr": [3, 6], "process_cif": [3, 6], "process_conf": [3, 6], "bert_sequence_mask": [3, 6], "no_sequence_mask": [3, 6], "transformerm_mask": [3, 6], "no_nois": [3, 6], "normal_nois": [3, 6], "chunk": [3, 6], "process_item": [3, 6], "add_to_env": [3, 6], "alphabet": [3, 6], "encod": [3, 6, 14, 15, 16, 17, 18, 19, 21, 39], "feat_idx": [3, 6], "feat_text": [3, 6], "get_idx": [3, 6], "get_tok": [3, 6], "to_dict": [3, 6], "processedscidataset": [3, 7], "scidataset": [3, 7], "collate_fn_pp": [3, 7], "batchtexttomoldata": [3, 8], "smile": [3, 4, 5, 8, 13, 28, 39], "texttomoldata": [3, 8], "texttomoldataset": [3, 8], "from_fil": [3, 8], "molxpttoken": [3, 8], "indic": [3, 6, 13, 17, 21, 22, 24, 33, 39, 41], "num_tokens_fn": 3, "max_length": [3, 10, 11, 12, 15, 35], "1024": [3, 5, 12, 14, 15, 16, 18, 19], "none": [3, 5, 6, 7, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 28, 29, 30, 31, 33, 34, 35, 39, 41, 42], "max_token": [3, 12, 20, 21, 23, 24, 33, 35, 39, 41], "max_sampl": 3, "required_batch_size_multipl": [3, 33], "fixed_shap": 3, "yield": [3, 6, 14], "mini": [3, 33], "bucket": 3, "mai": [3, 6, 7, 33], "contain": [3, 15, 18, 21, 33, 39, 41], "sequenc": [3, 4, 5, 6, 7, 8, 13, 14, 21, 33, 35, 39], "differ": [3, 14, 24, 33, 41, 42], "length": [3, 13, 14, 21, 33, 35, 39], "list": [3, 4, 5, 6, 7, 8, 12, 13, 14, 15, 16, 18, 20, 21, 22, 24, 33, 34, 39, 41, 42], "int": [3, 4, 5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 28, 33, 34, 35, 39, 41], "order": [3, 5, 33], "callabl": [3, 5, 16, 19, 33, 34], "return": [3, 4, 5, 6, 12, 13, 14, 15, 17, 18, 20, 21, 22, 24, 33, 34, 39, 42], "number": [3, 14, 16, 19, 24, 33, 34, 41], "given": [3, 15, 18, 21, 33, 39], "index": [3, 5, 6, 7, 16, 33, 34, 35, 37], "option": [3, 12, 13, 14, 17, 20, 21, 22, 33, 34, 39, 42], "max": [3, 6, 33, 34], "sampl": [3, 5, 6, 7, 8, 21, 22, 33, 35, 39], "precomput": [3, 21], "vector": [3, 16, 19], "each": [3, 12, 14, 16, 19, 21, 22, 24, 33, 34, 35, 39, 42], "enabl": [3, 24, 33, 42], "faster": [3, 35], "gener": [3, 10, 13, 24, 33, 39, 41], "default": [3, 6, 7, 14, 16, 22, 24, 33, 34, 41, 42], "sentenc": [3, 13, 14, 15, 16, 18, 20, 39], "requir": [3, 24, 33, 37, 41], "less": [3, 12], "than": [3, 6, 12, 33, 35, 36], "n": [3, 6, 12, 16, 19, 21, 22, 33, 38, 39], "multipl": [3, 24, 35, 41, 42], "onli": [3, 12, 14, 15, 18, 21, 22, 33, 38, 39], "creat": [3, 33, 38], "shape": [3, 13, 14, 16, 17, 19, 21, 22, 33, 39], "max_sent": [3, 33], "iter": [3, 5, 22, 33, 34], "filter": [3, 6, 33], "similar": [3, 14, 15, 16, 18, 19, 35, 39], "collect": [3, 33], "element": [3, 17, 22, 33, 39], "fals": [3, 4, 5, 6, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 33, 34, 35, 36, 39, 41], "store": [3, 14, 24, 33, 36, 41], "object": [3, 4, 5, 6, 9, 12, 13, 14, 15, 24, 25, 28, 33, 39, 41, 42], "an": [3, 4, 12, 13, 14, 15, 18, 20, 21, 33, 35, 39], "set": [3, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 33, 35, 39, 40, 41, 42], "accord": [3, 14, 16, 19, 33], "valid": [3, 16, 23, 24, 33, 41, 42], "exampl": [3, 5, 14, 21, 24, 33, 34, 35, 37, 38, 39, 40, 42], "8": [3, 16, 19, 21, 22, 33, 34, 36, 39, 42], "512": [3, 5, 13, 14, 33, 34], "16": [3, 33], "256": [3, 16, 33], "32": [3, 5, 14, 15, 16, 17, 18, 20, 21, 33, 39], "128": [3, 5, 15, 16, 19, 33], "The": [3, 5, 6, 12, 13, 14, 21, 24, 33, 34, 35, 36, 39, 41, 42], "first": [3, 12, 14, 15, 16, 18, 24, 33, 39, 41, 42], "dimens": [3, 14, 16, 19, 33], "i": [3, 4, 6, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 24, 33, 34, 35, 36, 38, 39, 41, 42], "can": [3, 6, 12, 14, 16, 17, 21, 24, 33, 35, 36, 38, 39, 41, 42], "automat": [3, 33], "infer": [3, 5, 10, 15, 20, 33], "second": [3, 24, 33, 41, 42], "support": [3, 13, 15, 18, 20, 22, 24, 33, 36, 38, 39, 41, 42], "fairseq": [3, 33], "us": [3, 6, 12, 13, 14, 15, 16, 17, 18, 21, 22, 24, 33, 34, 35, 36, 38, 39, 40, 41, 42], "restrict": [3, 33], "tpu": [3, 33], "avoid": [3, 13, 21, 33, 39], "too": [3, 33], "mani": [3, 33], "dynam": [3, 24, 33, 37, 41], "recompil": [3, 33], "lmdb_path": 3, "path": [3, 4, 6, 7, 21, 22, 24, 27, 33, 36, 39, 41], "num_bucket": 3, "min_siz": 3, "max_siz": [3, 6, 33], "1000": [3, 12, 15, 20, 21, 24, 34, 39, 41], "kwarg": [3, 4, 5, 6, 7, 8, 12, 14, 15, 16, 17, 18, 21, 22, 24, 33, 34, 39, 42], "epoch": [3, 21, 23, 24, 33, 34, 35, 41], "when": [3, 12, 14, 22, 33, 35], "shuffl": 3, "true": [3, 5, 6, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 36, 39], "ensur": [3, 24, 35, 41], "replica": 3, "random": [3, 24, 33, 41], "otherwis": [3, 33], "same": [3, 21, 33, 35], "atom_feat": 3, "longtensor": [3, 12, 13, 14, 17, 21, 33, 39], "arrai": [3, 6, 7, 33], "bond_feat": 3, "bond_index": 3, "atom_po": 3, "floattensor": [3, 12, 13, 17, 21, 33, 39], "A": [3, 4, 6, 13, 21, 24, 33, 34, 39, 41], "represent": [3, 14, 15, 16, 18, 19, 22, 33, 39], "organ": [3, 33], "featur": [3, 16, 19, 35], "extract": 3, "from": [3, 4, 6, 7, 14, 15, 16, 17, 18, 21, 22, 24, 33, 34, 35, 36, 39, 41, 42], "ogb": 3, "smiles2mol": 3, "atom": 3, "posit": [3, 6, 14, 15, 16, 18, 19, 21, 33, 39], "construct": [3, 16, 21, 33], "raw": [3, 6, 22], "string": [3, 4, 5, 6, 7, 8, 13, 14, 16, 19, 22, 33, 39], "xyz": 3, "file": [3, 4, 5, 21, 24, 33, 41], "bond": 3, "connect": 3, "classmethod": [3, 4, 8, 12, 14, 16, 17, 39], "str": [3, 4, 5, 6, 7, 8, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 27, 28, 33, 34, 39, 41, 42], "remove_h": 3, "keep_mol": 3, "xyz_path": 3, "bond_ord": 3, "mol": [3, 4, 5], "modifi": [3, 24, 34, 41], "http": [3, 6, 16, 19, 21, 33, 34], "github": [3, 6, 16, 19, 21, 38], "com": [3, 6, 16, 19, 21, 38], "snap": 3, "stanford": 3, "blob": [3, 6, 12, 21, 39], "745531be13c5403a93c80e21a41848e38ea7637c": 3, "py": [3, 6, 7, 36, 42], "l12": 3, "convert": [3, 4, 5, 6, 7, 8, 10, 12, 17, 26, 28, 33], "rdkit": 3, "graph": [3, 5, 16], "input": [3, 5, 13, 14, 15, 16, 17, 18, 19, 21, 22, 33, 39], "weight_dict": [3, 35], "float": [3, 5, 6, 7, 12, 14, 15, 16, 17, 19, 20, 21, 22, 24, 33, 34, 35, 39, 41], "num_replica": [3, 35], "rank": [3, 12, 20, 21, 23, 24, 33, 35, 39, 41], "seed": [3, 6, 12, 20, 21, 23, 24, 33, 35, 39, 41], "distributedsampl": [3, 35], "mol_pos": 3, "json_text": 3, "pretrainedtoken": [3, 5], "vocab_fil": [4, 8], "merges_fil": [4, 8], "unk_token": [4, 8], "unk": [4, 6, 8], "bos_token": [4, 8], "": [4, 6, 8, 12, 21, 33], "eos_token": [4, 8], "sep_token": [4, 8], "pad_token": [4, 8], "pad": [4, 6, 7, 8, 13, 17, 21, 22, 33, 35, 39], "biogpttoken": [4, 8], "singl": [4, 7, 8, 12, 16, 19, 20, 21, 22, 23, 24, 33, 34, 36, 39, 41, 42], "repres": [4, 14, 15, 16, 18, 24, 33, 39, 41], "mix": [4, 11, 12, 24, 41], "e": [4, 6, 14, 16, 24, 33, 42], "g": [4, 6, 14, 16, 33, 42], "fasta": 4, "etc": 4, "howev": 4, "form": [4, 22, 33], "t": [4, 6, 14, 15, 16, 18, 21, 33, 39], "properti": [4, 5, 12, 16, 17, 19, 24, 33, 34, 39, 42], "sent": [4, 13, 15, 18, 20, 39], "text_token": 4, "entity_token": 4, "max_text_len": [4, 11, 12], "max_entity_len": [4, 11, 12], "return_tupl": 4, "pad_left": 4, "marker_id": 4, "read": [4, 14, 33, 42], "jsonl": 4, "mol_path": [4, 8], "text_path": [4, 8], "show_exampl": 4, "special": [4, 33], "case": [4, 14, 21, 33], "valu": [4, 12, 14, 16, 17, 21, 22, 24, 33, 39, 41, 42], "enum": [4, 12, 24, 33, 41, 42], "enumer": [4, 12, 24, 41], "min_nod": 5, "max_nod": 5, "multi_hop_max_dist": [5, 16, 19], "20": [5, 13], "spatial_pos_max": 5, "32000": [5, 14, 17, 20], "x": [5, 6, 7, 12, 14, 15, 16, 17, 18, 19, 21, 22, 33, 39], "padlen": [5, 6, 7], "padlen1": 5, "padlen2": 5, "padlen3": 5, "dataset_vers": 5, "2d": [5, 33], "5": [5, 15, 16, 33], "ft": [5, 10, 15, 20], "local_rank": [5, 12, 20, 21, 23, 24, 33, 39, 41], "data_sampl": 5, "2": [5, 12, 14, 15, 16, 17, 19, 20, 21, 24, 33, 36, 39], "data_parallel_world_s": 5, "data_parallel_rank": 5, "dataloader_drop_last": 5, "dataset_nam": [5, 10, 15], "dataset_path": 5, "fmproj": 5, "pm6": 5, "86m": 5, "3d": [5, 16, 19, 24, 41, 42], "stage": [5, 24, 33, 41, 42], "inmemorydataset": 5, "self": [5, 16, 19, 21, 33, 42], "folder": 5, "name": [5, 6, 15, 16, 18, 22, 24, 33, 39, 42], "must": [5, 22, 33], "present": 5, "skip": 5, "loader": [5, 24, 33, 41], "dataload": [5, 24, 33, 34, 42], "t_co": 5, "labl": 5, "supervis": 5, "fine": [5, 24, 41], "tune": [5, 24, 41], "data_path": [5, 7, 10, 15], "smiles_dict_path": [5, 10, 13], "mode": [5, 21, 24, 25, 28, 33, 34, 39, 42], "pool_mod": [5, 10, 13], "cl": [5, 6, 14, 16], "embedding_length": [5, 10, 13], "namespac": 5, "mol_size_path": [5, 10, 13], "mol2idx_dict_path": 5, "dataset_split": [5, 10, 13], "in_memori": 5, "dataset_ratio": [5, 10, 13], "model_max_length": [5, 10, 13], "mol_embed_typ": 5, "molecule_max_s": 5, "full": [5, 13, 21], "num_token_id": 5, "32003": 5, "smiless": 5, "pose": 5, "ani": [5, 6, 16, 17, 22, 24, 33, 39, 41, 42], "input_id": [5, 12, 13, 14, 20, 21, 39], "llm_mask": [5, 20], "idx": [5, 33], "mask_ratio": [5, 10, 15], "sourc": 5, "smiles_dict": 5, "nnode": 5, "smiles_str": 5, "po": [5, 16, 19], "list_data": 5, "split": 5, "input_data": 5, "transform": [5, 6, 14, 16, 19, 21, 36], "pre_transform": 5, "root": [5, 16, 19, 21], "pre_filt": 5, "file_path": 5, "xyz_str": 5, "vocab": [6, 7], "overload": [6, 7], "basewrapperdataset": [6, 7], "futur": [6, 7, 21, 40], "chang": [6, 7, 33], "By": [6, 7, 16, 21], "torch": [6, 7, 13, 16, 17, 21, 22, 24, 33, 34, 39], "np": [6, 7, 33], "start": [6, 7, 33], "pad_num": [6, 7], "protein": [6, 35], "inform": [6, 14, 16, 19, 22, 33], "includ": [6, 12, 19, 35], "amino": [6, 35], "acid": [6, 35], "angl": [6, 19], "confid": 6, "score": 6, "ar": [6, 12, 16, 17, 19, 21, 22, 24, 33, 36, 39, 40, 41, 42], "pleas": [6, 33, 38], "ue": 6, "other": [6, 12], "eg": 6, "interfac": [6, 21, 33, 37, 40], "remov": [6, 33], "those": [6, 21, 33], "longer": [6, 33], "specifi": [6, 14, 15, 16, 18, 33, 39], "warn": [6, 33], "don": [6, 21, 33], "updat": [6, 12, 15, 16, 24, 33, 41], "overrid": [6, 13, 16, 33, 39], "method": [6, 16, 19, 21, 22, 24, 28, 33, 34, 35, 39, 42], "child": [6, 33], "origin": [6, 16, 19, 33], "separ": [6, 33], "src": [6, 21, 33], "tgt": [6, 33], "validation_ratio": 6, "03": 6, "sort": 6, "re": [6, 16, 19, 22], "defi": 6, "bstr": 6, "obj": [6, 33], "angle_str": 6, "mask_idx": 6, "standard_tok": 6, "coord_nois": 6, "angle_nois": 6, "lst": 6, "success": 6, "toadd": 6, "l": [6, 17, 22, 33], "v": [6, 12, 16, 19, 35], "r": [6, 33], "d": [6, 16, 19], "p": [6, 22, 33], "k": [6, 12, 16, 19, 22, 33], "q": [6, 12, 16, 19, 22], "f": [6, 33], "y": [6, 33], "m": [6, 12, 19, 22], "h": [6, 12], "w": [6, 33], "c": [6, 14, 15, 16, 18, 21, 39], "b": [6, 14, 15, 16, 18, 33, 39], "u": [6, 36], "z": 6, "o": [6, 36], "prepend_tok": 6, "eo": 6, "append_tok": 6, "mask": [6, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 33, 39], "prepend_bo": 6, "append_eo": 6, "tok": 6, "ind": 6, "inspir": 6, "huggingfac": [6, 21], "master": 6, "tokenization_util": 6, "llamatoken": 7, "padding_idx": 7, "max_len": [7, 19], "dict_path": [7, 10, 20], "home": [7, 12], "peiran": 7, "sfm_framework": [7, 38], "metric": 9, "prefix": [9, 12, 33], "record": 9, "dataclass_obj": 9, "deepfus": [10, 11, 39], "hidden_st": [10, 11, 14, 17, 21, 22, 33, 39], "graphormer_encod": [10, 13], "hybrid_emb": [10, 13], "hybrid_emb_3dmp": [10, 13], "generalistconfig": [10, 13], "btn_adaptor": [10, 13], "llm_model_name_or_path": [10, 13, 28, 36], "loadmfmcheck_path": [10, 13], "mfm_lora": [10, 13, 33, 34], "model_typ": [10, 13, 14, 15, 18, 20], "graphormerllamamodel": [10, 13, 39], "compute_loss": [10, 11, 12, 13, 15, 18, 20, 21, 23, 24, 33, 39, 42], "config_optim": [10, 11, 12, 13, 15, 18, 20, 21, 23, 24, 39, 42], "generate_with_smil": [10, 13, 39], "init_adaptor_config": [10, 13, 39], "init_mp_config": [10, 13, 39], "load_encoder_state_dict": [10, 13, 39], "to_lay": [10, 11, 12, 13, 15, 16, 17, 20, 23, 24, 39], "llamaforcausallmtextmolmask": [10, 13, 39], "llamamodeltextmolmask": [10, 13, 39], "unifieddecod": [10, 15, 18], "graphormer_embed": [10, 15], "graphormer_lay": [10, 15], "graphormer_layers_diff": [10, 15], "graphormer_layers_mp": [10, 15], "graphormer_layers_pp": [10, 15], "graphormer_sentence_encod": [10, 15], "graphormer_sentence_encoder_tmdiff": [10, 15], "graphormer_sentence_encoder_lay": [10, 15], "graphormer_sentence_encoder_layer_mp": [10, 15], "graphormer_sentence_encoder_mp": [10, 15], "graphormer_timestep_encod": [10, 15], "init_state_dict_weight": [10, 15, 18, 39], "upgrade_state_dict_nam": [10, 15, 18, 39], "graphormermodel": [10, 15, 39], "load_pretrained_weight": [10, 15, 18, 39], "max_posit": [10, 15, 18, 39], "graphormerconfig": [10, 15, 18], "act_dropout": [10, 15], "add_3d": [10, 15], "attn_dropout": [10, 15], "d_tild": [10, 15, 16, 19, 22, 34], "ddpm_beta_end": [10, 15], "ddpm_beta_start": [10, 15], "ddpm_schedul": [10, 15], "dropout": [10, 14, 15, 16, 17, 19, 21, 22, 24, 39, 42], "droppath_prob": [10, 15, 16, 19], "encoder_attention_head": [10, 15], "encoder_embed_dim": [10, 15], "encoder_ffn_embed_dim": [10, 15], "encoder_lay": [10, 15], "loadcheck_path": [10, 15], "mode_prob": [10, 15, 16], "no_2d": [10, 15, 16, 19], "noise_mod": [10, 15], "noise_scal": [10, 15], "num_3d_bias_kernel": [10, 15, 16, 19], "num_class": [10, 15, 16], "num_pred_attn_lay": [10, 15, 16, 19], "sandwich_ln": [10, 15, 16, 19], "t_timestep": [10, 15, 16, 19], "transformer_m_pretrain": [10, 15, 16], "base_architectur": [10, 15], "graphormer_base_architectur": [10, 15], "graphormerdiffmodel": [10, 15], "mpllamaconfig": [10, 17], "llamadecoderlayerpp": [10, 17, 36], "llamaembeddingsbas": [10, 17, 20], "emb_weight": [10, 17, 39], "freeze_parital_weight_hook": [10, 17, 39], "llamaembeddingspp": [10, 17], "llamaforcausallmpp": [10, 17], "llamahead": [10, 17], "llamamlpadapt": [10, 17], "llamamemeffattent": [10, 17], "llamamodelpp": [10, 17], "llamanorm": [10, 17], "nummlp": [10, 17], "lm_logit": [10, 17], "fusedllamanorm": [10, 17, 39], "llamadecoderlayermp": [10, 17, 39], "llamaembeddingsmp": [10, 17, 39], "llamaforcausallmtp": [10, 17, 39], "llamaheadmp": [10, 17, 39], "llamamodelmp": [10, 17, 39], "nummlpmp": [10, 17, 39], "parallelllamamlp": [10, 17, 39], "parallelllamamlpadapt": [10, 17, 39], "lm_logits_fn": [10, 17, 39], "pfm_embed": [10, 18], "pfm_encod": [10, 18], "pfm_encoder_lay": [10, 18], "pfm_layer": [10, 18], "timestep_encod": [10, 18], "pfmconfig": [10, 18], "add_rop": [10, 18, 19, 22], "max_num_aa": [10, 18], "num_residu": [10, 18, 19], "scigptconfig": [10, 20, 39], "bos_token_id": [10, 14, 17, 20], "eos_token_id": [10, 14, 17, 20], "hidden_act": [10, 11, 12, 14, 17, 20, 39], "hidden_s": [10, 11, 12, 14, 17, 20, 21, 39], "initializer_rang": [10, 14, 17, 20], "intermediate_s": [10, 11, 12, 14, 17, 20, 39], "learnable_cutoff": [10, 17, 20, 39], "load_ckpt": [10, 12, 16, 17, 20, 21, 23, 24, 26, 28, 36, 39, 41], "max_position_embed": [10, 11, 12, 14, 17, 20], "num_attention_head": [10, 11, 12, 14, 16, 17, 19, 20], "num_hidden_lay": [10, 11, 12, 14, 17, 20], "num_key_value_head": [10, 11, 12, 17, 20, 22], "pretrained_ckpt_path": [10, 20, 36], "pretraining_tp": [10, 11, 12, 17, 20], "rms_norm_ep": [10, 11, 12, 14, 17, 20], "rope_sc": [10, 11, 12, 17, 20], "rope_theta": [10, 11, 12, 20], "tie_word_embed": [10, 14, 17, 20], "tokens_per_sampl": [10, 20], "train_data_path": [10, 20], "use_cach": [10, 12, 14, 17, 20, 21, 33, 39], "valid_data_path": [10, 20], "scigpt_350m_config": [10, 20], "scigpt_7b_config": [10, 20], "scigpt_shallow_config": [10, 20], "scigpt_tiny_config": [10, 20], "scigptembeddingspp": [10, 20], "scigptmodel": [10, 20, 39], "bertattent": [10, 21], "prune_head": [10, 21], "bertembed": [10, 21], "bertencod": [10, 21], "bertformaskedlm": [10, 21], "get_output_embed": [10, 21], "set_output_embed": [10, 21], "bertintermedi": [10, 21], "bertlmheadmodel": [10, 21], "prepare_inputs_for_gener": [10, 11, 12, 21, 39], "bertlmpredictionhead": [10, 21], "bertlay": [10, 21], "feed_forward_chunk": [10, 21], "feed_forward_chunk_queri": [10, 21], "bertmodel": [10, 21], "get_extended_attention_mask": [10, 21], "get_input_embed": [10, 21], "set_input_embed": [10, 21], "bertonlymlmhead": [10, 21], "bertoutput": [10, 21], "bertpool": [10, 21], "bertpretrainedmodel": [10, 21], "base_model_prefix": [10, 21], "config_class": [10, 21], "bertpredictionheadtransform": [10, 21], "bertselfattent": [10, 21], "get_attention_map": [10, 21], "get_attn_gradi": [10, 21], "save_attention_map": [10, 21], "save_attn_gradi": [10, 21], "transpose_for_scor": [10, 21], "bertselfoutput": [10, 21], "before_batch": [10, 21, 23, 24, 39, 42], "emb_text": [10, 21, 39], "freeze_text_model": [10, 11, 12, 21, 39], "init_qform": [10, 21, 39], "init_llama": [10, 21, 39], "init_mol_token": [10, 21, 39], "init_smi_decod": [10, 21, 39], "init_text_token": [10, 21, 39], "tamgent2config": [10, 21, 39], "end_sym": [10, 21, 39], "iters_per_epoch": [10, 11, 12, 21, 39], "llama_model": [10, 11, 12, 21, 39], "low_resourc": [10, 21, 39], "max_txt_len_llama": [10, 21, 39], "max_txt_len_smil": [10, 21, 39], "molxpt_model": [10, 21, 39], "qformer_cross_attention_freq": [10, 21, 39], "qformer_num_attention_head": [10, 21, 39], "qformer_num_lay": [10, 21, 39], "qformer_num_query_token": [10, 21, 39], "text_hidden_s": [10, 21, 39], "train_mol_path": [10, 11, 12, 21, 39], "train_text_path": [10, 11, 12, 21, 39], "val_mol_path": [10, 11, 12, 21, 39], "val_text_path": [10, 11, 12, 21, 39], "linearwarmupcosinelrschedul": [10, 21], "get_lr": [10, 21, 33], "cosine_lr_schedul": [10, 21], "step_lr_schedul": [10, 21], "warmup_lr_schedul": [10, 21], "dataconfig": [11, 12], "data_typ": [11, 12], "decdeepfuseconfig": [11, 12, 39], "activation_dropout": [11, 12, 16, 19], "adapter_activ": [11, 12], "adapter_hidden_s": [11, 12], "attention_probs_dropout_prob": [11, 12], "entity_decoder_model": [11, 12], "entity_decoder_model_typ": [11, 12], "entity_head_dim": [11, 12], "entity_hidden_act": [11, 12], "entity_hidden_s": [11, 12], "entity_intermediate_s": [11, 12], "entity_num_attention_head": [11, 12], "entity_num_hidden_lay": [11, 12], "entity_vocab_s": [11, 12], "finetune_text_extra_emb": [11, 12], "freeze_entity_model": [11, 12], "head_dim": [11, 12, 16, 19], "hidden_dropout_prob": [11, 12, 14], "is_encoder_decod": [11, 12], "layer_usag": [11, 12], "llama_model_typ": [11, 12], "load_from_pretrain": [11, 12, 39], "loss_weight": [11, 12], "new_token_count": [11, 12], "num_adapter_lay": [11, 12], "decdeepfuseinferenceconfig": [11, 12], "ckpt_folder": [11, 12], "decoder_batch_s": [11, 12], "input_fil": [11, 12], "max_new_token": [11, 12], "output_fil": [11, 12], "entitydecodertyp": [11, 12], "biogpt": [11, 12], "llama": [11, 12, 14, 21, 36, 39], "layerusag": [11, 12], "notus": [11, 12], "seper": [11, 12], "from_str": [11, 12], "textdecodertyp": [11, 12], "llama2_7b": [11, 12], "llama2_7b_default_config": [11, 12], "mix_gpt_default_config": [11, 12], "hiddenst": [11, 12, 39], "apply_all_types_map": [11, 12], "devic": [11, 12, 16, 19, 21, 22, 24, 33, 36, 39, 41, 42], "dtype": [11, 12, 16, 19, 21, 22], "from_dens": [11, 12], "seq_len": [11, 12, 17, 33, 39], "to_dens": [11, 12], "to_single_type_batch": [11, 12], "update_single_type_batch": [11, 12], "update_x_dict": [11, 12], "x_dict": [11, 12], "decdeepfusemodel": [11, 12, 27, 39], "can_gener": [11, 12, 39], "extend_emb": [11, 12, 39], "freeze_param": [11, 12, 39], "main_input_nam": [11, 12, 39], "generationoutput": [11, 12, 39], "adapt": [11, 12], "attnoutputproj": [11, 12], "attnqkvproj": [11, 12], "biogptmlp": [11, 12], "deepfuselayerbas": [11, 12], "forward_impl": [11, 12], "map_state_dict": [11, 12], "emb": [11, 12, 39], "fusedattn": [11, 12], "head": [11, 12, 14, 16, 19, 21, 22], "mlp": [11, 12, 14, 16, 19], "mixlay": [11, 12], "seperatelay": [11, 12], "textonli": [11, 12], "make_norm_dict": [11, 12], "mask_to_bool": [11, 12], "mask_to_float": [11, 12], "text2mol": 12, "shufxi": [12, 21, 39], "chebi": [12, 21, 39], "textmol": [12, 21, 39], "smi": [12, 21, 39], "desc": [12, 21, 39], "val": [12, 21, 39], "46": [12, 20, 21, 24, 39, 41], "fp16": [12, 20, 21, 23, 24, 39, 41], "auto_cast": [12, 20, 21, 23, 24, 39, 41], "bf16": [12, 20, 21, 23, 24, 39, 41], "grad_scaler_init": [12, 20, 21, 23, 24, 39, 41], "gradient_accumulation_step": [12, 20, 21, 23, 24, 33, 39, 41], "2048": [12, 14, 17, 20, 21, 24, 39, 41], "train_batch_s": [12, 20, 21, 23, 24, 33, 39, 41], "val_batch_s": [12, 20, 21, 23, 24, 39, 41], "val_batch_interv": [12, 20, 21, 23, 24, 39, 41], "val_batch_log_interv": [12, 20, 21, 23, 24, 39, 41], "val_epoch_interv": [12, 20, 21, 23, 24, 39, 41], "save_dir": [12, 20, 21, 23, 24, 33, 39, 41], "checkpoint": [12, 17, 20, 21, 22, 24, 33, 36, 39, 41], "save_batch_interv": [12, 20, 21, 23, 24, 39, 41], "save_epoch_interv": [12, 20, 21, 23, 24, 39, 41], "log_interv": [12, 20, 21, 23, 24, 39, 41], "10000": [12, 16, 19, 20, 21, 24, 39, 41], "strategi": [12, 20, 21, 23, 24, 35, 36, 37, 39, 41], "trainstrategi": [12, 20, 21, 23, 24, 37, 39, 42], "pp_partition_layer_nam": [12, 20, 21, 23, 24, 39, 41], "pp_part_list": [12, 20, 21, 23, 24, 39, 41], "cpu": [12, 20, 21, 23, 24, 33, 36, 39, 41], "ifresum": [12, 20, 21, 23, 24, 39, 41], "unfreeze_param_list": [12, 20, 21, 23, 24, 39, 41], "finetune_from_checkpoint_dir": [12, 20, 21, 23, 24, 39, 41], "finetune_from_checkpoint_id": [12, 20, 21, 23, 24, 39, 41], "daliload": [12, 20, 21, 23, 24, 39, 41], "dynamic_load": [12, 20, 21, 23, 24, 35, 39, 41], "gradient_clip": [12, 20, 21, 23, 24, 39, 41], "total_num_step": [12, 20, 21, 23, 24, 34, 39, 41], "warmup_num_step": [12, 20, 21, 23, 24, 34, 39, 41], "60": [12, 20, 21, 24, 39, 41], "warmup_factor": [12, 20, 21, 23, 24, 39, 41], "06": [12, 14, 17, 20, 21, 24, 39, 41], "warmup_lr": [12, 20, 21, 23, 24, 39, 41], "1e": [12, 14, 16, 17, 19, 20, 21, 22, 24, 34, 39, 41], "warmup_num_epoch": [12, 20, 21, 23, 24, 39, 41], "10": [12, 20, 21, 24, 33, 34, 39, 41], "max_lr": [12, 20, 21, 23, 24, 34, 39, 41], "0001": [12, 15, 20, 21, 24, 39, 41], "init_lr": [12, 20, 21, 23, 24, 39, 41], "8e": [12, 20, 21, 24, 39, 41], "05": [12, 16, 19, 20, 21, 22, 24, 34, 39, 41], "min_lr": [12, 20, 21, 23, 24, 34, 39, 41], "weight_decai": [12, 20, 21, 23, 24, 34, 39, 41], "total_num_epoch": [12, 20, 21, 23, 24, 39, 41], "100": [12, 20, 21, 24, 39, 41], "wandb": [12, 20, 21, 23, 24, 39, 41], "wandb_team": [12, 20, 21, 23, 24, 39, 41], "wandb_group": [12, 20, 21, 23, 24, 39, 41], "wandb_project": [12, 20, 21, 23, 24, 39, 41], "beta1": [12, 20, 21, 23, 24, 39, 41], "9": [12, 20, 21, 24, 33, 34, 38, 39, 41], "beta2": [12, 20, 21, 23, 24, 39, 41], "999": [12, 20, 21, 24, 34, 39, 41], "08": [12, 20, 21, 24, 34, 39, 41], "node_rank": [12, 20, 21, 23, 24, 39, 41], "pipeline_model_parallel_s": [12, 20, 21, 23, 24, 36, 39, 41], "tensor_model_parallel_s": [12, 20, 21, 23, 24, 36, 39, 41], "deepspeed_config": [12, 20, 21, 23, 24, 33, 39, 41], "dist_backend": [12, 20, 21, 23, 24, 39, 41], "nccl": [12, 20, 21, 24, 39, 41], "hai1": 12, "ds_dataset": 12, "7b": [12, 14, 21, 39], "mixgpt": 12, "mixgpt_new": 12, "ckpt": [12, 33], "64": [12, 16], "nnnn": 12, "ssss": 12, "msss": 12, "factori": 12, "distributedtrainconfig": [12, 20, 21, 23, 24, 39], "static": 12, "reprens": 12, "jadg": 12, "becaus": [12, 33], "have": [12, 21, 22], "differernt": 12, "dim": [12, 16, 19, 22, 33], "fn": [12, 33], "moduledict": 12, "token_typ": 12, "attention_mask": [12, 13, 14, 17, 21, 22, 33, 39], "rag": 12, "bxtxc": 12, "kind": 12, "out": [12, 33, 36], "we": [12, 15, 18, 22, 33, 36, 39], "want": [12, 33], "appli": [12, 14, 16, 19, 22, 33, 35], "layer": [12, 14, 16, 17, 19, 21, 24, 33, 37, 39, 42], "new": [12, 33], "after": [12, 14, 16, 19, 24, 33, 42], "sfmpipelinemodelmixin": [12, 13, 20, 23, 24, 39], "generationmixin": [12, 39], "pred": [12, 13, 21, 24, 39, 42], "lrschedul": [12, 13, 15, 18, 20, 21, 24, 39, 42], "learn": [12, 13, 15, 18, 20, 21, 24, 33, 34, 35, 39, 41, 42], "rate": [12, 13, 15, 18, 20, 21, 24, 33, 34, 39, 41, 42], "return_dict": [12, 21, 39], "output_attent": [12, 14, 17, 21, 33, 39], "output_hidden_st": [12, 21, 39], "past_key_valu": [12, 14, 17, 21, 33, 39], "position_id": [12, 14, 17, 21, 33, 39], "load": [12, 15, 17, 18, 21, 22, 33, 36, 39], "pretrain": [12, 15, 18, 21, 37, 39], "here": [12, 15, 18, 19, 22, 36, 39], "renam": [12, 39], "paramet": [12, 16, 24, 33, 34, 36, 39, 41], "match": [12, 33, 39], "current": [12, 24, 33, 38, 39, 41], "special_token_map": [12, 39], "hidden": [12, 14, 16, 19, 21, 39], "in_dim": [12, 33], "out_dim": [12, 33], "tensortyp": 12, "bia": [12, 15, 16, 18, 19, 22, 33, 39], "abc": [12, 24, 42], "abstract": [12, 17, 21, 24, 42], "llama_state_dict": 12, "entity_layer_id": 12, "entity_decoder_state_dict": 12, "input_tupl": [12, 14, 16, 17, 33, 39], "project": [12, 17, 33, 38, 39], "space": 12, "conduct": 12, "attntion": 12, "Then": 12, "back": 12, "final": [12, 14, 16, 24, 42], "ln": 12, "lm_head": 12, "oper": 12, "both": [12, 16, 19, 22], "own": [12, 16, 19, 22, 33], "booltensor": 12, "graphormersentenceencoderpp": [13, 14], "adaptorconfig": [13, 14], "embedattent": [13, 14], "hybridembed": [13, 14], "hybridembeddingspp": [13, 14], "mlpadapt": [13, 14], "qformerblock": [13, 14], "hybridembeddingsmp": [13, 14], "ckp_list": [13, 16, 17, 39], "languag": [13, 15, 18, 20, 21, 35, 39], "It": [13, 14, 15, 18, 20, 33, 34, 38, 39, 42], "also": [13, 15, 18, 20, 35, 39], "addit": [13, 15, 18, 20, 33, 35, 39], "level": [13, 15, 16, 18, 20, 24, 39, 41, 42], "predict": [13, 14, 15, 16, 18, 19, 20, 21, 39], "loss": [13, 15, 18, 20, 21, 23, 24, 33, 34, 39, 42], "argument": [13, 14, 15, 16, 18, 20, 21, 33, 39, 41, 42], "modeloutput": [13, 15, 18, 20, 21, 23, 24, 39, 42], "batched_data": [13, 15, 16, 18, 19, 33, 39], "perturb": [13, 15, 16, 18, 19, 39], "segment_label": [13, 14, 15, 16, 18, 19, 39], "generate_kwarg": [13, 39], "abl": [13, 39], "condit": [13, 39], "pixel_valu": [13, 39], "num_channel": [13, 39], "height": [13, 39], "width": [13, 16, 19, 39], "imag": [13, 39], "sequence_length": [13, 21, 39], "prompt": [13, 39], "attent": [13, 14, 16, 17, 19, 21, 22, 33, 39], "caption": [13, 39], "num_capt": [13, 39], "llama_config": [13, 39], "graphormer_ckppth": [13, 39], "llama_ckppth": [13, 39], "strict": [13, 17, 22, 33, 39], "llamaforcausallm": [13, 17, 39], "llamaconfig": [13, 14, 17, 20, 33, 39], "llamamodel": [13, 14, 39], "graphormersentenceencod": [14, 15, 16], "implement": [14, 16, 19, 22, 33, 34, 35, 40], "bi": [14, 16], "direct": [14, 16], "bert": [14, 16, 21], "xlm": [14, 16], "style": [14, 16, 33], "pre": [14, 16], "embed": [14, 15, 16, 17, 18, 19, 21, 22, 39], "matrix": [14, 15, 16, 18, 19, 39], "segment": [14, 15, 16, 18, 39], "transformerencoderlay": [14, 16], "intern": [14, 15, 16, 18, 39], "state": [14, 15, 16, 17, 18, 21, 22, 33, 39, 41], "well": [14, 16, 21, 36], "associ": [14, 16], "usual": [14, 16, 33], "follow": [14, 15, 16, 18, 21, 35, 36, 38, 39, 42], "where": [14, 15, 16, 17, 18, 19, 22, 24, 33, 39, 41, 42], "ha": [14, 16, 24, 34, 41], "format": [14, 15, 16, 18, 33, 36, 39], "input_batchdata": [14, 16], "4096": [14, 17, 20, 21, 39], "11008": [14, 17], "silu": [14, 16, 17, 19, 20], "mask_text_to_mol_attent": 14, "mfm_hidden_s": 14, "pretrainedconfig": [14, 21], "configur": [14, 21, 24, 33, 41], "instanti": 14, "architectur": [14, 21], "inherit": 14, "control": [14, 24, 41], "document": 14, "more": [14, 15, 17, 18, 22, 33, 35, 36, 39], "vocabulari": [14, 21], "inputs_id": 14, "non": [14, 33], "linear": [14, 15, 16, 22, 33, 34], "activ": [14, 16, 17, 19, 22, 33, 38, 39], "maximum": [14, 34], "might": [14, 33], "ever": 14, "typic": [14, 22], "someth": 14, "larg": [14, 17, 24, 33, 37, 39, 40, 41, 42], "just": [14, 35, 36], "02": [14, 15, 17, 20], "standard": 14, "deviat": 14, "truncated_normal_initi": 14, "weight": [14, 15, 16, 17, 18, 21, 22, 24, 33, 34, 37, 39, 41], "matric": 14, "12": 14, "epsilon": 14, "rm": 14, "normal": [14, 16, 19], "whether": [14, 17, 24, 33, 34, 39, 41], "last": [14, 21, 33, 34], "relev": 14, "is_decod": [14, 21], "tie": 14, "python": [14, 38], "import": [14, 21, 33, 36, 42], "access": [14, 33], "graphormer_llama_adaptor": 14, "multi": [14, 16, 19, 22], "you": [14, 16, 19, 21, 22, 24, 33, 39, 41, 42], "paper": [14, 22, 34], "q_state": 14, "if_initi": 14, "mol_emb": 14, "mol_padding_mask": 14, "text_emb": 14, "new_num_token": [14, 17, 33, 39], "output_s": [14, 16, 17, 19, 39], "num_lay": [14, 28], "queri": [14, 16, 19, 22, 33], "mpllama_config": 14, "encoderlay": [15, 16, 18, 19], "equivariant2invariantattent": [15, 16, 18, 19], "equivariantattent": [15, 16, 18, 19], "equivariantlayernorm": [15, 16, 18, 19], "covari": [15, 16, 18, 19], "elementwise_linear": [15, 16, 18, 19], "mean_cent": [15, 16, 18, 19], "symsqrtinv": [15, 16, 18, 19], "equivariantselfattent": [15, 16, 18, 19], "equivariantvectoroutput": [15, 16, 18, 19], "gatedequivariantblock": [15, 16, 18, 19], "gaussianlay": [15, 16, 18, 19], "invariant2equivariantattent": [15, 16, 18, 19], "invariantattent": [15, 16, 18, 19], "invariantselfattent": [15, 16, 18, 19], "nodegaussianlay": [15, 16, 18, 19], "nodetaskhead": [15, 16, 18, 19], "nonlinear": [15, 16, 18, 19], "graphormerembeddingmp": [15, 16], "cosinecutoff": [15, 16], "distanc": [15, 16], "equivariantmultiheadattent": [15, 16], "aggreg": [15, 16], "messag": [15, 16, 19, 22], "expnormalsmear": [15, 16], "graph3dbia": [15, 16, 18, 19], "graphattnbia": [15, 16], "graphnodefeatur": [15, 16], "robertaclassificationhead": [15, 16], "init_param": [15, 16, 18, 19], "graph3dbiasdiff": [15, 16], "graphattnbiasdiff": [15, 16], "graphnodefeaturediff": [15, 16], "gaussianlayermp": [15, 16], "graph3dbiasmp": [15, 16], "graphattnbiasmp": [15, 16], "graphnodefeaturemp": [15, 16], "nonlinearmp": [15, 16], "graph3dbiaspip": [15, 16], "graphattnbiaspip": [15, 16], "graphnodefeaturepip": [15, 16], "nodetaskheadpip": [15, 16], "force_proj1": [15, 16], "k_proj": [15, 16], "q_proj": [15, 16], "v_proj": [15, 16], "build_transformer_sentence_encoder_lay": [15, 16, 18, 19], "nodedecod": [15, 16, 18, 19], "init_bert_param": [15, 16], "graphormersentenceencoderdiff": [15, 16], "graphormersentenceencoderlay": [15, 16], "build_fc1": [15, 16, 18, 19], "build_fc2": [15, 16, 18, 19], "build_self_attent": [15, 16, 18, 19], "graphormersentenceencoderlayer_pp": [15, 16], "tensors_encod": [15, 16], "graphormersentenceencoderlayermp": [15, 16], "build_fc1_tp": [15, 16], "build_fc2_tp": [15, 16], "build_self_attention_tp": [15, 16], "graphormerencodermp": [15, 16], "sinusoidalpositionembed": [15, 16, 18, 19], "timestepencod": [15, 16, 18, 19], "masked_token": [15, 18, 39], "unus": [15, 18, 33, 39], "lm": [15, 17, 18, 39], "assum": [15, 18, 39], "correspond": [15, 18, 22, 33, 39], "classification_token": [15, 18, 39], "see": [15, 17, 18, 21, 22, 33, 34, 39], "bert_task": [15, 18, 39], "cross_lingual_lm": [15, 18, 39], "detail": [15, 17, 18, 22, 33, 35, 36, 39, 40], "src_token": [15, 18, 39], "softmax": [15, 18, 22, 39], "dictionari": [15, 18, 33, 39], "pooled_output": [15, 18, 39], "inner_st": [15, 16, 18, 19, 39], "elmo": [15, 18, 39], "sentence_logit": [15, 18, 39], "nsp": [15, 18, 39], "state_dict": [15, 17, 18, 22, 33, 39], "loss_fn": [15, 18, 24, 33, 39], "not_init": [15, 18, 39], "model_output": [15, 18, 20, 39], "checkpoint_path": [15, 18, 39], "768": [15, 16, 19, 20], "24": 15, "6": [15, 16, 33], "const": 15, "4": [15, 21, 33, 35, 39], "hidden_channel": [16, 19], "num_head": [16, 19, 21, 22], "activation_fn": [16, 19], "ffn_embedding_dim": [16, 19], "attention_dropout": [16, 19], "eqi_choic": [16, 19], "gbf_arg": [16, 19], "layer_index": [16, 19], "vec": [16, 19], "attn_bias_iself": [16, 19], "attn_bias_i2": [16, 19], "attn_bias_eself": [16, 19], "attn_bias_e2i": [16, 19], "pos_unit": [16, 19], "attn_bia": [16, 19, 22], "key_padding_mask": [16, 19, 22], "rotation": [16, 19], "equivari": [16, 19], "expect": [16, 19, 21, 33], "extra": [16, 19, 22], "To": [16, 19, 22, 35, 36], "print": [16, 19, 22], "custom": [16, 19, 22, 33], "your": [16, 19, 22, 33, 35], "line": [16, 19, 22], "accept": [16, 19, 22], "invers": [16, 19], "squar": [16, 19, 34], "definit": [16, 19, 33], "pytorch": [16, 19, 33], "issu": [16, 19], "25481": [16, 19], "out_channel": [16, 19], "intermediate_channel": [16, 19], "scalar_activ": [16, 19], "gate": [16, 19], "block": [16, 19, 21, 22, 33], "sch\u00fctt": [16, 19], "et": [16, 19], "al": [16, 19], "2021": [16, 19], "tensori": [16, 19], "molecular": [16, 19], "spectra": [16, 19], "edge_typ": [16, 19], "embed_dim": [16, 17, 19, 22, 33, 39], "delta_po": [16, 19], "embedding_dim": [16, 19], "num_edg": [16, 19], "num_atom": [16, 19], "padding_mask": [16, 17, 19], "mp_config": [16, 22], "init_bia": [16, 19], "embed_scal": [16, 19], "freeze_embed": [16, 19], "n_trans_layers_to_freez": [16, 19], "export": [16, 19, 22], "q_nois": [16, 19, 22], "qn_block_siz": [16, 19, 22], "cutoff_low": 16, "cutoff_upp": 16, "max_num_neighbor": 16, "return_vec": 16, "loop": [16, 40], "num_rbf": 16, "distance_influ": 16, "nn": [16, 17, 21, 22, 33, 39, 42], "attn_activ": 16, "messagepass": 16, "ptr": 16, "dim_siz": 16, "neighbor": 16, "bigoplus_": 16, "j": 16, "mathcal": 16, "which": [16, 21, 22, 24, 33, 35, 41, 42], "wa": [16, 33], "propag": 16, "deleg": 16, "its": [16, 33, 34], "underli": 16, "reduc": [16, 33, 35, 40], "__init__": [16, 42], "aggr": 16, "edge_index": 16, "r_ij": 16, "f_ij": 16, "d_ij": 16, "q_i": 16, "k_j": 16, "v_j": 16, "vec_j": 16, "dk": 16, "dv": 16, "node": [16, 24, 33, 41], "analogi": 16, "phi_": 16, "mathbf": 16, "theta": 16, "edg": 16, "furthermor": 16, "map": [16, 17, 21, 22, 33, 39], "respect": 16, "append": [16, 36], "_i": 16, "_j": 16, "variabl": 16, "x_i": 16, "x_j": 16, "reset": [16, 23, 24, 33], "learnabl": [16, 19], "gamma_": 16, "trainabl": 16, "dist": 16, "1536": [16, 19], "n_layer": [16, 19, 21], "num_kernel": [16, 19], "no_share_rp": 16, "num_spati": [16, 19], "num_edge_di": [16, 19], "hidden_dim": [16, 19, 33], "mask_2d": [16, 19], "no_mask": 16, "num_in_degre": 16, "num_out_degre": 16, "input_dim": [16, 19], "inner_dim": 16, "pooler_dropout": 16, "classif": 16, "1010": [16, 19], "time_embedding_typ": [16, 19], "time_embedding_mlp": [16, 19], "inputs_tupl": 16, "enable_expert_tensor_parallel": [16, 17, 39], "moe": [16, 17, 39], "nl": [16, 19], "last_state_onli": [16, 19], "token_embed": [16, 19], "attn_mask": [16, 19, 22], "specif": [16, 22, 33, 35, 36], "depend": 16, "If": [16, 17, 21, 33, 39, 42], "normal_init_linear_weight": 16, "distribut": [16, 24, 33, 37, 40, 41, 42], "bai": 16, "normal_init_embed_weight": 16, "normal_init_proj_weight": 16, "in_project_weight": 16, "3072": [16, 19], "relu": [16, 19], "init_fn": [16, 19], "self_attn_mask": [16, 19], "output_dim": [16, 19], "self_attent": [16, 22], "self_attn_bia": 16, "self_attn_padding_mask": [16, 19], "either": [16, 19], "befor": [16, 19, 21, 22, 24, 33, 39, 41, 42], "ffn": [16, 19], "layer_id": [16, 17, 39], "max_period": [16, 19], "time": [16, 19, 22, 24, 33, 35, 41], "n_timestep": [16, 19], "timestep_emb_dim": [16, 19], "timestep_emb_typ": [16, 19], "timestep": [16, 19], "init_method": 17, "output_layer_init_method": 17, "embedding_dropout_prob": 17, "seq_length": 17, "rotary_perc": 17, "enable_mem_effici": 17, "llamadecoderlay": [17, 33, 36], "tgt_len": [17, 22, 33, 39], "src_len": [17, 22, 33, 39], "veri": [17, 33, 39], "neg": [17, 33, 39], "under": [17, 33, 39], "speed": [17, 21, 33, 35, 39], "up": [17, 21, 24, 33, 34, 35, 39, 40, 41], "cach": [17, 33, 39], "past": [17, 21, 33, 39], "grad": [17, 19, 39], "llamaattent": 17, "llamapretrainedmodel": [17, 39], "in_featur": [17, 39], "hidden_featur": [17, 39], "out_featur": [17, 39], "act_lay": [17, 39], "drop": [17, 22, 33, 39], "word": [17, 21, 39], "no_lay": [17, 39], "layer_typ": [17, 39], "layertyp": [17, 39], "self_attn_mask_typ": [17, 39], "attnmasktyp": [17, 22, 39], "drop_path_r": [17, 39], "num_expert": [17, 39], "tp_model_s": [17, 22, 39], "tp_rank": [17, 22, 39], "pfmembed": [18, 19], "pfmencod": [18, 19], "pfmencoderlay": [18, 19], "edge3dembed": [18, 19], "graph2dbia": [18, 19], "print_grad": [18, 19], "residuefeatur": [18, 19], "mae": 18, "edge_featur": 19, "node_type_edg": 19, "consid": [19, 33], "msa": 19, "prop_feat": 19, "angle_feat": 19, "residul": 19, "three": 19, "part": 19, "residu": [19, 22], "prior": 19, "3": [19, 21, 22, 33, 34, 38], "34177": 20, "copyright": 21, "2023": 21, "salesforc": 21, "inc": 21, "right": [21, 22, 33], "reserv": 21, "spdx": 21, "licens": 21, "identifi": [21, 33], "bsd": 21, "claus": 21, "For": [21, 22, 24, 33, 36, 38, 39, 42], "txt": 21, "repo": 21, "opensourc": 21, "org": [21, 33, 34], "junnan": 21, "li": 21, "code": [21, 33, 36, 40, 42], "v4": 21, "15": 21, "is_cross_attent": 21, "head_mask": 21, "encoder_hidden_st": 21, "encoder_attention_mask": 21, "query_emb": 21, "past_key_values_length": 21, "query_length": 21, "return_logit": [21, 33], "docstr": 21, "new_embed": 21, "cross": 21, "select": [21, 24, 33, 41], "left": [21, 33], "embed_size_per_head": 21, "user": [21, 33, 37, 40], "decoder_input_id": 21, "berttoken": 21, "bertconfig": 21, "from_pretrain": 21, "hello": 21, "my": 21, "dog": 21, "cute": 21, "return_tensor": 21, "pt": [21, 36], "prediction_logit": 21, "model_kwarg": 21, "layer_num": 21, "attention_output": 21, "add_pooling_lay": 21, "behav": 21, "ad": [21, 34], "between": [21, 24, 33, 41, 42], "describ": [21, 22, 33], "ashish": 21, "vaswani": 21, "noam": 21, "shazeer": 21, "niki": 21, "parmar": 21, "jakob": 21, "uszkoreit": 21, "llion": 21, "jone": 21, "aidan": 21, "gomez": 21, "lukasz": 21, "kaiser": 21, "illia": 21, "polosukhin": 21, "add_cross_attent": 21, "input_shap": 21, "has_queri": 21, "make": [21, 24, 41], "broadcast": 21, "causal": [21, 22], "so": [21, 24, 33, 39, 42], "ones": 21, "attend": 21, "zero": [21, 24, 33, 37, 41, 42], "extend": 21, "sequence_output": 21, "input_tensor": 21, "pretrainedmodel": 21, "handl": [21, 33, 41], "simpl": [21, 33], "alia": 21, "attention_map": 21, "attn_gradi": 21, "do": [21, 24, 33, 39, 42], "some": [21, 24, 33, 36, 39, 42], "eval": [21, 24, 26, 28, 33, 39, 42], "disabl": [21, 24, 33, 39, 42], "molxpt": [21, 39], "500": [21, 39], "max_epoch": 21, "warmup_step": 21, "warmup_start_lr": 21, "_lrschedul": [21, 24, 33, 42], "decai": [21, 24, 34, 41], "decay_r": 21, "max_step": 21, "warmup": [21, 34], "module_nam": 22, "inplac": 22, "retain_dropout": 22, "retain_dropout_modul": 22, "kdim": 22, "vdim": 22, "need_weight": 22, "expand_mask": 22, "outcell_index": 22, "before_softmax": 22, "need_head_weight": 22, "channel": 22, "bytetensor": 22, "exclud": 22, "averag": [22, 34], "over": [22, 33, 34], "prevent": [22, 36], "look": 22, "impli": 22, "prob": 22, "stochast": [22, 34], "depth": 22, "per": 22, "main": [22, 26, 27, 28, 29, 30, 31, 32, 36, 42], "cuda": 22, "stacklevel": 22, "attn_weight": 22, "bsz": 22, "layer_numb": 22, "attn_mask_typ": 22, "megatronmodul": 22, "query_lay": 22, "key_lay": 22, "value_lay": 22, "attention_typ": 22, "attntyp": 22, "self_attn": 22, "key_bia": 22, "query_bia": 22, "value_bia": 22, "output_bia": 22, "encoder_output": 22, "inference_param": 22, "rotary_pos_emb": 22, "n_rep": 22, "embedding_fn": 22, "new_embedding_cutoff": 22, "block_siz": 22, "wrap": [22, 33, 34], "quantiz": 22, "nois": 22, "subsequ": 22, "product": 22, "extrem": 22, "compress": 22, "amount": [22, 35], "ipq": 22, "remark": 22, "wrt": 22, "conv2d": 22, "moment": 22, "how": [22, 35, 40], "convolut": 22, "And": 22, "bit": 22, "goe": 22, "down": 22, "revisit": 22, "neural": 22, "network": 22, "simplest": 22, "consist": 22, "randomli": 22, "_": [22, 33], "__": 22, "co": 22, "sin": 22, "share_embeddings_and_output_weight": 22, "megatron": [22, 36], "extens": 22, "before_epoch": [23, 24], "build_data_load": [23, 24], "grad_scal": [23, 24], "save_checkpoint": [23, 24, 33, 42], "set_up": [23, 24], "sync_valid_loss": [23, 24], "train_step": [23, 24], "valid_step": [23, 24], "ddpacceler": [23, 24], "deepspeedacceler": [23, 24], "get_unfreeze_param_list": [23, 24], "set_ds_config": [23, 24], "groupedbatchit": [23, 24], "singlenodeacceler": [23, 24], "distributedconfig": [23, 24, 37], "log_output": [23, 24], "num_exampl": [23, 24], "trainlogoutput": [23, 24, 42], "extra_output": [23, 24, 42], "global_step": [23, 24, 25, 28, 41], "lr": [23, 24, 33, 34], "ddp": [23, 24, 36, 41, 42], "threed": [23, 24, 36, 41, 42], "zero1": [23, 24, 36, 41, 42], "zero2": [23, 24, 41, 42], "zero3": [23, 24, 33, 41, 42], "trainerconfig": [23, 24, 37, 42], "trainerst": [23, 24, 37], "validlogoutput": [23, 24], "valid_loss": [23, 24], "format_extra_output": [23, 24], "fp16scaler": [23, 24], "check_grad_overflow": [23, 24], "unscale_and_clip_grad": [23, 24], "after_batch": [23, 24, 42], "after_train": [23, 24, 42], "before_train": [23, 24, 42], "sfmpipelinemodul": [23, 24], "logaccumul": [23, 24], "add": [23, 24, 33, 36], "averge_log": [23, 24], "averge_loss": [23, 24], "lossaccumul": [23, 24], "build_acceler": [23, 24, 42], "build_log_output": [23, 24, 42], "finetune_from_checkpoint": [23, 24, 42], "resum": [23, 24, 25, 41, 42], "should_do_batch_valid": [23, 24, 42], "should_do_epoch_valid": [23, 24, 42], "should_log": [23, 24, 42], "should_save_batch_checkpoint": [23, 24, 42], "should_save_epoch_checkpoint": [23, 24, 42], "train_data_load": [23, 24, 42], "valid_data_load": [23, 24, 42], "seed_everyth": [23, 24], "trainer3d": [23, 25], "save_ckp": [23, 25], "train_tensor_pipelin": [23, 25], "make_supervised_data_modul": [23, 25, 26, 28], "train_data": [24, 42], "val_data": 24, "ckpt_dir": [24, 33], "ckpt_id": [24, 25], "trainer_st": 24, "model_states_onli": 24, "extra_st": 24, "total_loss": 24, "grouped_batch_data": 24, "lr_schedul": [24, 33, 42], "valid_data": [24, 28, 42], "loss_log_dict": [24, 33, 42], "unfreeze_param_name_list": 24, "group_siz": 24, "group": [24, 33, 34, 35, 41], "larger": 24, "gradient": [24, 33, 34, 41, 42], "accumul": [24, 33, 41], "serv": [24, 41], "conveni": [24, 33, 41, 42], "wai": [24, 33, 36, 41, 42], "manag": [24, 33, 35, 41], "variou": [24, 33, 41], "effici": [24, 35, 41], "local": [24, 41], "total": [24, 33, 34, 41], "involv": [24, 41], "cluster": [24, 41], "global": [24, 33, 41], "parallel": [24, 33, 37, 40, 41, 42], "deepspe": [24, 33, 41], "backend": [24, 41], "commun": [24, 33, 41], "among": [24, 41], "combin": [24, 41, 42], "provid": [24, 33, 40, 41, 42], "choos": [24, 41, 42], "a4sframework": [24, 36, 38, 39, 40, 41, 42], "without": [24, 41, 42], "replic": [24, 41, 42], "across": [24, 33, 35, 41, 42], "synchron": [24, 41, 42], "redund": [24, 40, 41, 42], "third": [24, 41, 42], "divid": [24, 41, 42], "fashion": [24, 41, 42], "scale": [24, 41, 42], "easier": [24, 41], "relat": [24, 33, 41], "reproduc": [24, 41], "result": [24, 35, 41], "flag": [24, 35, 36, 41], "precis": [24, 41], "factor": [24, 41], "interv": [24, 33, 41], "save": [24, 33, 41], "progress": [24, 33, 41], "dali": [24, 41], "clip": [24, 41], "warm": [24, 34, 41], "phase": [24, 41], "bias": [24, 41], "integr": [24, 41], "hyperparamet": [24, 41], "help": [24, 41], "attribut": [24, 41], "monitor": [24, 41], "count": [24, 41], "entir": [24, 41], "been": [24, 34, 41], "raw_extra_output": 24, "init_scal": 24, "scale_factor": 24, "scale_interv": 24, "param": [24, 34], "clip_grad_norm": 24, "postprocess": [24, 42], "freez": [24, 42], "partition_method": [24, 33], "num_stag": [24, 33], "part_list": [24, 33], "allreduce_fn": 24, "extra_log": 24, "test_data": [24, 42], "freeze_list": [25, 34], "unfreeze_list": [25, 34], "resume_path": 25, "load_state_dict": [26, 27, 33], "calc_precision_and_recal": [26, 28], "get_label": [26, 28], "test_aromat": [26, 28], "test_scor": [26, 28], "testgeneralistconfig": [26, 28], "infer_batch_s": [26, 28], "local_checkpoint_path": [26, 28], "num_gpu": [26, 28], "output_fnam": [26, 28], "question_list_fnam": [26, 28], "remote_checkpoint_query_str": [26, 28], "remote_checkpoint_storage_account": [26, 28], "remote_checkpoint_storage_contain": [26, 28], "smiles_list_fnam": [26, 28], "test_checkpoint_path": [26, 28], "test_global_step": [26, 28], "batch_mol": [26, 28], "create_device_map": [26, 28], "download_model": [26, 28], "get_token": [26, 28], "validate_cl": [26, 28], "validate_cls_mlp": [26, 28], "validate_reg": [26, 28], "validate_reg_multitask": [26, 28], "test_task": [26, 28], "respons": 28, "functional_group": 28, "in_fnam": 28, "num_batch": 28, "in_dir": 28, "out_dir": 28, "convert_offset": 28, "num_llama_lay": 28, "model_path": 28, "local_path": 28, "adamw": [33, 34], "supports_flat_param": [33, 34], "supports_memory_efficient_fp16": [33, 34], "myadam": [33, 34], "process_param": [33, 34], "split_param_and_layer_nam": [33, 34], "groupwarmupdecaylr": [33, 34], "group_param": [33, 34], "group_param_copilot": [33, 34], "mygroupadam": [33, 34], "mymuadam": [33, 34], "process_freeze_param": [33, 34], "process_param_group": [33, 34], "mixin": 33, "receiv": 33, "whenev": 33, "increment": 33, "reus": 33, "epochbatchiter": 33, "regener": 33, "reli": 33, "Will": 33, "begin": 33, "helper": 33, "eos_idx": 33, "left_pad": 33, "move_eos_to_begin": 33, "pad_to_length": 33, "pad_to_multipl": 33, "pad_to_bsz": 33, "1d": 33, "merg": 33, "suitabl": 33, "enforc": 33, "dure": [33, 34, 35, 41], "fetch": 33, "outsid": 33, "worker": 33, "iterabledataset": 33, "sequenti": 33, "being": 33, "stream": 33, "manipul": 33, "machin": 33, "layerdrop": 33, "arxiv": [33, 34], "ab": [33, 34], "1909": 33, "11556": 33, "modulelist": 33, "refresh": 33, "choic": 33, "evalu": 33, "alwai": 33, "usag": [33, 36], "layerdroplist": 33, "layer1": 33, "layer2": 33, "layer3": 33, "probabl": 33, "has_bool_tensor": 33, "model_ckpt_list": 33, "copilot_train": 33, "repeat_dataload": 33, "super_arg": 33, "super_kwarg": 33, "deepspeedengin": 33, "engin": 33, "hybrid": 33, "bfloat16": 33, "11": 33, "complex128": 33, "complex64": 33, "float16": 33, "float32": 33, "float64": 33, "int16": 33, "int32": 33, "int64": 33, "int8": 33, "7": [33, 42], "uint8": 33, "data_it": 33, "reduce_output": 33, "avg": 33, "equival": 33, "no_grad": 33, "entri": 33, "pull": 33, "There": 33, "suffici": 33, "els": 33, "stopiter": 33, "halt": 33, "repeatingload": 33, "restart": 33, "upon": 33, "arithmet": 33, "execut": 33, "instruct": 33, "forc": 33, "occur": 33, "load_dir": 33, "tag": 33, "load_module_strict": 33, "load_optimizer_st": 33, "load_lr_scheduler_st": 33, "load_module_onli": 33, "custom_load_fn": 33, "directori": 33, "uniqu": 33, "attempt": 33, "latest": 33, "boolean": [33, 34], "strictli": 33, "ex": 33, "momentum": [33, 34], "varianc": 33, "warmstart": 33, "load_path": 33, "client_stat": 33, "fail": 33, "client": 33, "cannot": 33, "pristin": 33, "insist": 33, "reiniti": 33, "hack": 33, "revert": 33, "super": [33, 42], "msg": 33, "print_rank": 33, "reset_max": 33, "pipe": 33, "recommend": [33, 38], "buffer": 33, "curriculum": 33, "seqlen": 33, "go": 33, "post": 33, "adjust": 33, "increas": [33, 34], "decreas": [33, 35], "micro": 33, "train_micro_batch_size_per_gpu": 33, "ingest": 33, "unless": 33, "In": [33, 35, 40], "event": 33, "model_paramet": 33, "training_data": 33, "dist_init_requir": 33, "config_param": 33, "field": 33, "json": 33, "what": 33, "unit": 33, "get_": 33, "_parallel_": 33, "auto": 33, "via": 33, "kept": 33, "compat": [33, 36], "training_dataload": 33, "runtim": 33, "suppli": 33, "parser": 33, "argumentpars": 33, "arg_nam": 33, "tp": [33, 36], "field_typ": 33, "cfg_classes_and_func": [33, 42], "seed_lay": 33, "seed_fn": 33, "base_se": 33, "1234": 33, "activation_checkpoint_interv": 33, "activation_checkpoint_func": 33, "checkpointable_lay": 33, "freeze_lay": 33, "constraint": 33, "implicitli": 33, "assumpt": 33, "directli": 33, "fed": 33, "like": [33, 35], "def": [33, 42], "structur": 33, "degre": 33, "ax": 33, "granular": 33, "term": [33, 34], "doe": 33, "forward_input": 33, "typenam": [33, 36], "module_arg": [33, 36], "module_kwarg": [33, 36], "hidden_hidden": 33, "becom": 33, "layer_spec": 33, "except": 33, "error": [33, 36], "forward_fn": 33, "tied_weight_attr": 33, "net": [33, 34], "indent": 33, "dump": 33, "written": [33, 36], "locat": 33, "disk": 33, "open": 33, "serializ": 33, "give": 33, "namedtupl": 33, "capabl": 33, "slice": 33, "row": 33, "wise": 33, "deviceobjtyp": 33, "non_block": 33, "move": 33, "union": 33, "num_pp": 33, "num_dp": 33, "encourag": 33, "high": 33, "bandwidth": 33, "intra": 33, "link": 33, "lower": [33, 35], "volum": 33, "low": 33, "inter": 33, "num_mp": 33, "dimension": 33, "cartesian": 33, "coordin": 33, "grid": 33, "axi": 33, "layout": 33, "major": 33, "would": 33, "adjac": 33, "processcoord": 33, "filter_kwarg": 33, "whose": 33, "criteria": 33, "along": 33, "topo": 33, "member": 33, "coord": 33, "coord_kwarg": 33, "omit_ax": 33, "inner_sep": 33, "outer_sep": 33, "primarili": 33, "a_01": 33, "b_01": 33, "descript": 33, "process_group": 33, "job": 33, "stage_id": 33, "data_parallel_id": 33, "id": 33, "dp_group": 33, "p2p_group": 33, "data_parallel": 33, "around": 33, "appear": 33, "similarli": 33, "resid": 33, "ti": 33, "local_layer_idx": 33, "checkpoints_path": 33, "checkpoint_engin": 33, "layer_idx": 33, "binary_weight": 33, "num_total_lay": 33, "attention_mask_bool": 33, "index_or_kei": 33, "forward_func": 33, "param_dict": 33, "lora": [33, 36], "checkpoints_st": [33, 36], "001": 34, "beta": 34, "amsgrad": 34, "algorithm": 34, "fix": 34, "regular": 34, "1711": 34, "05101": 34, "propos": 34, "coeffici": 34, "denomin": 34, "improv": [34, 35], "numer": 34, "stabil": 34, "l2": 34, "penalti": 34, "variant": 34, "On": [34, 38], "converg": 34, "beyond": 34, "closur": 34, "reevalu": 34, "impl": 34, "apex": 34, "fused_adam": 34, "fusedadam": 34, "name_list": 34, "warmup_min_lr": 34, "warmup_max_lr": 34, "warmup_typ": 34, "last_batch_iter": 34, "decay_typ": 34, "warmuplr": 34, "min": 34, "remain": 34, "minimum": 34, "sgd": 34, "warmupdecaylr": 34, "1000000": 34, "data_load": 34, "rang": 34, "ndim": 34, "ndim2": 34, "adaptoronli": 34, "mut": 34, "techniqu": [35, 36], "deep": 35, "especi": 35, "deal": 35, "vari": 35, "natur": 35, "nlp": 35, "primari": 35, "goal": 35, "enhanc": 35, "overhead": 35, "achiev": 35, "significantli": 35, "consequ": 35, "fewer": 35, "lead": 35, "biolog": 35, "nucleotid": 35, "togeth": 35, "minim": 35, "summari": 35, "valuabl": 35, "domain": 35, "bioinformat": 35, "figur": 35, "_dynamicbatch": [], "show": [35, 40], "signific": 35, "demonstr": 35, "effect": 35, "benefit": [], "inheret": 35, "sfm": [35, 36, 37, 38, 41, 42], "abov": 35, "script": [35, 36, 38, 41], "find": [35, 36], "_dynamicbatching_exampl": [], "bash": [35, 36, 38], "sh": [35, 36, 38], "fsdp": 36, "novel": 36, "allow": [35, 36], "billion": 36, "gpu": [36, 42], "torchrun": [36, 42], "use_env": [36, 42], "leverag": 36, "firstli": 36, "nproc_per_nod": [36, 42], "secondli": 36, "chemic": [36, 37], "mfm": 36, "tool": 36, "memori": 36, "oom": 36, "modeling_llama": 36, "pipe_lay": 36, "join": 36, "mod": 36, "finetun": 36, "ft_graphormer_llama_smil": 36, "pp": 36, "ftmp_graphormer_llama_smil": 36, "overview": 37, "friendli": [37, 40], "decor": 37, "unifi": 37, "spec": 37, "instal": [37, 40], "guid": [37, 39], "linux": 37, "zoo": [37, 40], "search": 37, "page": 37, "intal": 38, "easili": 38, "prepar": 38, "environ": 38, "python3": 38, "virtual": 38, "conda": 38, "virtualenv": 38, "command": 38, "git": 38, "clone": 38, "recurs": 38, "msr": 38, "ai4scienc": 38, "feynman": 38, "cd": 38, "experiment": 38, "install_megatron": 38, "aim": 40, "work": 40, "research": 40, "facilit": 40, "quickli": 40, "our": 40, "through": 40, "clear": 40, "releas": 40, "pip": 40, "develop": 40, "info": 40, "checklist": 41, "seven": 41, "found": 41, "design": [35, 41], "comprehens": 41, "keep": 41, "track": 41, "config1": 42, "config2": 42, "config3": 42, "__name__": 42, "__main__": 42, "shown": 42, "train_dataset": 42, "your_dataset": 42, "eval_dataset": 42, "yourmodel": 42, "your_model": 42, "dynamicbatch": [], "essenti": 35, "contribut": 35, "assign": 35, "balanc": 35}, "objects": {"": [[1, 0, 0, "-", "sfm"]], "sfm": [[2, 0, 0, "-", "criterions"], [3, 0, 0, "-", "data"], [9, 0, 0, "-", "logging"], [10, 0, 0, "-", "models"], [22, 0, 0, "-", "modules"], [23, 0, 0, "-", "pipeline"], [26, 0, 0, "-", "tasks"], [33, 0, 0, "-", "utils"]], "sfm.criterions": [[2, 0, 0, "-", "autoregressive"], [2, 0, 0, "-", "copilotloss"], [2, 0, 0, "-", "l1ft"], [2, 0, 0, "-", "mae2d"], [2, 0, 0, "-", "mae3d"], [2, 0, 0, "-", "mae3ddiff"]], "sfm.criterions.autoregressive": [[2, 1, 1, "", "AutoregressiveCriterion"]], "sfm.criterions.autoregressive.AutoregressiveCriterion": [[2, 2, 1, "", "forward"], [2, 3, 1, "", "training"]], "sfm.criterions.copilotloss": [[2, 1, 1, "", "CopilotCriterions"], [2, 1, 1, "", "CopilotCriterionsNumMP"], [2, 1, 1, "", "CopilotCriterionsNumPP"], [2, 1, 1, "", "CopilotCriterionsPP"]], "sfm.criterions.copilotloss.CopilotCriterions": [[2, 2, 1, "", "forward"], [2, 3, 1, "", "training"]], "sfm.criterions.copilotloss.CopilotCriterionsNumMP": [[2, 2, 1, "", "forward"], [2, 3, 1, "", "training"]], "sfm.criterions.copilotloss.CopilotCriterionsNumPP": [[2, 2, 1, "", "forward"], [2, 3, 1, "", "training"]], "sfm.criterions.copilotloss.CopilotCriterionsPP": [[2, 2, 1, "", "forward"], [2, 3, 1, "", "training"]], "sfm.criterions.l1ft": [[2, 1, 1, "", "BinaryCriterions"], [2, 1, 1, "", "L1Criterions"], [2, 1, 1, "", "L1CriterionsPP"]], "sfm.criterions.l1ft.BinaryCriterions": [[2, 2, 1, "", "forward"], [2, 3, 1, "", "training"]], "sfm.criterions.l1ft.L1Criterions": [[2, 2, 1, "", "forward"], [2, 3, 1, "", "training"]], "sfm.criterions.l1ft.L1CriterionsPP": [[2, 2, 1, "", "forward"], [2, 3, 1, "", "training"]], "sfm.criterions.mae2d": [[2, 1, 1, "", "MAE2d_criterions"]], "sfm.criterions.mae2d.MAE2d_criterions": [[2, 2, 1, "", "forward"], [2, 3, 1, "", "training"]], "sfm.criterions.mae3d": [[2, 1, 1, "", "MAE3dCriterions"], [2, 1, 1, "", "MAE3dCriterionsPP"], [2, 1, 1, "", "ProteinMAE3dCriterions"]], "sfm.criterions.mae3d.MAE3dCriterions": [[2, 2, 1, "", "forward"], [2, 3, 1, "", "training"]], "sfm.criterions.mae3d.MAE3dCriterionsPP": [[2, 2, 1, "", "forward"], [2, 2, 1, "", "tensors_decode"], [2, 3, 1, "", "training"]], "sfm.criterions.mae3d.ProteinMAE3dCriterions": [[2, 2, 1, "", "forward"], [2, 3, 1, "", "training"]], "sfm.criterions.mae3ddiff": [[2, 1, 1, "", "DiffMAE3dCriterions"]], "sfm.criterions.mae3ddiff.DiffMAE3dCriterions": [[2, 2, 1, "", "forward"], [2, 3, 1, "", "training"]], "sfm.data": [[3, 0, 0, "-", "data_utils"], [3, 0, 0, "-", "data_utils_fast"], [3, 0, 0, "-", "dataset"], [4, 0, 0, "-", "dec_data"], [3, 0, 0, "-", "dynamics_loader"], [5, 0, 0, "-", "mol_data"], [3, 0, 0, "-", "molecule"], [6, 0, 0, "-", "prot_data"], [3, 0, 0, "-", "sampler"], [7, 0, 0, "-", "sci_data"], [8, 0, 0, "-", "tamgent2"], [3, 0, 0, "-", "text"]], "sfm.data.data_utils": [[3, 4, 1, "", "batch_by_size"], [3, 4, 1, "", "collect_filtered"]], "sfm.data.data_utils_fast": [[3, 4, 1, "", "batch_by_size_fn"], [3, 4, 1, "", "batch_by_size_vec"], [3, 4, 1, "", "batch_fixed_shapes_fast"]], "sfm.data.dataset": [[3, 1, 1, "", "Batch"], [3, 1, 1, "", "Data"], [3, 1, 1, "", "FoundationModelDataset"], [3, 1, 1, "", "InMemoryFoundationModelDataset"], [3, 1, 1, "", "LMDBFoundationModelDataset"]], "sfm.data.dataset.Batch": [[3, 3, 1, "", "batch_size"]], "sfm.data.dataset.FoundationModelDataset": [[3, 2, 1, "", "batch_by_size"], [3, 2, 1, "", "collate"], [3, 2, 1, "", "get_batch_shapes"], [35, 2, 1, "", "num_tokens"], [3, 2, 1, "", "num_tokens_vec"]], "sfm.data.dec_data": [[4, 0, 0, "-", "SFMDecTokenizer"], [4, 0, 0, "-", "datasets"]], "sfm.data.dec_data.SFMDecTokenizer": [[4, 1, 1, "", "SFMDecTokenizer"]], "sfm.data.dec_data.SFMDecTokenizer.SFMDecTokenizer": [[4, 2, 1, "", "convert_tokens_to_string"]], "sfm.data.dec_data.datasets": [[4, 1, 1, "", "MixedTokenData"], [4, 1, 1, "", "MixedTokenDataset"], [4, 1, 1, "", "TextSpan"], [4, 1, 1, "", "TokenType"]], "sfm.data.dec_data.datasets.MixedTokenData": [[4, 2, 1, "", "from_tuple"], [4, 3, 1, "", "label_seq"], [4, 5, 1, "", "non_padding_mask"], [4, 3, 1, "", "pad_idx"], [4, 2, 1, "", "to_tuple"], [4, 3, 1, "", "token_seq"], [4, 3, 1, "", "token_type_mask"]], "sfm.data.dec_data.datasets.MixedTokenDataset": [[4, 2, 1, "", "collate"], [4, 2, 1, "", "entity_marker_from_entity_id_to_text_id"], [4, 2, 1, "", "from_jsonl"], [4, 2, 1, "", "from_text_to_mol"], [4, 2, 1, "", "init_tokenziers"], [4, 2, 1, "", "pad_sequence"]], "sfm.data.dec_data.datasets.TextSpan": [[4, 3, 1, "", "text"], [4, 3, 1, "", "type"]], "sfm.data.dec_data.datasets.TokenType": [[4, 3, 1, "", "Entity"], [4, 3, 1, "", "Text"]], "sfm.data.dynamics_loader": [[3, 1, 1, "", "DynamicBatchSampler"], [3, 1, 1, "", "DynamicDistributedSampler"]], "sfm.data.dynamics_loader.DynamicBatchSampler": [[3, 2, 1, "", "is_batch_full"]], "sfm.data.dynamics_loader.DynamicDistributedSampler": [[3, 2, 1, "", "set_epoch"], [3, 2, 1, "", "sort_dataset"]], "sfm.data.mol_data": [[5, 0, 0, "-", "algos"], [5, 0, 0, "-", "collator"], [5, 0, 0, "-", "dataset"], [5, 0, 0, "-", "molftdataapi"], [5, 0, 0, "-", "moltext_dataset"], [5, 0, 0, "-", "moltokenizer"], [5, 0, 0, "-", "tdc"], [5, 0, 0, "-", "wrapper"], [5, 0, 0, "-", "xyz2smiles"]], "sfm.data.mol_data.algos": [[5, 4, 1, "", "all_shortest_path_count"], [5, 4, 1, "", "all_shortest_path_count_custom"], [5, 4, 1, "", "floyd_warshall"], [5, 4, 1, "", "gen_edge_input"], [5, 4, 1, "", "gen_edge_input_with_node"], [5, 4, 1, "", "get_all_edges"]], "sfm.data.mol_data.collator": [[5, 4, 1, "", "collator"], [5, 4, 1, "", "collator_3d"], [5, 4, 1, "", "collator_3d_pp"], [5, 4, 1, "", "collator_copilot"], [5, 4, 1, "", "collator_copilot_multi_mol"], [5, 4, 1, "", "collator_copilot_multi_mol_PP"], [5, 4, 1, "", "collator_ft"], [5, 4, 1, "", "pad_1d_unsqueeze"], [5, 4, 1, "", "pad_2d_unsqueeze"], [5, 4, 1, "", "pad_3d_unsqueeze"], [5, 4, 1, "", "pad_attn_bias_unsqueeze"], [5, 4, 1, "", "pad_edge_type_unsqueeze"], [5, 4, 1, "", "pad_pos_unsqueeze"], [5, 4, 1, "", "pad_spatial_pos_unsqueeze"]], "sfm.data.mol_data.dataset": [[5, 1, 1, "", "BatchedDataDataset"], [5, 1, 1, "", "DSDataLoader"], [5, 1, 1, "", "PCQPreprocessedData"], [5, 1, 1, "", "PreprocessSmile"], [5, 1, 1, "", "TargetDataset"], [5, 1, 1, "", "data_prefetcher"], [5, 1, 1, "", "myDataLoader"], [5, 4, 1, "", "smile2data"]], "sfm.data.mol_data.dataset.BatchedDataDataset": [[5, 2, 1, "", "collate"], [5, 2, 1, "", "collater"], [5, 2, 1, "", "collaterft"]], "sfm.data.mol_data.dataset.PCQPreprocessedData": [[5, 2, 1, "", "setup"]], "sfm.data.mol_data.dataset.PreprocessSmile": [[5, 2, 1, "", "process"], [5, 5, 1, "", "processed_file_names"]], "sfm.data.mol_data.dataset.TargetDataset": [[5, 2, 1, "", "collater"]], "sfm.data.mol_data.dataset.data_prefetcher": [[5, 2, 1, "", "next"], [5, 2, 1, "", "preload"]], "sfm.data.mol_data.dataset.myDataLoader": [[5, 3, 1, "", "batch_size"], [5, 3, 1, "", "dataset"], [5, 3, 1, "", "drop_last"], [5, 3, 1, "", "num_workers"], [5, 3, 1, "", "pin_memory"], [5, 3, 1, "", "pin_memory_device"], [5, 3, 1, "", "prefetch_factor"], [5, 3, 1, "", "sampler"], [5, 3, 1, "", "timeout"]], "sfm.data.mol_data.molftdataapi": [[5, 1, 1, "", "MolFTDataAPI"]], "sfm.data.mol_data.molftdataapi.MolFTDataAPI": [[5, 2, 1, "", "collator"], [5, 2, 1, "", "smile2data"]], "sfm.data.mol_data.moltext_dataset": [[5, 1, 1, "", "DataCollatorForSupervisedDataset"], [5, 1, 1, "", "SupervisedMoleculeNetDataset"], [5, 1, 1, "", "SupervisedProcessedData"], [5, 1, 1, "", "SupervisedProcessedDataWithSmiles"], [5, 4, 1, "", "batch_collater_for_graphormer"], [5, 4, 1, "", "collator"], [5, 4, 1, "", "pad_1d_unsqueeze"], [5, 4, 1, "", "pad_2d_unsqueeze"], [5, 4, 1, "", "pad_3d_unsqueeze"], [5, 4, 1, "", "pad_attn_bias_unsqueeze"], [5, 4, 1, "", "pad_edge_type_unsqueeze"], [5, 4, 1, "", "pad_pos_unsqueeze"], [5, 4, 1, "", "pad_spatial_pos_unsqueeze"], [5, 4, 1, "", "preprocess"], [5, 4, 1, "", "preprocess_item"], [5, 4, 1, "", "preprocess_mask"], [5, 4, 1, "", "preprocess_moleculenet"], [5, 4, 1, "", "smiles2graph_removeh"]], "sfm.data.mol_data.moltext_dataset.DataCollatorForSupervisedDataset": [[5, 3, 1, "", "add_mfm"], [5, 3, 1, "", "pad_token_id"], [5, 3, 1, "", "use_pp"]], "sfm.data.mol_data.moltext_dataset.SupervisedMoleculeNetDataset": [[5, 2, 1, "", "collater"]], "sfm.data.mol_data.moltext_dataset.SupervisedProcessedData": [[5, 2, 1, "", "collater"]], "sfm.data.mol_data.moltext_dataset.SupervisedProcessedDataWithSmiles": [[5, 2, 1, "", "collate"], [5, 2, 1, "", "collater"]], "sfm.data.mol_data.moltokenizer": [[5, 1, 1, "", "MolTokenizer"]], "sfm.data.mol_data.moltokenizer.MolTokenizer": [[5, 2, 1, "", "split_text_and_mol"], [5, 2, 1, "", "tokenize"]], "sfm.data.mol_data.tdc": [[5, 1, 1, "", "TDCDataset"], [5, 4, 1, "", "get_rdk_descriptors"], [5, 4, 1, "", "mol2graph"], [5, 4, 1, "", "preprocess_item"], [5, 4, 1, "", "smiles2graph"], [5, 4, 1, "", "smiles2graphpos"]], "sfm.data.mol_data.tdc.TDCDataset": [[5, 2, 1, "", "preprocess"]], "sfm.data.mol_data.wrapper": [[5, 1, 1, "", "MyPygPCQM4MDataset"], [5, 1, 1, "", "MyPygPCQM4MPosDataset"], [5, 1, 1, "", "PM6FullLMDBDataset"], [5, 1, 1, "", "PygPCQM4Mv2Dataset"], [5, 1, 1, "", "PygPCQM4Mv2PosDataset"], [5, 4, 1, "", "mol2graph"], [5, 4, 1, "", "preprocess_item"], [5, 4, 1, "", "smiles2graph"], [5, 4, 1, "", "smiles2graphpos"]], "sfm.data.mol_data.wrapper.MyPygPCQM4MDataset": [[5, 2, 1, "", "download"], [5, 2, 1, "", "process"]], "sfm.data.mol_data.wrapper.MyPygPCQM4MPosDataset": [[5, 2, 1, "", "download"], [5, 2, 1, "", "process"]], "sfm.data.mol_data.wrapper.PM6FullLMDBDataset": [[5, 2, 1, "", "get_idx_split"], [5, 2, 1, "", "get_keys_list"], [5, 2, 1, "", "init_env_list"], [5, 5, 1, "", "msg_dir"], [5, 5, 1, "", "processed_dir"], [5, 5, 1, "", "processed_file_names"], [5, 5, 1, "", "raw_dir"], [5, 5, 1, "", "raw_file_names"], [5, 5, 1, "", "smiles_db_dir"]], "sfm.data.mol_data.wrapper.PygPCQM4Mv2Dataset": [[5, 2, 1, "", "download"], [5, 2, 1, "", "get_idx_split"], [5, 2, 1, "", "process"], [5, 5, 1, "", "processed_file_names"], [5, 5, 1, "", "raw_file_names"]], "sfm.data.mol_data.wrapper.PygPCQM4Mv2PosDataset": [[5, 2, 1, "", "download"], [5, 2, 1, "", "get_idx_split"], [5, 2, 1, "", "process"], [5, 5, 1, "", "processed_file_names"], [5, 5, 1, "", "raw_file_names"]], "sfm.data.mol_data.xyz2smiles": [[5, 4, 1, "", "read_xyz_file"], [5, 4, 1, "", "xyz_to_smiles"]], "sfm.data.molecule": [[3, 1, 1, "", "Molecule"], [3, 4, 1, "", "mol2graph"]], "sfm.data.molecule.Molecule": [[3, 2, 1, "", "from_smiles"], [3, 2, 1, "", "from_xyz_and_bond_index"]], "sfm.data.prot_data": [[6, 0, 0, "-", "collater"], [6, 0, 0, "-", "dataset"], [6, 0, 0, "-", "process"], [6, 0, 0, "-", "sequence_masking"], [6, 0, 0, "-", "spatial_noise"], [6, 0, 0, "-", "structure2lmdb"], [6, 0, 0, "-", "util"], [6, 0, 0, "-", "vocalubary"]], "sfm.data.prot_data.collater": [[6, 4, 1, "", "collate_fn"], [6, 4, 1, "", "pad_1d_unsqueeze"], [6, 4, 1, "", "pad_2d_square_unsqueeze"], [6, 4, 1, "", "pad_2d_unsqueeze"]], "sfm.data.prot_data.dataset": [[6, 1, 1, "", "BatchedDataDataset"], [6, 1, 1, "", "ProteinLMDBDataset"]], "sfm.data.prot_data.dataset.BatchedDataDataset": [[6, 2, 1, "", "collate"], [6, 2, 1, "", "num_tokens"]], "sfm.data.prot_data.dataset.ProteinLMDBDataset": [[6, 2, 1, "", "collate"], [6, 2, 1, "", "filter_indices_by_size"], [6, 2, 1, "", "num_tokens"], [6, 2, 1, "", "num_tokens_vec"], [6, 2, 1, "", "set_default_args"], [6, 2, 1, "", "size"], [6, 2, 1, "", "split_dataset"]], "sfm.data.prot_data.process": [[6, 4, 1, "", "angle_helper"], [6, 4, 1, "", "bstr2obj"], [6, 4, 1, "", "obj2bstr"], [6, 4, 1, "", "process_cif"], [6, 4, 1, "", "process_conf"]], "sfm.data.prot_data.sequence_masking": [[6, 4, 1, "", "BERT_sequence_masking"], [6, 4, 1, "", "no_sequence_masking"], [6, 4, 1, "", "transformerM_masking"]], "sfm.data.prot_data.spatial_noise": [[6, 4, 1, "", "no_noise"], [6, 4, 1, "", "normal_noise"]], "sfm.data.prot_data.structure2lmdb": [[6, 4, 1, "", "chunks"], [6, 4, 1, "", "process_item"]], "sfm.data.prot_data.util": [[6, 4, 1, "", "add_to_env"]], "sfm.data.prot_data.vocalubary": [[6, 1, 1, "", "Alphabet"]], "sfm.data.prot_data.vocalubary.Alphabet": [[6, 2, 1, "", "encode"], [6, 2, 1, "", "feat_idx"], [6, 2, 1, "", "feat_text"], [6, 2, 1, "", "get_idx"], [6, 2, 1, "", "get_tok"], [6, 2, 1, "", "to_dict"], [6, 2, 1, "", "tokenize"]], "sfm.data.sampler": [[35, 1, 1, "", "WeightedDistributedSampler"]], "sfm.data.sci_data": [[7, 0, 0, "-", "SFMDecTokenizer"], [7, 0, 0, "-", "dataset"]], "sfm.data.sci_data.SFMDecTokenizer": [[7, 1, 1, "", "SFMDecTokenizer"]], "sfm.data.sci_data.SFMDecTokenizer.SFMDecTokenizer": [[7, 2, 1, "", "convert_tokens_to_string"]], "sfm.data.sci_data.dataset": [[7, 1, 1, "", "BatchedDataDataset"], [7, 1, 1, "", "ProcessedSciDataset"], [7, 1, 1, "", "SciDataset"], [7, 4, 1, "", "collate_fn"], [7, 4, 1, "", "collate_fn_pp"], [7, 4, 1, "", "pad_1d_unsqueeze"]], "sfm.data.sci_data.dataset.BatchedDataDataset": [[7, 2, 1, "", "collate"], [7, 2, 1, "", "num_tokens"]], "sfm.data.sci_data.dataset.ProcessedSciDataset": [[7, 2, 1, "", "collate"]], "sfm.data.tamgent2": [[8, 0, 0, "-", "datasets"], [8, 0, 0, "-", "tokenizer"]], "sfm.data.tamgent2.datasets": [[8, 1, 1, "", "BatchTextToMolData"], [8, 1, 1, "", "TextToMolData"], [8, 1, 1, "", "TextToMolDataset"]], "sfm.data.tamgent2.datasets.BatchTextToMolData": [[8, 3, 1, "", "smiles"], [8, 3, 1, "", "text"]], "sfm.data.tamgent2.datasets.TextToMolData": [[8, 3, 1, "", "smiles"], [8, 3, 1, "", "text"]], "sfm.data.tamgent2.datasets.TextToMolDataset": [[8, 2, 1, "", "collate"], [8, 2, 1, "", "from_files"]], "sfm.data.tamgent2.tokenizer": [[8, 1, 1, "", "MolxptTokenizer"]], "sfm.data.tamgent2.tokenizer.MolxptTokenizer": [[8, 2, 1, "", "convert_tokens_to_string"]], "sfm.data.text": [[3, 1, 1, "", "MixedText"], [3, 1, 1, "", "Text"]], "sfm.data.text.MixedText": [[3, 2, 1, "", "from_json"]], "sfm.data.text.Text": [[3, 2, 1, "", "from_raw_text"]], "sfm.logging": [[9, 0, 0, "-", "loggers"]], "sfm.logging.loggers": [[9, 1, 1, "", "MetricLogger"], [9, 4, 1, "", "console_log_filter"], [9, 4, 1, "", "dataclass_to_dict"], [9, 4, 1, "", "get_logger"]], "sfm.logging.loggers.MetricLogger": [[9, 2, 1, "", "log"]], "sfm.models.decoder": [[12, 0, 0, "-", "deepfuse"]], "sfm.models.decoder.deepfuse": [[12, 0, 0, "-", "config"], [12, 0, 0, "-", "hidden_state"], [39, 0, 0, "-", "model"], [12, 0, 0, "-", "modules"]], "sfm.models.decoder.deepfuse.config": [[12, 1, 1, "", "DataConfig"], [12, 1, 1, "", "DecDeepFuseConfig"], [12, 1, 1, "", "DecDeepFuseInferenceConfig"], [12, 1, 1, "", "EntityDecoderType"], [12, 1, 1, "", "LayerUsage"], [12, 1, 1, "", "TextDecoderType"], [12, 4, 1, "", "llama2_7b_default_config"], [12, 4, 1, "", "mix_gpt_default_config"]], "sfm.models.decoder.deepfuse.config.DataConfig": [[12, 3, 1, "", "data_type"], [12, 3, 1, "", "train_mol_path"], [12, 3, 1, "", "train_text_path"], [12, 3, 1, "", "val_mol_path"], [12, 3, 1, "", "val_text_path"]], "sfm.models.decoder.deepfuse.config.DecDeepFuseConfig": [[12, 3, 1, "", "activation_dropout"], [12, 3, 1, "", "adapter_activation"], [12, 3, 1, "", "adapter_hidden_size"], [12, 3, 1, "", "attention_probs_dropout_prob"], [12, 3, 1, "", "entity_decoder_model"], [12, 3, 1, "", "entity_decoder_model_type"], [12, 5, 1, "", "entity_head_dim"], [12, 3, 1, "", "entity_hidden_act"], [12, 3, 1, "", "entity_hidden_size"], [12, 3, 1, "", "entity_intermediate_size"], [12, 3, 1, "", "entity_num_attention_heads"], [12, 3, 1, "", "entity_num_hidden_layers"], [12, 3, 1, "", "entity_vocab_size"], [12, 3, 1, "", "finetune_text_extra_emb"], [12, 3, 1, "", "freeze_entity_model"], [12, 3, 1, "", "freeze_text_model"], [12, 5, 1, "", "head_dim"], [12, 3, 1, "", "hidden_act"], [12, 3, 1, "", "hidden_dropout_prob"], [12, 3, 1, "", "hidden_size"], [12, 3, 1, "", "intermediate_size"], [12, 3, 1, "", "is_encoder_decoder"], [12, 3, 1, "", "iters_per_epoch"], [12, 3, 1, "", "layer_usage"], [12, 3, 1, "", "llama_model"], [12, 3, 1, "", "llama_model_type"], [12, 3, 1, "", "load_from_pretrained"], [12, 3, 1, "", "loss_weight"], [12, 3, 1, "", "max_entity_len"], [12, 5, 1, "", "max_position_embeddings"], [12, 3, 1, "", "max_text_len"], [12, 3, 1, "", "new_token_count"], [12, 3, 1, "", "num_adapter_layers"], [12, 3, 1, "", "num_attention_heads"], [12, 3, 1, "", "num_hidden_layers"], [12, 3, 1, "", "num_key_value_heads"], [12, 3, 1, "", "pretraining_tp"], [12, 3, 1, "", "rms_norm_eps"], [12, 3, 1, "", "rope_scaling"], [12, 3, 1, "", "rope_theta"], [12, 3, 1, "", "vocab_size"]], "sfm.models.decoder.deepfuse.config.DecDeepFuseInferenceConfig": [[12, 3, 1, "", "ckpt_folder"], [12, 3, 1, "", "decoder_batch_size"], [12, 3, 1, "", "input_file"], [12, 3, 1, "", "max_length"], [12, 3, 1, "", "max_new_tokens"], [12, 3, 1, "", "output_file"]], "sfm.models.decoder.deepfuse.config.EntityDecoderType": [[12, 3, 1, "", "BioGPT"], [12, 3, 1, "", "LLaMA"]], "sfm.models.decoder.deepfuse.config.LayerUsage": [[12, 3, 1, "", "Mixing"], [12, 3, 1, "", "NotUsed"], [12, 3, 1, "", "Seperate"], [12, 2, 1, "", "from_str"]], "sfm.models.decoder.deepfuse.config.TextDecoderType": [[12, 3, 1, "", "LLaMA2_7B"]], "sfm.models.decoder.deepfuse.hidden_state": [[12, 1, 1, "", "HiddenState"]], "sfm.models.decoder.deepfuse.hidden_state.HiddenState": [[12, 2, 1, "", "apply_all_types_mapping"], [12, 5, 1, "", "batch_size"], [12, 5, 1, "", "device"], [12, 5, 1, "", "dtype"], [12, 2, 1, "", "from_dense"], [12, 2, 1, "", "from_tuple"], [12, 5, 1, "", "logits"], [12, 5, 1, "", "seq_len"], [12, 2, 1, "", "to_dense"], [12, 2, 1, "", "to_single_type_batch"], [12, 2, 1, "", "to_tuple"], [12, 3, 1, "", "token_type_mask"], [12, 2, 1, "", "update_single_type_batch"], [12, 2, 1, "", "update_x_dict"], [12, 3, 1, "", "x_dict"]], "sfm.models.decoder.deepfuse.model": [[39, 1, 1, "", "DecDeepFuseModel"], [39, 1, 1, "", "GenerationOutput"]], "sfm.models.decoder.deepfuse.model.DecDeepFuseModel": [[39, 2, 1, "", "can_generate"], [39, 2, 1, "", "compute_loss"], [39, 2, 1, "", "config_optimizer"], [39, 5, 1, "", "device"], [39, 2, 1, "", "extend_emb"], [39, 2, 1, "", "forward"], [39, 2, 1, "", "freeze_params"], [39, 2, 1, "", "load_from_pretrained"], [39, 5, 1, "", "main_input_name"], [39, 2, 1, "", "prepare_inputs_for_generation"], [39, 2, 1, "", "to_layers"], [39, 3, 1, "", "training"]], "sfm.models.decoder.deepfuse.model.GenerationOutput": [[39, 5, 1, "", "logits"]], "sfm.models.decoder.deepfuse.modules": [[12, 1, 1, "", "Adapter"], [12, 1, 1, "", "AttnOutputProj"], [12, 1, 1, "", "AttnQKVProj"], [12, 1, 1, "", "BioGPTMLP"], [12, 1, 1, "", "DeepFuseLayerBase"], [12, 1, 1, "", "Embed"], [12, 1, 1, "", "FusedAttn"], [12, 1, 1, "", "Head"], [12, 1, 1, "", "MLP"], [12, 1, 1, "", "MixLayer"], [12, 1, 1, "", "SeperateLayer"], [12, 1, 1, "", "TextOnly"], [12, 4, 1, "", "make_norm_dict"], [12, 4, 1, "", "mask_to_bool"], [12, 4, 1, "", "mask_to_float"]], "sfm.models.decoder.deepfuse.modules.Adapter": [[12, 2, 1, "", "forward"], [12, 3, 1, "", "training"]], "sfm.models.decoder.deepfuse.modules.AttnOutputProj": [[12, 2, 1, "", "forward"], [12, 3, 1, "", "training"]], "sfm.models.decoder.deepfuse.modules.AttnQKVProj": [[12, 2, 1, "", "forward"], [12, 3, 1, "", "training"]], "sfm.models.decoder.deepfuse.modules.BioGPTMLP": [[12, 2, 1, "", "forward"], [12, 3, 1, "", "training"]], "sfm.models.decoder.deepfuse.modules.DeepFuseLayerBase": [[12, 2, 1, "", "forward"], [12, 2, 1, "", "forward_impl"], [12, 2, 1, "", "freeze_entity_model"], [12, 2, 1, "", "freeze_text_model"], [12, 2, 1, "", "map_state_dict"], [12, 3, 1, "", "training"]], "sfm.models.decoder.deepfuse.modules.Embed": [[12, 2, 1, "", "forward"], [12, 3, 1, "", "training"]], "sfm.models.decoder.deepfuse.modules.FusedAttn": [[12, 2, 1, "", "forward"], [12, 2, 1, "", "freeze_entity_model"], [12, 2, 1, "", "freeze_text_model"], [12, 3, 1, "", "training"]], "sfm.models.decoder.deepfuse.modules.Head": [[12, 2, 1, "", "forward"], [12, 3, 1, "", "training"]], "sfm.models.decoder.deepfuse.modules.MLP": [[12, 2, 1, "", "forward"], [12, 3, 1, "", "training"]], "sfm.models.decoder.deepfuse.modules.MixLayer": [[12, 2, 1, "", "forward_impl"], [12, 2, 1, "", "freeze_entity_model"], [12, 2, 1, "", "freeze_text_model"], [12, 2, 1, "", "map_state_dict"], [12, 3, 1, "", "training"]], "sfm.models.decoder.deepfuse.modules.SeperateLayer": [[12, 2, 1, "", "forward_impl"], [12, 2, 1, "", "freeze_entity_model"], [12, 2, 1, "", "freeze_text_model"], [12, 2, 1, "", "map_state_dict"], [12, 3, 1, "", "training"]], "sfm.models.decoder.deepfuse.modules.TextOnly": [[12, 2, 1, "", "forward_impl"], [12, 2, 1, "", "freeze_entity_model"], [12, 2, 1, "", "freeze_text_model"], [12, 2, 1, "", "map_state_dict"], [12, 3, 1, "", "training"]], "sfm.models": [[13, 0, 0, "-", "generalist"], [15, 0, 0, "-", "graphormer"], [17, 0, 0, "-", "llama2"], [18, 0, 0, "-", "pfm"], [20, 0, 0, "-", "scigpt"], [21, 0, 0, "-", "tamgent"]], "sfm.models.generalist": [[13, 0, 0, "-", "generalist_config"], [39, 0, 0, "-", "graphormer_llama"], [14, 0, 0, "-", "modules"]], "sfm.models.generalist.generalist_config": [[13, 1, 1, "", "GeneralistConfig"]], "sfm.models.generalist.generalist_config.GeneralistConfig": [[13, 3, 1, "", "btn_adaptor"], [13, 3, 1, "", "dataset_ratios"], [13, 3, 1, "", "dataset_splits"], [13, 3, 1, "", "embedding_length"], [13, 3, 1, "", "llm_model_name_or_path"], [13, 3, 1, "", "loadmfmcheck_path"], [13, 3, 1, "", "mfm_lora"], [13, 3, 1, "", "model_max_length"], [13, 3, 1, "", "model_type"], [13, 3, 1, "", "mol_size_path"], [13, 3, 1, "", "pool_mode"], [13, 3, 1, "", "smiles_dict_path"]], "sfm.models.generalist.graphormer_llama": [[39, 1, 1, "", "GraphormerLlamaModel"], [39, 1, 1, "", "LlamaForCausalLMTextMolMasked"], [39, 1, 1, "", "LlamaModelTextMolMasked"]], "sfm.models.generalist.graphormer_llama.GraphormerLlamaModel": [[39, 2, 1, "", "compute_loss"], [39, 2, 1, "", "config_optimizer"], [39, 2, 1, "", "forward"], [39, 2, 1, "", "generate"], [39, 2, 1, "", "generate_with_smiles"], [39, 2, 1, "", "init_adaptor_config"], [39, 2, 1, "", "init_mp_config"], [39, 2, 1, "", "load_encoder_state_dict"], [39, 2, 1, "", "to_layers"], [39, 3, 1, "", "training"]], "sfm.models.generalist.graphormer_llama.LlamaForCausalLMTextMolMasked": [[39, 3, 1, "", "training"]], "sfm.models.generalist.graphormer_llama.LlamaModelTextMolMasked": [[39, 3, 1, "", "training"]], "sfm.models.generalist.modules": [[14, 0, 0, "-", "graphormer_encoder"], [14, 0, 0, "-", "hybrid_emb"], [14, 0, 0, "-", "hybrid_emb_3dmp"]], "sfm.models.generalist.modules.graphormer_encoder": [[14, 1, 1, "", "GraphormerSentenceEncoderPP"]], "sfm.models.generalist.modules.graphormer_encoder.GraphormerSentenceEncoderPP": [[14, 2, 1, "", "config"], [14, 2, 1, "", "forward"], [14, 3, 1, "", "training"]], "sfm.models.generalist.modules.hybrid_emb": [[14, 1, 1, "", "AdaptorConfig"], [14, 1, 1, "", "EmbedAttention"], [14, 1, 1, "", "HybridEmbeddings"], [14, 1, 1, "", "HybridEmbeddingsPP"], [14, 1, 1, "", "MLPAdapter"], [14, 1, 1, "", "Qformer"], [14, 1, 1, "", "QformerBlock"]], "sfm.models.generalist.modules.hybrid_emb.AdaptorConfig": [[14, 3, 1, "", "model_type"]], "sfm.models.generalist.modules.hybrid_emb.EmbedAttention": [[14, 2, 1, "", "forward"], [14, 3, 1, "", "training"]], "sfm.models.generalist.modules.hybrid_emb.HybridEmbeddings": [[14, 2, 1, "", "forward"], [14, 3, 1, "", "training"]], "sfm.models.generalist.modules.hybrid_emb.HybridEmbeddingsPP": [[14, 2, 1, "", "forward"], [14, 2, 1, "", "resize_token_embeddings"], [14, 3, 1, "", "training"]], "sfm.models.generalist.modules.hybrid_emb.MLPAdapter": [[14, 2, 1, "", "forward"], [14, 3, 1, "", "training"]], "sfm.models.generalist.modules.hybrid_emb.Qformer": [[14, 2, 1, "", "forward"], [14, 3, 1, "", "training"]], "sfm.models.generalist.modules.hybrid_emb.QformerBlock": [[14, 2, 1, "", "forward"], [14, 3, 1, "", "training"]], "sfm.models.generalist.modules.hybrid_emb_3dmp": [[14, 1, 1, "", "HybridEmbeddingsMP"]], "sfm.models.generalist.modules.hybrid_emb_3dmp.HybridEmbeddingsMP": [[14, 2, 1, "", "forward"], [14, 3, 1, "", "training"]], "sfm.models.graphormer": [[39, 0, 0, "-", "graphormer"], [15, 0, 0, "-", "graphormer_config"], [15, 0, 0, "-", "graphormerdiff"], [16, 0, 0, "-", "modules"]], "sfm.models.graphormer.graphormer": [[39, 1, 1, "", "Graphormer"], [39, 1, 1, "", "GraphormerModel"]], "sfm.models.graphormer.graphormer.Graphormer": [[39, 2, 1, "", "forward"], [39, 2, 1, "", "init_state_dict_weight"], [39, 3, 1, "", "training"], [39, 2, 1, "", "upgrade_state_dict_named"]], "sfm.models.graphormer.graphormer.GraphormerModel": [[39, 2, 1, "", "compute_loss"], [39, 2, 1, "", "config_optimizer"], [39, 2, 1, "", "forward"], [39, 2, 1, "", "load_pretrained_weights"], [39, 2, 1, "", "max_positions"], [39, 3, 1, "", "training"]], "sfm.models.graphormer.graphormer_config": [[15, 1, 1, "", "GraphormerConfig"], [15, 4, 1, "", "base_architecture"], [15, 4, 1, "", "graphormer_base_architecture"]], "sfm.models.graphormer.graphormer_config.GraphormerConfig": [[15, 3, 1, "", "act_dropout"], [15, 3, 1, "", "add_3d"], [15, 3, 1, "", "attn_dropout"], [15, 3, 1, "", "d_tilde"], [15, 3, 1, "", "data_path"], [15, 3, 1, "", "dataset_names"], [15, 3, 1, "", "ddpm_beta_end"], [15, 3, 1, "", "ddpm_beta_start"], [15, 3, 1, "", "ddpm_schedule"], [15, 3, 1, "", "dropout"], [15, 3, 1, "", "droppath_prob"], [15, 3, 1, "", "encoder_attention_heads"], [15, 3, 1, "", "encoder_embed_dim"], [15, 3, 1, "", "encoder_ffn_embed_dim"], [15, 3, 1, "", "encoder_layers"], [15, 3, 1, "", "ft"], [15, 3, 1, "", "infer"], [15, 3, 1, "", "loadcheck_path"], [15, 3, 1, "", "mask_ratio"], [15, 3, 1, "", "max_length"], [15, 3, 1, "", "mode_prob"], [15, 3, 1, "", "model_type"], [15, 3, 1, "", "no_2d"], [15, 3, 1, "", "noise_mode"], [15, 3, 1, "", "noise_scale"], [15, 3, 1, "", "num_3d_bias_kernel"], [15, 3, 1, "", "num_classes"], [15, 3, 1, "", "num_pred_attn_layer"], [15, 3, 1, "", "sandwich_ln"], [15, 3, 1, "", "t_timesteps"], [15, 3, 1, "", "transformer_m_pretrain"]], "sfm.models.graphormer.graphormerdiff": [[15, 1, 1, "", "GraphormerDiff"], [15, 1, 1, "", "GraphormerDiffModel"]], "sfm.models.graphormer.graphormerdiff.GraphormerDiff": [[15, 2, 1, "", "forward"], [15, 2, 1, "", "init_state_dict_weight"], [15, 3, 1, "", "training"]], "sfm.models.graphormer.graphormerdiff.GraphormerDiffModel": [[15, 2, 1, "", "compute_loss"], [15, 2, 1, "", "config_optimizer"], [15, 3, 1, "", "training"]], "sfm.models.graphormer.modules": [[16, 0, 0, "-", "UnifiedDecoder"], [16, 0, 0, "-", "graphormer_embedding"], [16, 0, 0, "-", "graphormer_layers"], [16, 0, 0, "-", "graphormer_layers_diff"], [16, 0, 0, "-", "graphormer_layers_mp"], [16, 0, 0, "-", "graphormer_layers_pp"], [16, 0, 0, "-", "graphormer_sentence_encoder"], [16, 0, 0, "-", "graphormer_sentence_encoder_TMdiff"], [16, 0, 0, "-", "graphormer_sentence_encoder_layer"], [16, 0, 0, "-", "graphormer_sentence_encoder_layer_MP"], [16, 0, 0, "-", "graphormer_sentence_encoder_mp"], [16, 0, 0, "-", "graphormer_timestep_encoder"]], "sfm.models.graphormer.modules.UnifiedDecoder": [[16, 1, 1, "", "EncoderLayer"], [16, 1, 1, "", "Equivariant2InvariantAttention"], [16, 1, 1, "", "EquivariantAttention"], [16, 1, 1, "", "EquivariantLayerNorm"], [16, 1, 1, "", "EquivariantSelfAttention"], [16, 1, 1, "", "EquivariantVectorOutput"], [16, 1, 1, "", "GatedEquivariantBlock"], [16, 1, 1, "", "GaussianLayer"], [16, 1, 1, "", "Invariant2EquivariantAttention"], [16, 1, 1, "", "InvariantAttention"], [16, 1, 1, "", "InvariantSelfAttention"], [16, 1, 1, "", "NodeGaussianLayer"], [16, 1, 1, "", "NodeTaskHead"], [16, 1, 1, "", "NonLinear"], [16, 1, 1, "", "UnifiedDecoder"], [16, 4, 1, "", "gelu"]], "sfm.models.graphormer.modules.UnifiedDecoder.EncoderLayer": [[16, 2, 1, "", "forward"], [16, 2, 1, "", "reset_parameters"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.UnifiedDecoder.Equivariant2InvariantAttention": [[16, 2, 1, "", "forward"], [16, 2, 1, "", "reset_parameters"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.UnifiedDecoder.EquivariantAttention": [[16, 2, 1, "", "forward"], [16, 2, 1, "", "reset_parameters"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.UnifiedDecoder.EquivariantLayerNorm": [[16, 2, 1, "", "covariance"], [16, 3, 1, "", "elementwise_linear"], [16, 3, 1, "", "eps"], [16, 2, 1, "", "extra_repr"], [16, 2, 1, "", "forward"], [16, 2, 1, "", "mean_center"], [16, 3, 1, "", "normalized_shape"], [16, 2, 1, "", "reset_parameters"], [16, 2, 1, "", "symsqrtinv"]], "sfm.models.graphormer.modules.UnifiedDecoder.EquivariantSelfAttention": [[16, 2, 1, "", "forward"], [16, 2, 1, "", "reset_parameters"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.UnifiedDecoder.EquivariantVectorOutput": [[16, 2, 1, "", "forward"], [16, 2, 1, "", "reset_parameters"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.UnifiedDecoder.GatedEquivariantBlock": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.UnifiedDecoder.GaussianLayer": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.UnifiedDecoder.Invariant2EquivariantAttention": [[16, 2, 1, "", "forward"], [16, 2, 1, "", "reset_parameters"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.UnifiedDecoder.InvariantAttention": [[16, 2, 1, "", "forward"], [16, 2, 1, "", "reset_parameters"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.UnifiedDecoder.InvariantSelfAttention": [[16, 2, 1, "", "forward"], [16, 2, 1, "", "reset_parameters"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.UnifiedDecoder.NodeGaussianLayer": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.UnifiedDecoder.NodeTaskHead": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.UnifiedDecoder.NonLinear": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.UnifiedDecoder.UnifiedDecoder": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_embedding": [[16, 1, 1, "", "GraphormerEmbeddingMP"]], "sfm.models.graphormer.modules.graphormer_embedding.GraphormerEmbeddingMP": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers": [[16, 1, 1, "", "CosineCutoff"], [16, 1, 1, "", "Distance"], [16, 1, 1, "", "EquivariantLayerNorm"], [16, 1, 1, "", "EquivariantMultiHeadAttention"], [16, 1, 1, "", "EquivariantVectorOutput"], [16, 1, 1, "", "ExpNormalSmearing"], [16, 1, 1, "", "GatedEquivariantBlock"], [16, 1, 1, "", "GaussianLayer"], [16, 1, 1, "", "Graph3DBias"], [16, 1, 1, "", "GraphAttnBias"], [16, 1, 1, "", "GraphNodeFeature"], [16, 1, 1, "", "NodeTaskHead"], [16, 1, 1, "", "NonLinear"], [16, 1, 1, "", "RobertaClassificationHead"], [16, 4, 1, "", "init_params"]], "sfm.models.graphormer.modules.graphormer_layers.CosineCutoff": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers.Distance": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers.EquivariantLayerNorm": [[16, 2, 1, "", "covariance"], [16, 3, 1, "", "elementwise_linear"], [16, 3, 1, "", "eps"], [16, 2, 1, "", "extra_repr"], [16, 2, 1, "", "forward"], [16, 2, 1, "", "mean_center"], [16, 3, 1, "", "normalized_shape"], [16, 2, 1, "", "reset_parameters"], [16, 2, 1, "", "symsqrtinv"]], "sfm.models.graphormer.modules.graphormer_layers.EquivariantMultiHeadAttention": [[16, 2, 1, "", "aggregate"], [16, 2, 1, "", "forward"], [16, 2, 1, "", "message"], [16, 2, 1, "", "reset_parameters"], [16, 2, 1, "", "update"]], "sfm.models.graphormer.modules.graphormer_layers.EquivariantVectorOutput": [[16, 2, 1, "", "forward"], [16, 2, 1, "", "reset_parameters"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers.ExpNormalSmearing": [[16, 2, 1, "", "forward"], [16, 2, 1, "", "reset_parameters"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers.GatedEquivariantBlock": [[16, 2, 1, "", "forward"], [16, 2, 1, "", "reset_parameters"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers.GaussianLayer": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers.Graph3DBias": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers.GraphAttnBias": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers.GraphNodeFeature": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers.NodeTaskHead": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers.NonLinear": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers.RobertaClassificationHead": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers_diff": [[16, 1, 1, "", "Graph3DBiasDiff"], [16, 1, 1, "", "GraphAttnBiasDiff"], [16, 1, 1, "", "GraphNodeFeatureDiff"]], "sfm.models.graphormer.modules.graphormer_layers_diff.Graph3DBiasDiff": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers_diff.GraphAttnBiasDiff": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers_diff.GraphNodeFeatureDiff": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers_mp": [[16, 1, 1, "", "GaussianLayerMP"], [16, 1, 1, "", "Graph3DBiasMP"], [16, 1, 1, "", "GraphAttnBiasMP"], [16, 1, 1, "", "GraphNodeFeatureMP"], [16, 1, 1, "", "NonLinearMP"], [16, 4, 1, "", "init_params"]], "sfm.models.graphormer.modules.graphormer_layers_mp.GaussianLayerMP": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers_mp.Graph3DBiasMP": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers_mp.GraphAttnBiasMP": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers_mp.GraphNodeFeatureMP": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers_mp.NonLinearMP": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers_pp": [[16, 1, 1, "", "GaussianLayer"], [16, 1, 1, "", "Graph3DBiasPipe"], [16, 1, 1, "", "GraphAttnBiasPipe"], [16, 1, 1, "", "GraphNodeFeaturePipe"], [16, 1, 1, "", "NodeTaskHeadPipe"], [16, 1, 1, "", "NonLinear"], [16, 1, 1, "", "RobertaClassificationHead"], [16, 4, 1, "", "init_params"]], "sfm.models.graphormer.modules.graphormer_layers_pp.GaussianLayer": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers_pp.Graph3DBiasPipe": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers_pp.GraphAttnBiasPipe": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers_pp.GraphNodeFeaturePipe": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers_pp.NodeTaskHeadPipe": [[16, 3, 1, "", "force_proj1"], [16, 2, 1, "", "forward"], [16, 3, 1, "", "k_proj"], [16, 3, 1, "", "q_proj"], [16, 3, 1, "", "training"], [16, 3, 1, "", "v_proj"]], "sfm.models.graphormer.modules.graphormer_layers_pp.NonLinear": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_layers_pp.RobertaClassificationHead": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder": [[16, 1, 1, "", "GraphormerSentenceEncoder"], [16, 1, 1, "", "NodeDecoder"], [16, 4, 1, "", "init_bert_params"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder.GraphormerSentenceEncoder": [[16, 2, 1, "", "build_transformer_sentence_encoder_layer"], [16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder.NodeDecoder": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder_TMdiff": [[16, 1, 1, "", "GraphormerSentenceEncoderDiff"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder_TMdiff.GraphormerSentenceEncoderDiff": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder_layer": [[16, 1, 1, "", "GraphormerSentenceEncoderLayer"], [16, 1, 1, "", "GraphormerSentenceEncoderLayer_PP"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer": [[16, 2, 1, "", "build_fc1"], [16, 2, 1, "", "build_fc2"], [16, 2, 1, "", "build_self_attention"], [16, 2, 1, "", "forward"], [16, 2, 1, "", "reset_parameters"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder_layer.GraphormerSentenceEncoderLayer_PP": [[16, 2, 1, "", "config"], [16, 2, 1, "", "forward"], [16, 2, 1, "", "tensors_decode"], [16, 2, 1, "", "tensors_encode"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP": [[16, 1, 1, "", "GraphormerSentenceEncoderLayerMP"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP.GraphormerSentenceEncoderLayerMP": [[16, 2, 1, "", "build_fc1"], [16, 2, 1, "", "build_fc1_TP"], [16, 2, 1, "", "build_fc2"], [16, 2, 1, "", "build_fc2_TP"], [16, 2, 1, "", "build_self_attention_TP"], [16, 2, 1, "", "forward"], [16, 2, 1, "", "reset_parameters"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder_mp": [[16, 1, 1, "", "GraphormerEncoderMP"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder_mp.GraphormerEncoderMP": [[16, 2, 1, "", "to_layers"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_timestep_encoder": [[16, 1, 1, "", "SinusoidalPositionEmbeddings"], [16, 1, 1, "", "TimeStepEncoder"]], "sfm.models.graphormer.modules.graphormer_timestep_encoder.SinusoidalPositionEmbeddings": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.graphormer.modules.graphormer_timestep_encoder.TimeStepEncoder": [[16, 2, 1, "", "forward"], [16, 3, 1, "", "training"]], "sfm.models.llama2": [[17, 0, 0, "-", "convert_llamaweight2pp"], [17, 0, 0, "-", "llama2mp_config"], [17, 0, 0, "-", "llama_modules"], [39, 0, 0, "-", "llama_modules_3dmp"]], "sfm.models.llama2.convert_llamaweight2pp": [[17, 4, 1, "", "convert"]], "sfm.models.llama2.llama2mp_config": [[17, 1, 1, "", "MPLlamaConfig"]], "sfm.models.llama2.llama_modules": [[17, 1, 1, "", "LlamaDecoderLayerPP"], [17, 1, 1, "", "LlamaEmbeddingsBase"], [17, 1, 1, "", "LlamaEmbeddingsPP"], [17, 1, 1, "", "LlamaForCausalLMPP"], [17, 1, 1, "", "LlamaHead"], [17, 1, 1, "", "LlamaMLPAdapter"], [17, 1, 1, "", "LlamaMemEffAttention"], [17, 1, 1, "", "LlamaModelPP"], [17, 1, 1, "", "LlamaNorm"], [17, 1, 1, "", "NumMLP"], [17, 4, 1, "", "lm_logits"]], "sfm.models.llama2.llama_modules.LlamaDecoderLayerPP": [[17, 2, 1, "", "forward"], [17, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules.LlamaEmbeddingsBase": [[17, 5, 1, "", "emb_weight"], [17, 2, 1, "", "forward"], [17, 2, 1, "", "freeze_parital_weight_hook"], [17, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules.LlamaEmbeddingsPP": [[17, 2, 1, "", "forward"], [17, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules.LlamaForCausalLMPP": [[17, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules.LlamaHead": [[17, 5, 1, "", "emb_weight"], [17, 2, 1, "", "forward"], [17, 2, 1, "", "freeze_parital_weight_hook"], [17, 2, 1, "", "resize_token_embeddings"], [17, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules.LlamaMLPAdapter": [[17, 2, 1, "", "forward"], [17, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules.LlamaMemEffAttention": [[17, 2, 1, "", "forward"], [17, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules.LlamaModelPP": [[17, 2, 1, "", "to_layers"], [17, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules.LlamaNorm": [[17, 2, 1, "", "forward"], [17, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules.NumMLP": [[17, 2, 1, "", "forward"], [17, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules_3dmp": [[39, 1, 1, "", "FusedLlamaNorm"], [39, 1, 1, "", "LlamaDecoderLayerMP"], [39, 1, 1, "", "LlamaEmbeddingsMP"], [39, 1, 1, "", "LlamaForCausalLMTP"], [39, 1, 1, "", "LlamaHeadMP"], [39, 1, 1, "", "LlamaModelMP"], [39, 1, 1, "", "NumMLPMP"], [39, 1, 1, "", "ParallelLlamaMLP"], [39, 1, 1, "", "ParallelLlamaMLPAdapter"], [39, 4, 1, "", "lm_logits_fn"]], "sfm.models.llama2.llama_modules_3dmp.FusedLlamaNorm": [[39, 2, 1, "", "forward"], [39, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules_3dmp.LlamaDecoderLayerMP": [[39, 2, 1, "", "auto_partition_load_state_dict"], [39, 2, 1, "", "forward"], [39, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules_3dmp.LlamaEmbeddingsMP": [[39, 2, 1, "", "auto_partition_load_state_dict"], [39, 5, 1, "", "emb_weight"], [39, 2, 1, "", "forward"], [39, 2, 1, "", "freeze_parital_weight_hook"], [39, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules_3dmp.LlamaForCausalLMTP": [[39, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules_3dmp.LlamaHeadMP": [[39, 2, 1, "", "auto_partition_load_state_dict"], [39, 5, 1, "", "emb_weight"], [39, 2, 1, "", "forward"], [39, 2, 1, "", "freeze_parital_weight_hook"], [39, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules_3dmp.LlamaModelMP": [[39, 2, 1, "", "to_layers"], [39, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules_3dmp.NumMLPMP": [[39, 2, 1, "", "forward"], [39, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules_3dmp.ParallelLlamaMLP": [[39, 2, 1, "", "forward"], [39, 3, 1, "", "training"]], "sfm.models.llama2.llama_modules_3dmp.ParallelLlamaMLPAdapter": [[39, 2, 1, "", "forward"], [39, 3, 1, "", "training"]], "sfm.models.pfm": [[19, 0, 0, "-", "modules"], [18, 0, 0, "-", "pfm_config"], [18, 0, 0, "-", "pfmmodel"]], "sfm.models.pfm.modules": [[19, 0, 0, "-", "UnifiedDecoder"], [19, 0, 0, "-", "pfm_embedding"], [19, 0, 0, "-", "pfm_encoder"], [19, 0, 0, "-", "pfm_encoder_layer"], [19, 0, 0, "-", "pfm_layer"], [19, 0, 0, "-", "timestep_encoder"]], "sfm.models.pfm.modules.UnifiedDecoder": [[19, 1, 1, "", "EncoderLayer"], [19, 1, 1, "", "Equivariant2InvariantAttention"], [19, 1, 1, "", "EquivariantAttention"], [19, 1, 1, "", "EquivariantLayerNorm"], [19, 1, 1, "", "EquivariantSelfAttention"], [19, 1, 1, "", "EquivariantVectorOutput"], [19, 1, 1, "", "GatedEquivariantBlock"], [19, 1, 1, "", "GaussianLayer"], [19, 1, 1, "", "Invariant2EquivariantAttention"], [19, 1, 1, "", "InvariantAttention"], [19, 1, 1, "", "InvariantSelfAttention"], [19, 1, 1, "", "NodeGaussianLayer"], [19, 1, 1, "", "NodeTaskHead"], [19, 1, 1, "", "NonLinear"], [19, 1, 1, "", "UnifiedDecoder"], [19, 4, 1, "", "gelu"]], "sfm.models.pfm.modules.UnifiedDecoder.EncoderLayer": [[19, 2, 1, "", "forward"], [19, 2, 1, "", "reset_parameters"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.UnifiedDecoder.Equivariant2InvariantAttention": [[19, 2, 1, "", "forward"], [19, 2, 1, "", "reset_parameters"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.UnifiedDecoder.EquivariantAttention": [[19, 2, 1, "", "forward"], [19, 2, 1, "", "reset_parameters"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.UnifiedDecoder.EquivariantLayerNorm": [[19, 2, 1, "", "covariance"], [19, 3, 1, "", "elementwise_linear"], [19, 3, 1, "", "eps"], [19, 2, 1, "", "extra_repr"], [19, 2, 1, "", "forward"], [19, 2, 1, "", "mean_center"], [19, 3, 1, "", "normalized_shape"], [19, 2, 1, "", "reset_parameters"], [19, 2, 1, "", "symsqrtinv"]], "sfm.models.pfm.modules.UnifiedDecoder.EquivariantSelfAttention": [[19, 2, 1, "", "forward"], [19, 2, 1, "", "reset_parameters"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.UnifiedDecoder.EquivariantVectorOutput": [[19, 2, 1, "", "forward"], [19, 2, 1, "", "reset_parameters"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.UnifiedDecoder.GatedEquivariantBlock": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.UnifiedDecoder.GaussianLayer": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.UnifiedDecoder.Invariant2EquivariantAttention": [[19, 2, 1, "", "forward"], [19, 2, 1, "", "reset_parameters"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.UnifiedDecoder.InvariantAttention": [[19, 2, 1, "", "forward"], [19, 2, 1, "", "reset_parameters"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.UnifiedDecoder.InvariantSelfAttention": [[19, 2, 1, "", "forward"], [19, 2, 1, "", "reset_parameters"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.UnifiedDecoder.NodeGaussianLayer": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.UnifiedDecoder.NodeTaskHead": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.UnifiedDecoder.NonLinear": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.UnifiedDecoder.UnifiedDecoder": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.pfm_embedding": [[19, 1, 1, "", "PFMEmbedding"]], "sfm.models.pfm.modules.pfm_embedding.PFMEmbedding": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.pfm_encoder": [[19, 1, 1, "", "NodeDecoder"], [19, 1, 1, "", "PFMEncoder"], [19, 4, 1, "", "init_params"]], "sfm.models.pfm.modules.pfm_encoder.NodeDecoder": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.pfm_encoder.PFMEncoder": [[19, 2, 1, "", "build_transformer_sentence_encoder_layer"], [19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.pfm_encoder_layer": [[19, 1, 1, "", "PFMEncoderLayer"]], "sfm.models.pfm.modules.pfm_encoder_layer.PFMEncoderLayer": [[19, 2, 1, "", "build_fc1"], [19, 2, 1, "", "build_fc2"], [19, 2, 1, "", "build_self_attention"], [19, 2, 1, "", "forward"], [19, 2, 1, "", "reset_parameters"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.pfm_layer": [[19, 1, 1, "", "Edge3DEmbedding"], [19, 1, 1, "", "GaussianLayer"], [19, 1, 1, "", "Graph2DBias"], [19, 1, 1, "", "Graph3DBias"], [19, 1, 1, "", "NonLinear"], [19, 1, 1, "", "ResidueFeature"], [19, 4, 1, "", "init_params"]], "sfm.models.pfm.modules.pfm_layer.Edge3DEmbedding": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.pfm_layer.GaussianLayer": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.pfm_layer.Graph2DBias": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.pfm_layer.Graph3DBias": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.pfm_layer.NonLinear": [[19, 2, 1, "", "forward"], [19, 2, 1, "", "print_grad"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.pfm_layer.ResidueFeature": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.timestep_encoder": [[19, 1, 1, "", "SinusoidalPositionEmbeddings"], [19, 1, 1, "", "TimeStepEncoder"]], "sfm.models.pfm.modules.timestep_encoder.SinusoidalPositionEmbeddings": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.modules.timestep_encoder.TimeStepEncoder": [[19, 2, 1, "", "forward"], [19, 3, 1, "", "training"]], "sfm.models.pfm.pfm_config": [[18, 1, 1, "", "PFMConfig"]], "sfm.models.pfm.pfm_config.PFMConfig": [[18, 3, 1, "", "add_rope"], [18, 3, 1, "", "max_num_aa"], [18, 3, 1, "", "model_type"], [18, 3, 1, "", "num_residues"], [18, 3, 1, "", "task"]], "sfm.models.pfm.pfmmodel": [[18, 1, 1, "", "PFM"], [18, 1, 1, "", "PFMModel"]], "sfm.models.pfm.pfmmodel.PFM": [[18, 2, 1, "", "forward"], [18, 2, 1, "", "init_state_dict_weight"], [18, 3, 1, "", "training"], [18, 2, 1, "", "upgrade_state_dict_named"]], "sfm.models.pfm.pfmmodel.PFMModel": [[18, 2, 1, "", "compute_loss"], [18, 2, 1, "", "config_optimizer"], [18, 2, 1, "", "forward"], [18, 2, 1, "", "load_pretrained_weights"], [18, 2, 1, "", "max_positions"], [18, 3, 1, "", "training"]], "sfm.models.scigpt": [[20, 0, 0, "-", "config"], [20, 0, 0, "-", "modules"], [39, 0, 0, "-", "scigpt"]], "sfm.models.scigpt.config": [[20, 1, 1, "", "ScigptConfig"], [20, 4, 1, "", "scigpt_350m_config"], [20, 4, 1, "", "scigpt_7b_config"], [20, 4, 1, "", "scigpt_shallow_config"], [20, 4, 1, "", "scigpt_tiny_config"]], "sfm.models.scigpt.config.ScigptConfig": [[20, 3, 1, "", "bos_token_id"], [20, 3, 1, "", "dict_path"], [20, 3, 1, "", "eos_token_id"], [20, 3, 1, "", "ft"], [20, 3, 1, "", "hidden_act"], [20, 3, 1, "", "hidden_size"], [20, 3, 1, "", "infer"], [20, 3, 1, "", "initializer_range"], [20, 3, 1, "", "intermediate_size"], [20, 3, 1, "", "learnable_cutoff"], [20, 3, 1, "", "load_ckpt"], [20, 3, 1, "", "max_position_embeddings"], [20, 3, 1, "", "model_type"], [20, 3, 1, "", "num_attention_heads"], [20, 3, 1, "", "num_hidden_layers"], [20, 3, 1, "", "num_key_value_heads"], [20, 3, 1, "", "pad_token_id"], [20, 3, 1, "", "pretrained_ckpt_path"], [20, 3, 1, "", "pretraining_tp"], [20, 3, 1, "", "rms_norm_eps"], [20, 3, 1, "", "rope_scaling"], [20, 3, 1, "", "rope_theta"], [20, 3, 1, "", "tie_word_embeddings"], [20, 3, 1, "", "tokens_per_sample"], [20, 3, 1, "", "train_data_path"], [20, 3, 1, "", "use_cache"], [20, 3, 1, "", "valid_data_path"], [20, 3, 1, "", "vocab_size"]], "sfm.models.scigpt.modules": [[20, 1, 1, "", "SciGPTEmbeddingsPP"]], "sfm.models.scigpt.modules.SciGPTEmbeddingsPP": [[20, 2, 1, "", "forward"], [20, 3, 1, "", "training"]], "sfm.models.scigpt.scigpt": [[39, 1, 1, "", "ScigptModel"]], "sfm.models.scigpt.scigpt.ScigptModel": [[39, 2, 1, "", "compute_loss"], [39, 2, 1, "", "config_optimizer"], [39, 2, 1, "", "to_layers"], [39, 3, 1, "", "training"]], "sfm.models.tamgent": [[21, 0, 0, "-", "Qformer"], [39, 0, 0, "-", "model"], [21, 0, 0, "-", "scheduler"]], "sfm.models.tamgent.Qformer": [[21, 1, 1, "", "BertAttention"], [21, 1, 1, "", "BertEmbeddings"], [21, 1, 1, "", "BertEncoder"], [21, 1, 1, "", "BertForMaskedLM"], [21, 1, 1, "", "BertIntermediate"], [21, 1, 1, "", "BertLMHeadModel"], [21, 1, 1, "", "BertLMPredictionHead"], [21, 1, 1, "", "BertLayer"], [21, 1, 1, "", "BertModel"], [21, 1, 1, "", "BertOnlyMLMHead"], [21, 1, 1, "", "BertOutput"], [21, 1, 1, "", "BertPooler"], [21, 1, 1, "", "BertPreTrainedModel"], [21, 1, 1, "", "BertPredictionHeadTransform"], [21, 1, 1, "", "BertSelfAttention"], [21, 1, 1, "", "BertSelfOutput"]], "sfm.models.tamgent.Qformer.BertAttention": [[21, 2, 1, "", "forward"], [21, 2, 1, "", "prune_heads"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.Qformer.BertEmbeddings": [[21, 2, 1, "", "forward"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.Qformer.BertEncoder": [[21, 2, 1, "", "forward"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.Qformer.BertForMaskedLM": [[21, 2, 1, "", "forward"], [21, 2, 1, "", "get_output_embeddings"], [21, 2, 1, "", "set_output_embeddings"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.Qformer.BertIntermediate": [[21, 2, 1, "", "forward"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.Qformer.BertLMHeadModel": [[21, 2, 1, "", "forward"], [21, 2, 1, "", "get_output_embeddings"], [21, 2, 1, "", "prepare_inputs_for_generation"], [21, 2, 1, "", "set_output_embeddings"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.Qformer.BertLMPredictionHead": [[21, 2, 1, "", "forward"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.Qformer.BertLayer": [[21, 2, 1, "", "feed_forward_chunk"], [21, 2, 1, "", "feed_forward_chunk_query"], [21, 2, 1, "", "forward"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.Qformer.BertModel": [[21, 2, 1, "", "forward"], [21, 2, 1, "", "get_extended_attention_mask"], [21, 2, 1, "", "get_input_embeddings"], [21, 2, 1, "", "set_input_embeddings"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.Qformer.BertOnlyMLMHead": [[21, 2, 1, "", "forward"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.Qformer.BertOutput": [[21, 2, 1, "", "forward"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.Qformer.BertPooler": [[21, 2, 1, "", "forward"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.Qformer.BertPreTrainedModel": [[21, 3, 1, "", "base_model_prefix"], [21, 3, 1, "", "config_class"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.Qformer.BertPredictionHeadTransform": [[21, 2, 1, "", "forward"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.Qformer.BertSelfAttention": [[21, 2, 1, "", "forward"], [21, 2, 1, "", "get_attention_map"], [21, 2, 1, "", "get_attn_gradients"], [21, 2, 1, "", "save_attention_map"], [21, 2, 1, "", "save_attn_gradients"], [21, 3, 1, "", "training"], [21, 2, 1, "", "transpose_for_scores"]], "sfm.models.tamgent.Qformer.BertSelfOutput": [[21, 2, 1, "", "forward"], [21, 3, 1, "", "training"]], "sfm.models.tamgent.model": [[39, 1, 1, "", "Tamgent2"], [39, 1, 1, "", "Tamgent2Config"]], "sfm.models.tamgent.model.Tamgent2": [[39, 2, 1, "", "before_batch"], [39, 2, 1, "", "compute_loss"], [39, 2, 1, "", "config_optimizer"], [39, 2, 1, "", "emb_text"], [39, 2, 1, "", "forward"], [39, 2, 1, "", "freeze_text_model"], [39, 2, 1, "", "init_Qformer"], [39, 2, 1, "", "init_llama"], [39, 2, 1, "", "init_mol_tokenizer"], [39, 2, 1, "", "init_smi_decoder"], [39, 2, 1, "", "init_text_tokenizer"], [39, 3, 1, "", "training"]], "sfm.models.tamgent.model.Tamgent2Config": [[39, 3, 1, "", "end_sym"], [39, 3, 1, "", "freeze_text_model"], [39, 3, 1, "", "iters_per_epoch"], [39, 3, 1, "", "llama_model"], [39, 3, 1, "", "low_resource"], [39, 3, 1, "", "max_txt_len_llama"], [39, 3, 1, "", "max_txt_len_smiles"], [39, 3, 1, "", "molxpt_model"], [39, 3, 1, "", "qformer_cross_attention_freq"], [39, 3, 1, "", "qformer_num_attention_heads"], [39, 3, 1, "", "qformer_num_layer"], [39, 3, 1, "", "qformer_num_query_token"], [39, 3, 1, "", "text_hidden_size"], [39, 3, 1, "", "train_mol_path"], [39, 3, 1, "", "train_text_path"], [39, 3, 1, "", "val_mol_path"], [39, 3, 1, "", "val_text_path"]], "sfm.models.tamgent.scheduler": [[21, 1, 1, "", "LinearWarmupCosineLRScheduler"], [21, 4, 1, "", "cosine_lr_schedule"], [21, 4, 1, "", "step_lr_schedule"], [21, 4, 1, "", "warmup_lr_schedule"]], "sfm.models.tamgent.scheduler.LinearWarmupCosineLRScheduler": [[21, 2, 1, "", "get_lr"]], "sfm.modules": [[22, 0, 0, "-", "FairseqDropout"], [22, 0, 0, "-", "FlashAtt_bk"], [22, 0, 0, "-", "droppath"], [22, 0, 0, "-", "fused_layernorm"], [22, 0, 0, "-", "get_activation_fn"], [22, 0, 0, "-", "layer_norm"], [22, 0, 0, "-", "mem_eff_attn"], [22, 0, 0, "-", "multihead_attention"], [22, 0, 0, "-", "parallelattentionbias"], [22, 0, 0, "-", "partial_grad_emb"], [22, 0, 0, "-", "quant_noise"], [22, 0, 0, "-", "rotary_embedding"], [22, 0, 0, "-", "sfmmodule"]], "sfm.modules.FairseqDropout": [[22, 1, 1, "", "FairseqDropout"]], "sfm.modules.FairseqDropout.FairseqDropout": [[22, 2, 1, "", "forward"], [22, 2, 1, "", "make_generation_fast_"], [22, 3, 1, "", "training"]], "sfm.modules.FlashAtt_bk": [[22, 1, 1, "", "FlashAtt"], [22, 4, 1, "", "init_to_zero"]], "sfm.modules.FlashAtt_bk.FlashAtt": [[22, 2, 1, "", "forward"], [22, 2, 1, "", "prepare_for_onnx_export_"], [22, 2, 1, "", "reset_parameters"], [22, 3, 1, "", "training"]], "sfm.modules.droppath": [[22, 1, 1, "", "DropPath"]], "sfm.modules.droppath.DropPath": [[22, 2, 1, "", "extra_repr"], [22, 2, 1, "", "forward"], [22, 3, 1, "", "training"]], "sfm.modules.fused_layernorm": [[22, 1, 1, "", "fusedLayerNorm"], [22, 4, 1, "", "test_layer_norm"]], "sfm.modules.fused_layernorm.fusedLayerNorm": [[22, 3, 1, "", "elementwise_affine"], [22, 3, 1, "", "eps"], [22, 2, 1, "", "forward"], [22, 3, 1, "", "normalized_shape"]], "sfm.modules.get_activation_fn": [[22, 4, 1, "", "deprecation_warning"], [22, 4, 1, "", "gelu"], [22, 4, 1, "", "gelu_accurate"], [22, 4, 1, "", "get_activation_fn"], [22, 4, 1, "", "get_available_activation_fns"], [22, 4, 1, "", "relu_squared"]], "sfm.modules.layer_norm": [[22, 1, 1, "", "Fp32LayerNorm"], [22, 1, 1, "", "FusedLayerNorm"], [22, 4, 1, "", "LayerNorm"]], "sfm.modules.layer_norm.Fp32LayerNorm": [[22, 3, 1, "", "elementwise_affine"], [22, 3, 1, "", "eps"], [22, 2, 1, "", "forward"], [22, 3, 1, "", "normalized_shape"]], "sfm.modules.layer_norm.FusedLayerNorm": [[22, 2, 1, "", "forward"], [22, 3, 1, "", "training"]], "sfm.modules.mem_eff_attn": [[22, 1, 1, "", "MemEffAttn"]], "sfm.modules.mem_eff_attn.MemEffAttn": [[22, 2, 1, "", "apply_sparse_mask"], [22, 2, 1, "", "forward"], [22, 2, 1, "", "prepare_for_onnx_export_"], [22, 2, 1, "", "reset_parameters"], [22, 3, 1, "", "training"]], "sfm.modules.multihead_attention": [[22, 1, 1, "", "MultiheadAttention"]], "sfm.modules.multihead_attention.MultiheadAttention": [[22, 2, 1, "", "apply_sparse_mask"], [22, 2, 1, "", "forward"], [22, 2, 1, "", "prepare_for_onnx_export_"], [22, 2, 1, "", "reset_parameters"], [22, 3, 1, "", "training"]], "sfm.modules.parallelattentionbias": [[22, 1, 1, "", "CoreAttentionBias"], [22, 1, 1, "", "TPMultiheadAttention"]], "sfm.modules.parallelattentionbias.CoreAttentionBias": [[22, 2, 1, "", "forward"], [22, 3, 1, "", "training"]], "sfm.modules.parallelattentionbias.TPMultiheadAttention": [[22, 2, 1, "", "forward"], [22, 2, 1, "", "repeat_kv"], [22, 3, 1, "", "training"]], "sfm.modules.partial_grad_emb": [[22, 1, 1, "", "PartialGradEmbedding"]], "sfm.modules.partial_grad_emb.PartialGradEmbedding": [[22, 2, 1, "", "forward"], [22, 3, 1, "", "training"]], "sfm.modules.quant_noise": [[22, 4, 1, "", "quant_noise"]], "sfm.modules.rotary_embedding": [[22, 1, 1, "", "RotaryEmbedding"], [22, 4, 1, "", "apply_rotary_pos_emb"], [22, 4, 1, "", "rotate_half"]], "sfm.modules.rotary_embedding.RotaryEmbedding": [[22, 2, 1, "", "forward"], [22, 3, 1, "", "training"]], "sfm.modules.sfmmodule": [[22, 1, 1, "", "SFMModule"]], "sfm.modules.sfmmodule.SFMModule": [[22, 2, 1, "", "auto_partition_load_state_dict"], [22, 3, 1, "", "training"]], "sfm.pipeline": [[24, 0, 0, "-", "accelerator"], [25, 0, 0, "-", "generalist"]], "sfm.pipeline.accelerator": [[24, 0, 0, "-", "accelerator"], [24, 0, 0, "-", "dataclasses"], [24, 0, 0, "-", "fp16_scaler"], [24, 0, 0, "-", "model"], [24, 0, 0, "-", "pipeline_module"], [24, 0, 0, "-", "trainer"]], "sfm.pipeline.accelerator.accelerator": [[24, 1, 1, "", "Accelerator"], [24, 1, 1, "", "DdpAccelerator"], [24, 1, 1, "", "DeepSpeedAccelerator"], [24, 1, 1, "", "GroupedBatchIter"], [24, 1, 1, "", "SingleNodeAccelerator"]], "sfm.pipeline.accelerator.accelerator.Accelerator": [[24, 2, 1, "", "barrier"], [24, 2, 1, "", "before_epoch"], [24, 2, 1, "", "build_data_loader"], [24, 5, 1, "", "grad_scale"], [24, 2, 1, "", "load_checkpoint"], [24, 2, 1, "", "save_checkpoint"], [24, 2, 1, "", "set_up"], [24, 2, 1, "", "sync_valid_loss"], [24, 2, 1, "", "train_step"], [24, 2, 1, "", "valid_step"]], "sfm.pipeline.accelerator.accelerator.DdpAccelerator": [[24, 2, 1, "", "barrier"], [24, 2, 1, "", "before_epoch"], [24, 2, 1, "", "build_data_loader"], [24, 2, 1, "", "save_checkpoint"], [24, 2, 1, "", "set_up"], [24, 2, 1, "", "sync_valid_loss"], [24, 2, 1, "", "train_step"]], "sfm.pipeline.accelerator.accelerator.DeepSpeedAccelerator": [[24, 2, 1, "", "barrier"], [24, 2, 1, "", "before_epoch"], [24, 2, 1, "", "build_data_loader"], [24, 2, 1, "", "get_unfreeze_param_list"], [24, 5, 1, "", "grad_scale"], [24, 2, 1, "", "load_checkpoint"], [24, 2, 1, "", "save_checkpoint"], [24, 2, 1, "", "set_ds_config"], [24, 2, 1, "", "set_up"], [24, 2, 1, "", "sync_valid_loss"], [24, 2, 1, "", "train_step"], [24, 2, 1, "", "valid_step"], [24, 5, 1, "", "world_size"]], "sfm.pipeline.accelerator.accelerator.SingleNodeAccelerator": [[24, 2, 1, "", "barrier"], [24, 2, 1, "", "build_data_loader"], [24, 5, 1, "", "grad_scale"], [24, 2, 1, "", "load_checkpoint"], [24, 2, 1, "", "save_checkpoint"], [24, 2, 1, "", "set_up"], [24, 2, 1, "", "sync_valid_loss"], [24, 2, 1, "", "train_step"], [24, 2, 1, "", "valid_step"]], "sfm.pipeline.accelerator.dataclasses": [[41, 1, 1, "", "DistributedConfig"], [24, 1, 1, "", "DistributedTrainConfig"], [24, 1, 1, "", "ModelOutput"], [24, 1, 1, "", "TrainLogOutput"], [42, 1, 1, "", "TrainStrategy"], [41, 1, 1, "", "TrainerConfig"], [41, 1, 1, "", "TrainerState"], [24, 1, 1, "", "ValidLogOutput"], [24, 4, 1, "", "format_extra_output"]], "sfm.pipeline.accelerator.dataclasses.DistributedConfig": [[41, 3, 1, "", "deepspeed_config"], [41, 3, 1, "", "dist_backend"], [41, 3, 1, "", "local_rank"], [41, 3, 1, "", "node_rank"], [41, 3, 1, "", "pipeline_model_parallel_size"], [41, 3, 1, "", "rank"], [41, 3, 1, "", "tensor_model_parallel_size"], [41, 3, 1, "", "world_size"]], "sfm.pipeline.accelerator.dataclasses.DistributedTrainConfig": [[24, 3, 1, "", "deepspeed_config"], [24, 3, 1, "", "dist_backend"], [24, 3, 1, "", "local_rank"], [24, 3, 1, "", "node_rank"], [24, 3, 1, "", "pipeline_model_parallel_size"], [24, 3, 1, "", "rank"], [24, 3, 1, "", "tensor_model_parallel_size"], [24, 3, 1, "", "world_size"]], "sfm.pipeline.accelerator.dataclasses.ModelOutput": [[24, 3, 1, "", "log_output"], [24, 3, 1, "", "loss"], [24, 3, 1, "", "num_examples"]], "sfm.pipeline.accelerator.dataclasses.TrainLogOutput": [[24, 3, 1, "", "batch"], [24, 3, 1, "", "epoch"], [24, 3, 1, "", "extra_output"], [24, 3, 1, "", "global_step"], [24, 3, 1, "", "grad_scale"], [24, 3, 1, "", "loss"], [24, 3, 1, "", "lr"]], "sfm.pipeline.accelerator.dataclasses.TrainStrategy": [[42, 3, 1, "", "DDP"], [42, 3, 1, "", "Pipeline"], [42, 3, 1, "", "Single"], [42, 3, 1, "", "ThreeD"], [42, 3, 1, "", "Zero1"], [42, 3, 1, "", "Zero2"], [42, 3, 1, "", "Zero3"]], "sfm.pipeline.accelerator.dataclasses.TrainerConfig": [[41, 3, 1, "", "auto_cast"], [41, 3, 1, "", "beta1"], [41, 3, 1, "", "beta2"], [41, 3, 1, "", "bf16"], [41, 3, 1, "", "cpu"], [41, 3, 1, "", "daliLoader"], [41, 3, 1, "", "dynamic_loader"], [41, 3, 1, "", "eps"], [41, 3, 1, "", "finetune_from_checkpoint_dir"], [41, 3, 1, "", "finetune_from_checkpoint_id"], [41, 3, 1, "", "fp16"], [41, 3, 1, "", "grad_scaler_init"], [41, 3, 1, "", "gradient_accumulation_steps"], [41, 3, 1, "", "gradient_clipping"], [41, 3, 1, "", "ifresume"], [41, 3, 1, "", "init_lr"], [41, 3, 1, "", "load_ckpt"], [41, 3, 1, "", "log_interval"], [41, 3, 1, "", "max_lr"], [41, 3, 1, "", "max_tokens"], [41, 3, 1, "", "min_lr"], [41, 3, 1, "", "pp_part_list"], [41, 3, 1, "", "pp_partition_layer_name"], [41, 3, 1, "", "save_batch_interval"], [41, 3, 1, "", "save_dir"], [41, 3, 1, "", "save_epoch_interval"], [41, 3, 1, "", "seed"], [41, 3, 1, "", "strategy"], [41, 3, 1, "", "total_num_epochs"], [41, 3, 1, "", "total_num_steps"], [41, 3, 1, "", "train_batch_size"], [41, 3, 1, "", "unfreeze_param_list"], [41, 3, 1, "", "val_batch_interval"], [41, 3, 1, "", "val_batch_log_interval"], [41, 3, 1, "", "val_batch_size"], [41, 3, 1, "", "val_epoch_interval"], [41, 3, 1, "", "wandb"], [41, 3, 1, "", "wandb_group"], [41, 3, 1, "", "wandb_project"], [41, 3, 1, "", "wandb_team"], [41, 3, 1, "", "warmup_factor"], [41, 3, 1, "", "warmup_lr"], [41, 3, 1, "", "warmup_num_epochs"], [41, 3, 1, "", "warmup_num_steps"], [41, 3, 1, "", "weight_decay"]], "sfm.pipeline.accelerator.dataclasses.TrainerState": [[41, 3, 1, "", "args"], [41, 3, 1, "", "batch"], [41, 3, 1, "", "epoch"], [41, 3, 1, "", "global_step"]], "sfm.pipeline.accelerator.dataclasses.ValidLogOutput": [[24, 3, 1, "", "extra_output"], [24, 3, 1, "", "num_examples"], [24, 3, 1, "", "valid_loss"]], "sfm.pipeline.accelerator.fp16_scaler": [[24, 1, 1, "", "FP16Scaler"]], "sfm.pipeline.accelerator.fp16_scaler.FP16Scaler": [[24, 2, 1, "", "backward"], [24, 2, 1, "", "check_grad_overflow"], [24, 2, 1, "", "step"], [24, 2, 1, "", "unscale_and_clip_grad"]], "sfm.pipeline.accelerator.model": [[42, 1, 1, "", "Model"]], "sfm.pipeline.accelerator.model.Model": [[42, 2, 1, "", "after_batch"], [42, 2, 1, "", "after_training"], [42, 2, 1, "", "before_batch"], [42, 2, 1, "", "before_training"], [42, 2, 1, "", "compute_loss"], [42, 2, 1, "", "config_optimizer"], [42, 3, 1, "", "training"]], "sfm.pipeline.accelerator.pipeline_module": [[24, 1, 1, "", "SFMPipelineModelMixin"], [24, 1, 1, "", "SFMPipelineModule"]], "sfm.pipeline.accelerator.pipeline_module.SFMPipelineModelMixin": [[24, 2, 1, "", "to_layers"], [24, 3, 1, "", "training"]], "sfm.pipeline.accelerator.pipeline_module.SFMPipelineModule": [[24, 2, 1, "", "after_batch"], [24, 2, 1, "", "after_training"], [24, 2, 1, "", "before_batch"], [24, 2, 1, "", "before_training"], [24, 2, 1, "", "compute_loss"], [24, 2, 1, "", "config_optimizer"], [24, 3, 1, "", "training"]], "sfm.pipeline.accelerator.trainer": [[24, 1, 1, "", "LogAccumulator"], [24, 1, 1, "", "LossAccumulator"], [42, 1, 1, "", "Trainer"], [24, 4, 1, "", "seed_everything"]], "sfm.pipeline.accelerator.trainer.LogAccumulator": [[24, 2, 1, "", "add"], [24, 5, 1, "", "averge_log"], [24, 5, 1, "", "averge_loss"], [24, 2, 1, "", "reset"]], "sfm.pipeline.accelerator.trainer.LossAccumulator": [[24, 2, 1, "", "add"], [24, 5, 1, "", "averge_loss"], [24, 2, 1, "", "reset"]], "sfm.pipeline.accelerator.trainer.Trainer": [[42, 2, 1, "", "build_accelerator"], [42, 2, 1, "", "build_log_output"], [42, 2, 1, "", "finetune_from_checkpoint"], [42, 2, 1, "", "resume"], [42, 2, 1, "", "save_checkpoint"], [42, 2, 1, "", "should_do_batch_validate"], [42, 2, 1, "", "should_do_epoch_validate"], [42, 2, 1, "", "should_log"], [42, 2, 1, "", "should_save_batch_checkpoint"], [42, 2, 1, "", "should_save_epoch_checkpoint"], [42, 2, 1, "", "train"], [42, 5, 1, "", "train_data_loader"], [42, 5, 1, "", "valid_data_loader"], [42, 2, 1, "", "validate"]], "sfm.pipeline.generalist": [[25, 0, 0, "-", "graphormerllama_3Dtrainer"]], "sfm.pipeline.generalist.graphormerllama_3Dtrainer": [[25, 1, 1, "", "Trainer3D"], [25, 4, 1, "", "make_supervised_data_module"]], "sfm.pipeline.generalist.graphormerllama_3Dtrainer.Trainer3D": [[25, 2, 1, "", "resume"], [25, 2, 1, "", "save_ckp"], [25, 2, 1, "", "train_tensor_pipeline"]], "sfm.tasks": [[27, 0, 0, "-", "decoder"], [28, 0, 0, "-", "generalist"], [29, 0, 0, "-", "graphormer"], [30, 0, 0, "-", "pfm"], [31, 0, 0, "-", "scigpt"], [32, 0, 0, "-", "tamgent"]], "sfm.tasks.decoder": [[27, 0, 0, "-", "gen_dec_deepfuse"], [27, 0, 0, "-", "train_dec_deepfuse"]], "sfm.tasks.decoder.gen_dec_deepfuse": [[27, 4, 1, "", "load_state_dict"], [27, 4, 1, "", "main"]], "sfm.tasks.decoder.train_dec_deepfuse": [[27, 4, 1, "", "main"]], "sfm.tasks.generalist": [[28, 0, 0, "-", "eval_generalist_metric"], [28, 0, 0, "-", "ft3d_graphormer_llama_inst"], [28, 0, 0, "-", "ft_graphormer_llama_inst"], [28, 0, 0, "-", "test_generalist"], [28, 0, 0, "-", "test_generalist_pp"]], "sfm.tasks.generalist.eval_generalist_metric": [[28, 4, 1, "", "calc_precision_and_recall"], [28, 4, 1, "", "eval"], [28, 4, 1, "", "get_label"], [28, 4, 1, "", "test_aromatic"], [28, 4, 1, "", "test_score"]], "sfm.tasks.generalist.ft3d_graphormer_llama_inst": [[28, 4, 1, "", "main"]], "sfm.tasks.generalist.ft_graphormer_llama_inst": [[28, 4, 1, "", "main"], [28, 4, 1, "", "make_supervised_data_module"]], "sfm.tasks.generalist.test_generalist": [[28, 1, 1, "", "TestGeneralistConfig"], [28, 4, 1, "", "batch_mol"], [28, 4, 1, "", "convert"], [28, 4, 1, "", "create_device_map"], [28, 4, 1, "", "download_model"], [28, 4, 1, "", "get_tokenizer"], [28, 4, 1, "", "main"]], "sfm.tasks.generalist.test_generalist.TestGeneralistConfig": [[28, 3, 1, "", "infer_batch_size"], [28, 3, 1, "", "local_checkpoint_path"], [28, 3, 1, "", "num_gpus"], [28, 3, 1, "", "output_fname"], [28, 3, 1, "", "question_list_fname"], [28, 3, 1, "", "remote_checkpoint_query_string"], [28, 3, 1, "", "remote_checkpoint_storage_account"], [28, 3, 1, "", "remote_checkpoint_storage_container"], [28, 3, 1, "", "smiles_list_fname"], [28, 3, 1, "", "test_checkpoint_path"], [28, 3, 1, "", "test_global_step"]], "sfm.tasks.generalist.test_generalist_pp": [[28, 1, 1, "", "Evaler"], [28, 1, 1, "", "TestGeneralistConfig"], [28, 4, 1, "", "main"], [28, 4, 1, "", "make_supervised_data_module"]], "sfm.tasks.generalist.test_generalist_pp.Evaler": [[28, 2, 1, "", "load_ckpt"], [28, 2, 1, "", "validate_cls"], [28, 2, 1, "", "validate_cls_mlp"], [28, 2, 1, "", "validate_reg"], [28, 2, 1, "", "validate_reg_multitask"]], "sfm.tasks.generalist.test_generalist_pp.TestGeneralistConfig": [[28, 3, 1, "", "batch_size"], [28, 3, 1, "", "output_fname"], [28, 3, 1, "", "test_checkpoint_path"], [28, 3, 1, "", "test_task"]], "sfm.tasks.graphormer": [[29, 0, 0, "-", "ft_graphormer"], [29, 0, 0, "-", "pretrain_graphormer"]], "sfm.tasks.graphormer.ft_graphormer": [[29, 4, 1, "", "main"]], "sfm.tasks.graphormer.pretrain_graphormer": [[29, 4, 1, "", "main"]], "sfm.tasks.pfm": [[30, 0, 0, "-", "pretrain_pfm"]], "sfm.tasks.pfm.pretrain_pfm": [[30, 4, 1, "", "main"]], "sfm.tasks.scigpt": [[31, 0, 0, "-", "pretrain_scigpt"]], "sfm.tasks.scigpt.pretrain_scigpt": [[31, 4, 1, "", "main"]], "sfm.tasks.tamgent": [[32, 0, 0, "-", "finetune_tamgent2"]], "sfm.tasks.tamgent.finetune_tamgent2": [[32, 4, 1, "", "main"]], "sfm.utils": [[33, 0, 0, "-", "FairseqDataset"], [33, 0, 0, "-", "LayerDropModuleList"], [33, 0, 0, "-", "PPEngine"], [33, 0, 0, "-", "arg_utils"], [33, 0, 0, "-", "barrier"], [33, 0, 0, "-", "cli_utils"], [33, 0, 0, "-", "copilot_module"], [33, 0, 0, "-", "defaultdsconfig"], [33, 0, 0, "-", "dist_utils"], [33, 0, 0, "-", "env_init"], [33, 0, 0, "-", "get_paranum"], [33, 0, 0, "-", "jload"], [33, 0, 0, "-", "move_to_device"], [33, 0, 0, "-", "myPipelineParallelGrid"], [33, 0, 0, "-", "mypp_module"], [34, 0, 0, "-", "optim"], [33, 0, 0, "-", "peft"], [33, 0, 0, "-", "pipelinemode"], [33, 0, 0, "-", "pretrained_layer_spec"], [33, 0, 0, "-", "science_tokens"]], "sfm.utils.FairseqDataset": [[33, 1, 1, "", "EpochListening"], [33, 1, 1, "", "FairseqDataset"], [33, 1, 1, "", "FairseqIterableDataset"]], "sfm.utils.FairseqDataset.EpochListening": [[33, 5, 1, "", "can_reuse_epoch_itr_across_epochs"], [33, 2, 1, "", "set_epoch"]], "sfm.utils.FairseqDataset.FairseqDataset": [[33, 2, 1, "", "attr"], [33, 2, 1, "", "batch_by_size"], [33, 2, 1, "", "collate_tokens"], [33, 2, 1, "", "collater"], [33, 2, 1, "", "filter_indices_by_size"], [33, 2, 1, "", "get_batch_shapes"], [33, 2, 1, "", "num_tokens"], [33, 2, 1, "", "num_tokens_vec"], [33, 2, 1, "", "ordered_indices"], [33, 2, 1, "", "prefetch"], [33, 2, 1, "", "size"], [33, 5, 1, "", "supports_fetch_outside_dataloader"], [33, 5, 1, "", "supports_prefetch"]], "sfm.utils.LayerDropModuleList": [[33, 1, 1, "", "LayerDropModuleList"]], "sfm.utils.PPEngine": [[33, 1, 1, "", "SFMPipeEngine"], [33, 4, 1, "", "initialize"], [33, 4, 1, "", "is_even"]], "sfm.utils.PPEngine.SFMPipeEngine": [[33, 3, 1, "", "DTYPE_TO_ID"], [33, 3, 1, "", "ID_TO_DTYPE"], [33, 2, 1, "", "allreduce_gradients"], [33, 2, 1, "", "backward"], [33, 2, 1, "", "eval_batch"], [33, 2, 1, "", "fast_load_checkpoint"], [33, 2, 1, "", "forward"], [33, 2, 1, "", "is_first_stage"], [33, 2, 1, "", "is_gradient_accumulation_boundary"], [33, 2, 1, "", "is_last_stage"], [33, 2, 1, "", "load_checkpoint"], [33, 2, 1, "", "load_module_state_dict"], [33, 2, 1, "", "log_for_device"], [33, 2, 1, "", "mem_status"], [33, 2, 1, "", "module_state_dict"], [33, 2, 1, "", "reset_activation_shape"], [33, 2, 1, "", "set_batch_fn"], [33, 2, 1, "", "set_dataiterator"], [33, 2, 1, "", "set_dataloader"], [33, 2, 1, "", "set_has_attention_mask"], [33, 2, 1, "", "set_train_batch_size"], [33, 2, 1, "", "step"], [33, 2, 1, "", "tput_log"], [33, 2, 1, "", "train_batch"], [33, 3, 1, "", "training"]], "sfm.utils.arg_utils": [[33, 1, 1, "", "ExtraArgsProvider"], [33, 4, 1, "", "add_dataclass_to_parser"], [33, 4, 1, "", "argument_exists"], [33, 4, 1, "", "from_args"], [33, 4, 1, "", "is_collection"], [33, 4, 1, "", "is_enum_type"], [33, 4, 1, "", "make_enum_praser"], [33, 4, 1, "", "unwarp_optional"]], "sfm.utils.barrier": [[33, 4, 1, "", "check"]], "sfm.utils.cli_utils": [[33, 4, 1, "", "cli"]], "sfm.utils.copilot_module": [[33, 1, 1, "", "CopilotModule"], [33, 1, 1, "", "LayerSpec"], [33, 6, 1, "", "PipelineError"], [33, 1, 1, "", "TiedLayerSpec"]], "sfm.utils.copilot_module.CopilotModule": [[33, 2, 1, "", "forward"], [33, 3, 1, "", "training"]], "sfm.utils.copilot_module.LayerSpec": [[33, 2, 1, "", "build"]], "sfm.utils.dist_utils": [[33, 4, 1, "", "is_master_node"]], "sfm.utils.env_init": [[33, 4, 1, "", "set_env"]], "sfm.utils.get_paranum": [[33, 4, 1, "", "count_paranum"]], "sfm.utils.jload": [[33, 4, 1, "", "jdump"], [33, 4, 1, "", "jload"]], "sfm.utils.move_to_device": [[33, 1, 1, "", "OutputMixIn"], [33, 4, 1, "", "move_to_device"]], "sfm.utils.move_to_device.OutputMixIn": [[33, 2, 1, "", "get"], [33, 2, 1, "", "iget"], [33, 2, 1, "", "items"], [33, 2, 1, "", "keys"]], "sfm.utils.myPipelineParallelGrid": [[33, 1, 1, "", "PipeDataParallelTopology"], [33, 1, 1, "", "PipeModelDataParallelTopology"], [33, 1, 1, "", "ProcessTopology"], [33, 1, 1, "", "myPipelineParallelGrid"]], "sfm.utils.myPipelineParallelGrid.ProcessTopology": [[33, 2, 1, "", "filter_match"], [33, 2, 1, "", "get_axis_comm_lists"], [33, 2, 1, "", "get_axis_list"], [33, 2, 1, "", "get_axis_names"], [33, 2, 1, "", "get_coord"], [33, 2, 1, "", "get_dim"], [33, 2, 1, "", "get_rank"], [33, 2, 1, "", "get_rank_repr"], [33, 2, 1, "", "world_size"]], "sfm.utils.myPipelineParallelGrid.myPipelineParallelGrid": [[33, 2, 1, "", "get_data_parallel_group"], [33, 2, 1, "", "get_data_parallel_id"], [33, 2, 1, "", "get_data_parallel_rank"], [33, 2, 1, "", "get_data_parallel_world_size"], [33, 2, 1, "", "get_global_rank"], [33, 2, 1, "", "get_model_parallel_group"], [33, 2, 1, "", "get_model_parallel_rank"], [33, 2, 1, "", "get_model_parallel_world_size"], [33, 2, 1, "", "get_pipe_parallel_group"], [33, 2, 1, "", "get_pipe_parallel_rank"], [33, 2, 1, "", "get_pipe_parallel_world_size"], [33, 2, 1, "", "get_slice_parallel_group"], [33, 2, 1, "", "get_slice_parallel_rank"], [33, 2, 1, "", "get_slice_parallel_world_size"], [33, 2, 1, "", "get_stage_id"], [33, 2, 1, "", "stage_to_global"], [33, 2, 1, "", "topology"]], "sfm.utils.mypp_module": [[33, 1, 1, "", "LayerSpec"], [33, 6, 1, "", "PipelineError"], [33, 1, 1, "", "PipelineModule"], [33, 1, 1, "", "TiedLayerSpec"], [33, 4, 1, "", "partition_by_layers"]], "sfm.utils.mypp_module.LayerSpec": [[33, 2, 1, "", "build"]], "sfm.utils.mypp_module.PipelineModule": [[33, 2, 1, "", "allreduce_tied_weight_gradients"], [33, 2, 1, "", "ckpt_layer_path"], [33, 2, 1, "", "ckpt_layer_path_list"], [33, 2, 1, "", "ckpt_prefix"], [33, 2, 1, "", "fast_load_state_dir"], [33, 2, 1, "", "forward"], [33, 2, 1, "", "get_tied_weights_and_groups"], [33, 2, 1, "", "load_state_dir"], [33, 2, 1, "", "mpu"], [33, 2, 1, "", "num_pipeline_stages"], [33, 2, 1, "", "partitions"], [33, 2, 1, "", "save_state_dict"], [33, 2, 1, "", "set_checkpoint_interval"], [33, 2, 1, "", "stage_owner"], [33, 2, 1, "", "topology"], [33, 3, 1, "", "training"]], "sfm.utils.optim": [[34, 0, 0, "-", "adam"], [34, 0, 0, "-", "optimizer"], [34, 0, 0, "-", "set_lr"]], "sfm.utils.optim.adam": [[34, 1, 1, "", "AdamW"]], "sfm.utils.optim.adam.AdamW": [[34, 2, 1, "", "step"], [34, 5, 1, "", "supports_flat_params"], [34, 5, 1, "", "supports_memory_efficient_fp16"]], "sfm.utils.optim.optimizer": [[34, 4, 1, "", "myAdam"], [34, 4, 1, "", "process_param"], [34, 4, 1, "", "split_param_and_layer_name"]], "sfm.utils.optim.set_lr": [[34, 1, 1, "", "groupWarmupDecayLR"], [34, 4, 1, "", "group_param"], [34, 4, 1, "", "group_param_copilot"], [34, 4, 1, "", "myAdam"], [34, 4, 1, "", "myGroupAdam"], [34, 4, 1, "", "myMuAdam"], [34, 4, 1, "", "process_freeze_param"], [34, 4, 1, "", "process_param_groups"]], "sfm.utils.optim.set_lr.groupWarmupDecayLR": [[34, 2, 1, "", "step"]], "sfm.utils.peft": [[33, 4, 1, "", "create_peft_model"]], "sfm.utils.pipelinemode": [[33, 1, 1, "", "LlamaDecoderLayerTest"], [33, 4, 1, "", "check_grad_requirements"], [33, 4, 1, "", "convert2list"], [33, 4, 1, "", "pipemode"], [33, 4, 1, "", "pipemodegradcheck"], [33, 4, 1, "", "tuple2input"]], "sfm.utils.pipelinemode.LlamaDecoderLayerTest": [[33, 2, 1, "", "dict_forward"], [33, 2, 1, "", "tensor_forward"], [33, 3, 1, "", "training"]], "sfm.utils.pretrained_layer_spec": [[33, 1, 1, "", "PretrainedLayerSpec"], [33, 1, 1, "", "TiedPretrainedLayerSpec"], [33, 4, 1, "", "load_ckp_tied_modules"]], "sfm.utils.pretrained_layer_spec.PretrainedLayerSpec": [[33, 2, 1, "", "build"], [33, 2, 1, "", "create_peft_model"], [33, 2, 1, "", "load_pretrained"], [33, 2, 1, "", "partition_load_pretrained"], [33, 2, 1, "", "resize_token_embeddings"]]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:attribute", "4": "py:function", "5": "py:property", "6": "py:exception"}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "function", "Python function"], "5": ["py", "property", "Python property"], "6": ["py", "exception", "Python exception"]}, "titleterms": {"sfm": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 39], "packag": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34], "subpackag": [1, 3, 10, 11, 13, 15, 18, 23, 26, 33], "modul": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 42], "content": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 37], "criterion": 2, "submodul": [2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34], "autoregress": 2, "copilotloss": 2, "l1ft": 2, "mae2d": 2, "mae3d": 2, "mae3ddiff": 2, "data": [3, 4, 5, 6, 7, 8, 35], "data_util": 3, "data_utils_fast": 3, "dataset": [3, 4, 5, 6, 7, 8], "dynamics_load": 3, "molecul": 3, "sampler": [3, 35], "text": 3, "dec_data": 4, "sfmdectoken": [4, 7], "mol_data": 5, "algo": 5, "collat": [5, 6], "molftdataapi": 5, "moltext_dataset": 5, "moltoken": 5, "tdc": 5, "wrapper": 5, "xyz2smil": 5, "prot_data": 6, "process": 6, "sequence_mask": 6, "spatial_nois": 6, "structure2lmdb": 6, "util": [6, 33, 34], "vocalubari": 6, "sci_data": 7, "tamgent2": 8, "token": 8, "log": 9, "logger": 9, "model": [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 24, 36, 39], "decod": [11, 12, 27, 39], "deepfus": 12, "config": [12, 20], "hidden_st": 12, "generalist": [13, 14, 25, 28, 39], "generalist_config": 13, "graphormer_llama": 13, "graphormer_encod": 14, "hybrid_emb": 14, "hybrid_emb_3dmp": 14, "graphorm": [15, 16, 29, 39], "graphormer_config": 15, "graphormerdiff": 15, "unifieddecod": [16, 19], "graphormer_embed": 16, "graphormer_lay": 16, "graphormer_layers_diff": 16, "graphormer_layers_mp": 16, "graphormer_layers_pp": 16, "graphormer_sentence_encod": 16, "graphormer_sentence_encoder_tmdiff": 16, "graphormer_sentence_encoder_lay": 16, "graphormer_sentence_encoder_layer_mp": 16, "graphormer_sentence_encoder_mp": 16, "graphormer_timestep_encod": 16, "llama2": [17, 39], "convert_llamaweight2pp": 17, "llama2mp_config": 17, "llama_modul": 17, "llama_modules_3dmp": 17, "pfm": [18, 19, 30], "pfm_config": 18, "pfmmodel": 18, "pfm_embed": 19, "pfm_encod": 19, "pfm_encoder_lay": 19, "pfm_layer": 19, "timestep_encod": 19, "scigpt": [20, 31, 39], "tamgent": [21, 32, 39], "qformer": 21, "schedul": 21, "fairseqdropout": 22, "flashatt_bk": 22, "droppath": 22, "fused_layernorm": 22, "get_activation_fn": 22, "layer_norm": 22, "mem_eff_attn": 22, "multihead_attent": 22, "parallelattentionbia": 22, "partial_grad_emb": 22, "quant_nois": 22, "rotary_embed": 22, "sfmmodul": 22, "pipelin": [23, 24, 25, 35], "acceler": 24, "dataclass": 24, "fp16_scaler": 24, "pipeline_modul": 24, "trainer": [24, 42], "graphormerllama_3dtrain": 25, "task": [26, 27, 28, 29, 30, 31, 32], "gen_dec_deepfus": 27, "train_dec_deepfus": 27, "eval_generalist_metr": 28, "ft3d_graphormer_llama_inst": 28, "ft_graphormer_llama_inst": 28, "test_generalist": 28, "test_generalist_pp": 28, "ft_graphorm": 29, "pretrain_graphorm": 29, "pretrain_pfm": 30, "pretrain_scigpt": 31, "finetune_tamgent2": 32, "fairseqdataset": 33, "layerdropmodulelist": 33, "ppengin": 33, "arg_util": 33, "barrier": 33, "cli_util": 33, "copilot_modul": 33, "defaultdsconfig": 33, "dist_util": 33, "env_init": 33, "get_paranum": 33, "jload": 33, "move_to_devic": 33, "mypipelineparallelgrid": 33, "mypp_modul": 33, "peft": 33, "pipelinemod": 33, "pretrained_layer_spec": 33, "science_token": 33, "optim": [34, 36], "adam": 34, "set_lr": 34, "dynam": 35, "batch": 35, "larg": 36, "distribut": [35, 36], "train": [36, 41, 42], "zero": 36, "parallel": 36, "pretrain": 36, "layer": 36, "spec": 36, "exampl": 36, "welcom": 37, "a4sframework": 37, "": 37, "document": 37, "indic": 37, "tabl": 37, "instal": 38, "guid": 38, "linux": 38, "zoo": 39, "chemic": 39, "overview": 40, "arg": 41, "trainstrategi": 41, "distributedconfig": 41, "trainerconfig": 41, "trainerst": 41, "user": 42, "friendli": 42, "interfac": 42, "cli": 42, "decor": 42, "unifi": 42, "strategi": 42, "requir": 42, "weight": 35}, "envversion": {"sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.todo": 2, "sphinx.ext.intersphinx": 1, "sphinx": 60}, "alltitles": {"sfm": [[0, "sfm"]], "sfm package": [[1, "sfm-package"]], "Subpackages": [[1, "subpackages"], [10, "subpackages"], [11, "subpackages"], [23, "subpackages"], [26, "subpackages"], [13, "subpackages"], [15, "subpackages"], [18, "subpackages"], [3, "subpackages"], [33, "subpackages"]], "Module contents": [[1, "module-sfm"], [2, "module-sfm.criterions"], [4, "module-sfm.data.dec_data"], [5, "module-sfm.data.mol_data"], [6, "module-sfm.data.prot_data"], [7, "module-sfm.data.sci_data"], [8, "module-sfm.data.tamgent2"], [9, "module-sfm.logging"], [10, "module-sfm.models"], [11, "module-sfm.models.decoder"], [14, "module-sfm.models.generalist.modules"], [16, "module-sfm.models.graphormer.modules"], [19, "module-sfm.models.pfm.modules"], [22, "module-sfm.modules"], [23, "module-sfm.pipeline"], [26, "module-sfm.tasks"], [27, "module-sfm.tasks.decoder"], [31, "module-sfm.tasks.scigpt"], [32, "module-sfm.tasks.tamgent"], [34, "module-sfm.utils.optim"], [17, "module-sfm.models.llama2"], [12, "module-sfm.models.decoder.deepfuse"], [13, "module-sfm.models.generalist"], [15, "module-sfm.models.graphormer"], [18, "module-sfm.models.pfm"], [20, "module-sfm.models.scigpt"], [21, "module-sfm.models.tamgent"], [24, "module-sfm.pipeline.accelerator"], [25, "module-sfm.pipeline.generalist"], [28, "module-sfm.tasks.generalist"], [29, "module-sfm.tasks.graphormer"], [30, "module-sfm.tasks.pfm"], [3, "module-sfm.data"], [33, "module-sfm.utils"]], "sfm.criterions package": [[2, "sfm-criterions-package"]], "Submodules": [[2, "submodules"], [4, "submodules"], [5, "submodules"], [6, "submodules"], [7, "submodules"], [8, "submodules"], [9, "submodules"], [14, "submodules"], [16, "submodules"], [19, "submodules"], [22, "submodules"], [27, "submodules"], [31, "submodules"], [32, "submodules"], [34, "submodules"], [17, "submodules"], [12, "submodules"], [13, "submodules"], [15, "submodules"], [18, "submodules"], [20, "submodules"], [21, "submodules"], [24, "submodules"], [25, "submodules"], [28, "submodules"], [29, "submodules"], [30, "submodules"], [3, "submodules"], [33, "submodules"]], "sfm.criterions.autoregressive module": [[2, "module-sfm.criterions.autoregressive"]], "sfm.criterions.copilotloss module": [[2, "module-sfm.criterions.copilotloss"]], "sfm.criterions.l1ft module": [[2, "module-sfm.criterions.l1ft"]], "sfm.criterions.mae2d module": [[2, "module-sfm.criterions.mae2d"]], "sfm.criterions.mae3d module": [[2, "module-sfm.criterions.mae3d"]], "sfm.criterions.mae3ddiff module": [[2, "module-sfm.criterions.mae3ddiff"]], "sfm.data.dec_data package": [[4, "sfm-data-dec-data-package"]], "sfm.data.dec_data.SFMDecTokenizer module": [[4, "module-sfm.data.dec_data.SFMDecTokenizer"]], "sfm.data.dec_data.datasets module": [[4, "module-sfm.data.dec_data.datasets"]], "sfm.data.mol_data package": [[5, "sfm-data-mol-data-package"]], "sfm.data.mol_data.algos module": [[5, "module-sfm.data.mol_data.algos"]], "sfm.data.mol_data.collator module": [[5, "module-sfm.data.mol_data.collator"]], "sfm.data.mol_data.dataset module": [[5, "module-sfm.data.mol_data.dataset"]], "sfm.data.mol_data.molftdataapi module": [[5, "module-sfm.data.mol_data.molftdataapi"]], "sfm.data.mol_data.moltext_dataset module": [[5, "module-sfm.data.mol_data.moltext_dataset"]], "sfm.data.mol_data.moltokenizer module": [[5, "module-sfm.data.mol_data.moltokenizer"]], "sfm.data.mol_data.tdc module": [[5, "module-sfm.data.mol_data.tdc"]], "sfm.data.mol_data.wrapper module": [[5, "module-sfm.data.mol_data.wrapper"]], "sfm.data.mol_data.xyz2smiles module": [[5, "module-sfm.data.mol_data.xyz2smiles"]], "sfm.data.prot_data package": [[6, "sfm-data-prot-data-package"]], "sfm.data.prot_data.collater module": [[6, "module-sfm.data.prot_data.collater"]], "sfm.data.prot_data.dataset module": [[6, "module-sfm.data.prot_data.dataset"]], "sfm.data.prot_data.process module": [[6, "module-sfm.data.prot_data.process"]], "sfm.data.prot_data.sequence_masking module": [[6, "module-sfm.data.prot_data.sequence_masking"]], "sfm.data.prot_data.spatial_noise module": [[6, "module-sfm.data.prot_data.spatial_noise"]], "sfm.data.prot_data.structure2lmdb module": [[6, "module-sfm.data.prot_data.structure2lmdb"]], "sfm.data.prot_data.util module": [[6, "module-sfm.data.prot_data.util"]], "sfm.data.prot_data.vocalubary module": [[6, "module-sfm.data.prot_data.vocalubary"]], "sfm.data.sci_data package": [[7, "sfm-data-sci-data-package"]], "sfm.data.sci_data.SFMDecTokenizer module": [[7, "module-sfm.data.sci_data.SFMDecTokenizer"]], "sfm.data.sci_data.dataset module": [[7, "module-sfm.data.sci_data.dataset"]], "sfm.data.tamgent2 package": [[8, "sfm-data-tamgent2-package"]], "sfm.data.tamgent2.datasets module": [[8, "module-sfm.data.tamgent2.datasets"]], "sfm.data.tamgent2.tokenizer module": [[8, "module-sfm.data.tamgent2.tokenizer"]], "sfm.logging package": [[9, "sfm-logging-package"]], "sfm.logging.loggers module": [[9, "module-sfm.logging.loggers"]], "sfm.models package": [[10, "sfm-models-package"]], "sfm.models.decoder package": [[11, "sfm-models-decoder-package"]], "sfm.models.generalist.modules package": [[14, "sfm-models-generalist-modules-package"]], "sfm.models.generalist.modules.graphormer_encoder module": [[14, "module-sfm.models.generalist.modules.graphormer_encoder"]], "sfm.models.generalist.modules.hybrid_emb module": [[14, "module-sfm.models.generalist.modules.hybrid_emb"]], "sfm.models.generalist.modules.hybrid_emb_3dmp module": [[14, "module-sfm.models.generalist.modules.hybrid_emb_3dmp"]], "sfm.models.graphormer.modules package": [[16, "sfm-models-graphormer-modules-package"]], "sfm.models.graphormer.modules.UnifiedDecoder module": [[16, "module-sfm.models.graphormer.modules.UnifiedDecoder"]], "sfm.models.graphormer.modules.graphormer_embedding module": [[16, "module-sfm.models.graphormer.modules.graphormer_embedding"]], "sfm.models.graphormer.modules.graphormer_layers module": [[16, "module-sfm.models.graphormer.modules.graphormer_layers"]], "sfm.models.graphormer.modules.graphormer_layers_diff module": [[16, "module-sfm.models.graphormer.modules.graphormer_layers_diff"]], "sfm.models.graphormer.modules.graphormer_layers_mp module": [[16, "module-sfm.models.graphormer.modules.graphormer_layers_mp"]], "sfm.models.graphormer.modules.graphormer_layers_pp module": [[16, "module-sfm.models.graphormer.modules.graphormer_layers_pp"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder module": [[16, "module-sfm.models.graphormer.modules.graphormer_sentence_encoder"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder_TMdiff module": [[16, "module-sfm.models.graphormer.modules.graphormer_sentence_encoder_TMdiff"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder_layer module": [[16, "module-sfm.models.graphormer.modules.graphormer_sentence_encoder_layer"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP module": [[16, "module-sfm.models.graphormer.modules.graphormer_sentence_encoder_layer_MP"]], "sfm.models.graphormer.modules.graphormer_sentence_encoder_mp module": [[16, "module-sfm.models.graphormer.modules.graphormer_sentence_encoder_mp"]], "sfm.models.graphormer.modules.graphormer_timestep_encoder module": [[16, "module-sfm.models.graphormer.modules.graphormer_timestep_encoder"]], "sfm.models.pfm.modules package": [[19, "sfm-models-pfm-modules-package"]], "sfm.models.pfm.modules.UnifiedDecoder module": [[19, "module-sfm.models.pfm.modules.UnifiedDecoder"]], "sfm.models.pfm.modules.pfm_embedding module": [[19, "module-sfm.models.pfm.modules.pfm_embedding"]], "sfm.models.pfm.modules.pfm_encoder module": [[19, "module-sfm.models.pfm.modules.pfm_encoder"]], "sfm.models.pfm.modules.pfm_encoder_layer module": [[19, "module-sfm.models.pfm.modules.pfm_encoder_layer"]], "sfm.models.pfm.modules.pfm_layer module": [[19, "module-sfm.models.pfm.modules.pfm_layer"]], "sfm.models.pfm.modules.timestep_encoder module": [[19, "module-sfm.models.pfm.modules.timestep_encoder"]], "sfm.modules package": [[22, "sfm-modules-package"]], "sfm.modules.FairseqDropout module": [[22, "module-sfm.modules.FairseqDropout"]], "sfm.modules.FlashAtt_bk module": [[22, "module-sfm.modules.FlashAtt_bk"]], "sfm.modules.droppath module": [[22, "module-sfm.modules.droppath"]], "sfm.modules.fused_layernorm module": [[22, "module-sfm.modules.fused_layernorm"]], "sfm.modules.get_activation_fn module": [[22, "module-sfm.modules.get_activation_fn"]], "sfm.modules.layer_norm module": [[22, "module-sfm.modules.layer_norm"]], "sfm.modules.mem_eff_attn module": [[22, "module-sfm.modules.mem_eff_attn"]], "sfm.modules.multihead_attention module": [[22, "module-sfm.modules.multihead_attention"]], "sfm.modules.parallelattentionbias module": [[22, "module-sfm.modules.parallelattentionbias"]], "sfm.modules.partial_grad_emb module": [[22, "module-sfm.modules.partial_grad_emb"]], "sfm.modules.quant_noise module": [[22, "module-sfm.modules.quant_noise"]], "sfm.modules.rotary_embedding module": [[22, "module-sfm.modules.rotary_embedding"]], "sfm.modules.sfmmodule module": [[22, "module-sfm.modules.sfmmodule"]], "sfm.pipeline package": [[23, "sfm-pipeline-package"]], "sfm.tasks package": [[26, "sfm-tasks-package"]], "sfm.tasks.decoder package": [[27, "sfm-tasks-decoder-package"]], "sfm.tasks.decoder.gen_dec_deepfuse module": [[27, "module-sfm.tasks.decoder.gen_dec_deepfuse"]], "sfm.tasks.decoder.train_dec_deepfuse module": [[27, "module-sfm.tasks.decoder.train_dec_deepfuse"]], "sfm.tasks.scigpt package": [[31, "sfm-tasks-scigpt-package"]], "sfm.tasks.scigpt.pretrain_scigpt module": [[31, "module-sfm.tasks.scigpt.pretrain_scigpt"]], "sfm.tasks.tamgent package": [[32, "sfm-tasks-tamgent-package"]], "sfm.tasks.tamgent.finetune_tamgent2 module": [[32, "module-sfm.tasks.tamgent.finetune_tamgent2"]], "sfm.utils.optim package": [[34, "sfm-utils-optim-package"]], "sfm.utils.optim.adam module": [[34, "module-sfm.utils.optim.adam"]], "sfm.utils.optim.optimizer module": [[34, "module-sfm.utils.optim.optimizer"]], "sfm.utils.optim.set_lr module": [[34, "module-sfm.utils.optim.set_lr"]], "Installation Guide": [[38, "installation-guide"]], "Linux": [[38, "linux"]], "Welcome to A4SFramework\u2019s documentation!": [[37, "welcome-to-a4sframework-s-documentation"]], "Contents:": [[37, null]], "Indices and tables": [[37, "indices-and-tables"]], "Overview": [[40, "overview"]], "sfm.models.llama2 package": [[17, "sfm-models-llama2-package"]], "sfm.models.llama2.convert_llamaweight2pp module": [[17, "module-sfm.models.llama2.convert_llamaweight2pp"]], "sfm.models.llama2.llama2mp_config module": [[17, "module-sfm.models.llama2.llama2mp_config"]], "sfm.models.llama2.llama_modules module": [[17, "module-sfm.models.llama2.llama_modules"]], "sfm.models.llama2.llama_modules_3dmp module": [[17, "module-sfm.models.llama2.llama_modules_3dmp"]], "sfm.models.decoder.deepfuse package": [[12, "sfm-models-decoder-deepfuse-package"]], "sfm.models.decoder.deepfuse.config module": [[12, "module-sfm.models.decoder.deepfuse.config"]], "sfm.models.decoder.deepfuse.hidden_state module": [[12, "module-sfm.models.decoder.deepfuse.hidden_state"]], "sfm.models.decoder.deepfuse.model module": [[12, "module-sfm.models.decoder.deepfuse.model"]], "sfm.models.decoder.deepfuse.modules module": [[12, "module-sfm.models.decoder.deepfuse.modules"]], "sfm.models.generalist package": [[13, "sfm-models-generalist-package"]], "sfm.models.generalist.generalist_config module": [[13, "module-sfm.models.generalist.generalist_config"]], "sfm.models.generalist.graphormer_llama module": [[13, "module-sfm.models.generalist.graphormer_llama"]], "sfm.models.graphormer package": [[15, "sfm-models-graphormer-package"]], "sfm.models.graphormer.graphormer module": [[15, "module-sfm.models.graphormer.graphormer"]], "sfm.models.graphormer.graphormer_config module": [[15, "module-sfm.models.graphormer.graphormer_config"]], "sfm.models.graphormer.graphormerdiff module": [[15, "module-sfm.models.graphormer.graphormerdiff"]], "sfm.models.pfm package": [[18, "sfm-models-pfm-package"]], "sfm.models.pfm.pfm_config module": [[18, "module-sfm.models.pfm.pfm_config"]], "sfm.models.pfm.pfmmodel module": [[18, "module-sfm.models.pfm.pfmmodel"]], "sfm.models.scigpt package": [[20, "sfm-models-scigpt-package"]], "sfm.models.scigpt.config module": [[20, "module-sfm.models.scigpt.config"]], "sfm.models.scigpt.modules module": [[20, "module-sfm.models.scigpt.modules"]], "sfm.models.scigpt.scigpt module": [[20, "module-sfm.models.scigpt.scigpt"]], "sfm.models.tamgent package": [[21, "sfm-models-tamgent-package"]], "sfm.models.tamgent.Qformer module": [[21, "module-sfm.models.tamgent.Qformer"]], "sfm.models.tamgent.model module": [[21, "module-sfm.models.tamgent.model"]], "sfm.models.tamgent.scheduler module": [[21, "module-sfm.models.tamgent.scheduler"]], "sfm.pipeline.accelerator package": [[24, "sfm-pipeline-accelerator-package"]], "sfm.pipeline.accelerator.accelerator module": [[24, "module-sfm.pipeline.accelerator.accelerator"]], "sfm.pipeline.accelerator.dataclasses module": [[24, "module-sfm.pipeline.accelerator.dataclasses"]], "sfm.pipeline.accelerator.fp16_scaler module": [[24, "module-sfm.pipeline.accelerator.fp16_scaler"]], "sfm.pipeline.accelerator.model module": [[24, "module-sfm.pipeline.accelerator.model"]], "sfm.pipeline.accelerator.pipeline_module module": [[24, "module-sfm.pipeline.accelerator.pipeline_module"]], "sfm.pipeline.accelerator.trainer module": [[24, "module-sfm.pipeline.accelerator.trainer"]], "sfm.pipeline.generalist package": [[25, "sfm-pipeline-generalist-package"]], "sfm.pipeline.generalist.graphormerllama_3Dtrainer module": [[25, "module-sfm.pipeline.generalist.graphormerllama_3Dtrainer"]], "sfm.tasks.generalist package": [[28, "sfm-tasks-generalist-package"]], "sfm.tasks.generalist.eval_generalist_metric module": [[28, "module-sfm.tasks.generalist.eval_generalist_metric"]], "sfm.tasks.generalist.ft3d_graphormer_llama_inst module": [[28, "module-sfm.tasks.generalist.ft3d_graphormer_llama_inst"]], "sfm.tasks.generalist.ft_graphormer_llama_inst module": [[28, "module-sfm.tasks.generalist.ft_graphormer_llama_inst"]], "sfm.tasks.generalist.test_generalist module": [[28, "module-sfm.tasks.generalist.test_generalist"]], "sfm.tasks.generalist.test_generalist_pp module": [[28, "module-sfm.tasks.generalist.test_generalist_pp"]], "sfm.tasks.graphormer package": [[29, "sfm-tasks-graphormer-package"]], "sfm.tasks.graphormer.ft_graphormer module": [[29, "module-sfm.tasks.graphormer.ft_graphormer"]], "sfm.tasks.graphormer.pretrain_graphormer module": [[29, "module-sfm.tasks.graphormer.pretrain_graphormer"]], "sfm.tasks.pfm package": [[30, "sfm-tasks-pfm-package"]], "sfm.tasks.pfm.pretrain_pfm module": [[30, "module-sfm.tasks.pfm.pretrain_pfm"]], "Model Zoo": [[39, "model-zoo"]], "Graphormer": [[39, "module-sfm.models.graphormer.graphormer"]], "SFM Decoder": [[39, "module-sfm.models.decoder.deepfuse.model"]], "Tamgent": [[39, "module-sfm.models.tamgent.model"]], "Chemical Generalist": [[39, "chemical-generalist"]], "SciGPT": [[39, "module-sfm.models.scigpt.scigpt"]], "Llama2": [[39, "module-sfm.models.llama2.llama_modules_3dmp"]], "Training Args": [[41, "training-args"]], "TrainStrategy": [[41, "trainstrategy"]], "DistributedConfig": [[41, "distributedconfig"]], "TrainerConfig": [[41, "trainerconfig"]], "TrainerState": [[41, "trainerstate"]], "User-friendly Interface": [[42, "user-friendly-interface"]], "Cli decorator": [[42, "module-sfm.utils.cli_utils"]], "Unified Trainer": [[42, "unified-trainer"]], "Training Strategy": [[42, "training-strategy"]], "Required Module": [[42, "required-module"]], "Large Distributed Training": [[36, "large-distributed-training"]], "Zero Optimization": [[36, "zero-optimization"]], "Model parallelism": [[36, "model-parallelism"]], "Pretrained Layer Spec": [[36, "pretrained-layer-spec"]], "Examples": [[36, "examples"]], "sfm.data package": [[3, "sfm-data-package"]], "sfm.data.data_utils module": [[3, "module-sfm.data.data_utils"]], "sfm.data.data_utils_fast module": [[3, "module-sfm.data.data_utils_fast"]], "sfm.data.dataset module": [[3, "module-sfm.data.dataset"]], "sfm.data.dynamics_loader module": [[3, "module-sfm.data.dynamics_loader"]], "sfm.data.molecule module": [[3, "module-sfm.data.molecule"]], "sfm.data.sampler module": [[3, "module-sfm.data.sampler"]], "sfm.data.text module": [[3, "module-sfm.data.text"]], "sfm.utils package": [[33, "sfm-utils-package"]], "sfm.utils.FairseqDataset module": [[33, "module-sfm.utils.FairseqDataset"]], "sfm.utils.LayerDropModuleList module": [[33, "module-sfm.utils.LayerDropModuleList"]], "sfm.utils.PPEngine module": [[33, "module-sfm.utils.PPEngine"]], "sfm.utils.arg_utils module": [[33, "module-sfm.utils.arg_utils"]], "sfm.utils.barrier module": [[33, "module-sfm.utils.barrier"]], "sfm.utils.cli_utils module": [[33, "module-sfm.utils.cli_utils"]], "sfm.utils.copilot_module module": [[33, "module-sfm.utils.copilot_module"]], "sfm.utils.defaultdsconfig module": [[33, "module-sfm.utils.defaultdsconfig"]], "sfm.utils.dist_utils module": [[33, "module-sfm.utils.dist_utils"]], "sfm.utils.env_init module": [[33, "module-sfm.utils.env_init"]], "sfm.utils.get_paranum module": [[33, "module-sfm.utils.get_paranum"]], "sfm.utils.jload module": [[33, "module-sfm.utils.jload"]], "sfm.utils.move_to_device module": [[33, "module-sfm.utils.move_to_device"]], "sfm.utils.myPipelineParallelGrid module": [[33, "module-sfm.utils.myPipelineParallelGrid"]], "sfm.utils.mypp_module module": [[33, "module-sfm.utils.mypp_module"]], "sfm.utils.peft module": [[33, "module-sfm.utils.peft"]], "sfm.utils.pipelinemode module": [[33, "module-sfm.utils.pipelinemode"]], "sfm.utils.pretrained_layer_spec module": [[33, "module-sfm.utils.pretrained_layer_spec"]], "sfm.utils.science_tokens module": [[33, "module-sfm.utils.science_tokens"]], "Data Pipeline": [[35, "data-pipeline"]], "Dynamic Batching": [[35, "dynamic-batching"]], "Weighted Distributed Sampler": [[35, "weighted-distributed-sampler"]]}, "indexentries": {"weighteddistributedsampler (class in sfm.data.sampler)": [[35, "sfm.data.sampler.WeightedDistributedSampler"]], "num_tokens() (sfm.data.dataset.foundationmodeldataset method)": [[35, "sfm.data.dataset.FoundationModelDataset.num_tokens"]]}})
