
-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 2:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                             4
data parallel size:                                                     4
model parallel size:                                                    1
batch size per GPU:                                                     2
params per GPU:                                                         227.98 M
params of model = params per GPU * mp_size:                             227.98 M
fwd MACs per GPU:                                                       540.17 GMACs
fwd flops per GPU:                                                      1.08 T
fwd flops of model = fwd flops per GPU * mp_size:                       1.08 T
fwd latency:                                                            182.81 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    5.91 TFLOPS
bwd latency:                                                            128.91 ms
bwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency:                16.77 TFLOPS
fwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency):      10.4 TFLOPS
step latency:                                                           34.35 ms
iter latency:                                                           346.08 ms
FLOPS per GPU = 3 * fwd flops per GPU / iter latency:                   9.37 TFLOPS
samples/second:                                                         23.12

----------------------------- Aggregated Profile per GPU -----------------------------
Top 1 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'PSMModel': '227.98 M'}
    MACs        - {'PSMModel': '540.17 GMACs'}
    fwd latency - {'PSMModel': '182.69 ms'}
depth 1:
    params      - {'PSM': '227.98 M'}
    MACs        - {'PSM': '540.17 GMACs'}
    fwd latency - {'PSM': '180.15 ms'}
depth 2:
    params      - {'PSMEncoder': '154.76 M'}
    MACs        - {'PSMEncoder': '456.16 GMACs'}
    fwd latency - {'PSMEncoder': '149.24 ms'}
depth 3:
    params      - {'ModuleList': '152.94 M'}
    MACs        - {'ModuleList': '456.16 GMACs'}
    fwd latency - {'ModuleList': '131.25 ms'}
depth 4:
    params      - {'GraphormerSentenceEncoderLayer': '152.94 M'}
    MACs        - {'GraphormerSentenceEncoderLayer': '456.16 GMACs'}
    fwd latency - {'GraphormerSentenceEncoderLayer': '131.25 ms'}
depth 5:
    params      - {'Linear': '102.86 M'}
    MACs        - {'PSMBias': '386.8 GMACs'}
    fwd latency - {'PSMBias': '104.91 ms'}
depth 6:
    params      - {'Linear': '93.88 M'}
    MACs        - {'Linear': '394.8 GMACs'}
    fwd latency - {'Linear': '29.08 ms'}
depth 7:
    params      - {'Linear': '16.99 M'}
    MACs        - {'Linear': '61.35 GMACs'}
    fwd latency - {'Linear': '7.8 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order:
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

PSMModel(
  227.98 M = 100% Params, 540.17 GMACs = 100% MACs, 182.69 ms = 100% latency, 5.92 TFLOPS
  (net): PSM(
    227.98 M = 100% Params, 540.17 GMACs = 100% MACs, 180.15 ms = 98.61% latency, 6 TFLOPS
    (embedding): PSMMixEmbedding(
      7.81 M = 3.43% Params, 26.7 GMACs = 4.94% MACs, 7.88 ms = 4.32% latency, 6.78 TFLOPS
      (embed): Embedding(163.84 K = 0.07% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 0 FLOPS, 160, 1024)
      (atom_feature_embed): Embedding(5.24 M = 2.3% Params, 0 MACs = 0% MACs, 73.19 us = 0.04% latency, 0 FLOPS, 5120, 1024)
      (time_step_encoder): TimeStepEncoder(
        2.1 M = 0.92% Params, 4.19 MMACs = 0% MACs, 513.08 us = 0.28% latency, 16.35 GFLOPS
        (time_proj): SinusoidalPositionEmbeddings(0 = 0% Params, 0 MACs = 0% MACs, 195.98 us = 0.11% latency, 0 FLOPS)
        (time_embedding): Sequential(
          2.1 M = 0.92% Params, 4.19 MMACs = 0% MACs, 231.5 us = 0.13% latency, 36.24 GFLOPS
          (0): Linear(1.05 M = 0.46% Params, 2.1 MMACs = 0% MACs, 81.78 us = 0.04% latency, 51.29 GFLOPS, in_features=1024, out_features=1024, bias=True)
          (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 43.87 us = 0.02% latency, 46.68 MFLOPS, approximate='none')
          (2): Linear(1.05 M = 0.46% Params, 2.1 MMACs = 0% MACs, 46.97 us = 0.03% latency, 89.3 GFLOPS, in_features=1024, out_features=1024, bias=True)
        )
      )
      (pos_embedding_bias): PSMBias(
        152.97 K = 0.07% Params, 13.35 GMACs = 2.47% MACs, 3.39 ms = 1.86% latency, 7.87 TFLOPS
        (gbf): GaussianLayer(
          3.33 K = 0% Params, 0 MACs = 0% MACs, 691.65 us = 0.38% latency, 0 FLOPS
          (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
          (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
          (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 51.98 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
          (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 38.15 us = 0.02% latency, 0 FLOPS, 1536, 1, padding_idx=0)
        )
        (gbf_proj): NonLinear(
          17.54 K = 0.01% Params, 1.56 GMACs = 0.29% MACs, 449.9 us = 0.25% latency, 6.98 TFLOPS
          (layer1): Linear(16.51 K = 0.01% Params, 1.47 GMACs = 0.27% MACs, 235.56 us = 0.13% latency, 12.5 TFLOPS, in_features=128, out_features=128, bias=True)
          (layer2): Linear(1.03 K = 0% Params, 92.05 MMACs = 0.02% MACs, 108.48 us = 0.06% latency, 1.7 TFLOPS, in_features=128, out_features=8, bias=True)
        )
        (pos_embedding_proj): Linear(132.1 K = 0.06% Params, 11.78 GMACs = 2.18% MACs, 833.51 us = 0.46% latency, 28.27 TFLOPS, in_features=128, out_features=1024, bias=True)
      )
      (init_pos_embedding_bias): PSMBias(
        152.97 K = 0.07% Params, 13.35 GMACs = 2.47% MACs, 3.24 ms = 1.77% latency, 8.25 TFLOPS
        (gbf): GaussianLayer(
          3.33 K = 0% Params, 0 MACs = 0% MACs, 652.55 us = 0.36% latency, 0 FLOPS
          (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
          (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
          (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 48.88 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
          (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 37.67 us = 0.02% latency, 0 FLOPS, 1536, 1, padding_idx=0)
        )
        (gbf_proj): NonLinear(
          17.54 K = 0.01% Params, 1.56 GMACs = 0.29% MACs, 388.86 us = 0.21% latency, 8.08 TFLOPS
          (layer1): Linear(16.51 K = 0.01% Params, 1.47 GMACs = 0.27% MACs, 189.54 us = 0.1% latency, 15.54 TFLOPS, in_features=128, out_features=128, bias=True)
          (layer2): Linear(1.03 K = 0% Params, 92.05 MMACs = 0.02% MACs, 92.74 us = 0.05% latency, 1.98 TFLOPS, in_features=128, out_features=8, bias=True)
        )
        (pos_embedding_proj): Linear(132.1 K = 0.06% Params, 11.78 GMACs = 2.18% MACs, 816.82 us = 0.45% latency, 28.85 TFLOPS, in_features=128, out_features=1024, bias=True)
      )
    )
    (encoder): PSMEncoder(
      154.76 M = 67.88% Params, 456.16 GMACs = 84.45% MACs, 149.24 ms = 81.69% latency, 6.12 TFLOPS
      (layers): ModuleList(
        (0): GraphormerSentenceEncoderLayer(
          12.75 M = 5.59% Params, 38.01 GMACs = 7.04% MACs, 11.21 ms = 6.14% latency, 6.78 TFLOPS
          (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 95.61 us = 0.05% latency, 0 FLOPS)
          (activation_dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
          (pre_attn_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 164.03 us = 0.09% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (pre_mlp_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 107.29 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            4.19 M = 1.84% Params, 2.22 GMACs = 0.41% MACs, 1.11 ms = 0.61% latency, 4.01 TFLOPS
            (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 69.62 us = 0.04% latency, 0 FLOPS)
            (k_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 78.44 us = 0.04% latency, 11.34 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 88.21 us = 0.05% latency, 10.08 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (q_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 124.45 us = 0.07% latency, 7.14 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (out_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 88.45 us = 0.05% latency, 10.05 TFLOPS, in_features=1024, out_features=1024, bias=False)
          )
          (fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 198.6 us = 0.11% latency, 17.91 TFLOPS, in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 242.95 us = 0.13% latency, 14.64 TFLOPS, in_features=4096, out_features=1024, bias=True)
          (attn_bias): PSMBias(
            152.97 K = 0.07% Params, 32.23 GMACs = 5.97% MACs, 8.82 ms = 4.83% latency, 7.31 TFLOPS
            (gbf): GaussianLayer(
              3.33 K = 0% Params, 0 MACs = 0% MACs, 1.23 ms = 0.67% latency, 0 FLOPS
              (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 48.64 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
              (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 38.62 us = 0.02% latency, 0 FLOPS, 1536, 1, padding_idx=0)
            )
            (gbf_proj): NonLinear(
              17.54 K = 0.01% Params, 3.78 GMACs = 0.7% MACs, 711.2 us = 0.39% latency, 10.67 TFLOPS
              (layer1): Linear(16.51 K = 0.01% Params, 3.56 GMACs = 0.66% MACs, 367.88 us = 0.2% latency, 19.34 TFLOPS, in_features=128, out_features=128, bias=True)
              (layer2): Linear(1.03 K = 0% Params, 222.3 MMACs = 0.04% MACs, 161.65 us = 0.09% latency, 2.75 TFLOPS, in_features=128, out_features=8, bias=True)
            )
            (pos_embedding_proj): Linear(132.1 K = 0.06% Params, 28.45 GMACs = 5.27% MACs, 1.86 ms = 1.02% latency, 30.54 TFLOPS, in_features=128, out_features=1024, bias=True)
          )
        )
        (1): GraphormerSentenceEncoderLayer(
          12.75 M = 5.59% Params, 38.01 GMACs = 7.04% MACs, 10.98 ms = 6.01% latency, 6.93 TFLOPS
          (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 99.9 us = 0.05% latency, 0 FLOPS)
          (activation_dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
          (pre_attn_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 117.3 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (pre_mlp_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 130.18 us = 0.07% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            4.19 M = 1.84% Params, 2.22 GMACs = 0.41% MACs, 1.03 ms = 0.56% latency, 4.32 TFLOPS
            (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 55.07 us = 0.03% latency, 0 FLOPS)
            (k_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 77.49 us = 0.04% latency, 11.48 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 75.58 us = 0.04% latency, 11.77 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (q_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 104.67 us = 0.06% latency, 8.5 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (out_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 117.78 us = 0.06% latency, 7.55 TFLOPS, in_features=1024, out_features=1024, bias=False)
          )
          (fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 184.06 us = 0.1% latency, 19.32 TFLOPS, in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 225.78 us = 0.12% latency, 15.75 TFLOPS, in_features=4096, out_features=1024, bias=True)
          (attn_bias): PSMBias(
            152.97 K = 0.07% Params, 32.23 GMACs = 5.97% MACs, 8.74 ms = 4.78% latency, 7.38 TFLOPS
            (gbf): GaussianLayer(
              3.33 K = 0% Params, 0 MACs = 0% MACs, 1.23 ms = 0.67% latency, 0 FLOPS
              (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 50.78 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
              (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 38.86 us = 0.02% latency, 0 FLOPS, 1536, 1, padding_idx=0)
            )
            (gbf_proj): NonLinear(
              17.54 K = 0.01% Params, 3.78 GMACs = 0.7% MACs, 652.31 us = 0.36% latency, 11.63 TFLOPS
              (layer1): Linear(16.51 K = 0.01% Params, 3.56 GMACs = 0.66% MACs, 328.3 us = 0.18% latency, 21.67 TFLOPS, in_features=128, out_features=128, bias=True)
              (layer2): Linear(1.03 K = 0% Params, 222.3 MMACs = 0.04% MACs, 146.87 us = 0.08% latency, 3.03 TFLOPS, in_features=128, out_features=8, bias=True)
            )
            (pos_embedding_proj): Linear(132.1 K = 0.06% Params, 28.45 GMACs = 5.27% MACs, 1.85 ms = 1.01% latency, 30.84 TFLOPS, in_features=128, out_features=1024, bias=True)
          )
        )
        (2): GraphormerSentenceEncoderLayer(
          12.75 M = 5.59% Params, 38.01 GMACs = 7.04% MACs, 10.91 ms = 5.97% latency, 6.97 TFLOPS
          (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 94.65 us = 0.05% latency, 0 FLOPS)
          (activation_dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
          (pre_attn_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 119.45 us = 0.07% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (pre_mlp_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 107.05 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            4.19 M = 1.84% Params, 2.22 GMACs = 0.41% MACs, 987.05 us = 0.54% latency, 4.51 TFLOPS
            (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 56.27 us = 0.03% latency, 0 FLOPS)
            (k_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 76.53 us = 0.04% latency, 11.62 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 73.43 us = 0.04% latency, 12.11 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (q_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 101.33 us = 0.06% latency, 8.78 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (out_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 85.12 us = 0.05% latency, 10.45 TFLOPS, in_features=1024, out_features=1024, bias=False)
          )
          (fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 180.48 us = 0.1% latency, 19.71 TFLOPS, in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 226.97 us = 0.12% latency, 15.67 TFLOPS, in_features=4096, out_features=1024, bias=True)
          (attn_bias): PSMBias(
            152.97 K = 0.07% Params, 32.23 GMACs = 5.97% MACs, 8.73 ms = 4.78% latency, 7.39 TFLOPS
            (gbf): GaussianLayer(
              3.33 K = 0% Params, 0 MACs = 0% MACs, 1.23 ms = 0.67% latency, 0 FLOPS
              (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 49.83 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
              (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 37.19 us = 0.02% latency, 0 FLOPS, 1536, 1, padding_idx=0)
            )
            (gbf_proj): NonLinear(
              17.54 K = 0.01% Params, 3.78 GMACs = 0.7% MACs, 646.35 us = 0.35% latency, 11.74 TFLOPS
              (layer1): Linear(16.51 K = 0.01% Params, 3.56 GMACs = 0.66% MACs, 325.92 us = 0.18% latency, 21.83 TFLOPS, in_features=128, out_features=128, bias=True)
              (layer2): Linear(1.03 K = 0% Params, 222.3 MMACs = 0.04% MACs, 142.34 us = 0.08% latency, 3.12 TFLOPS, in_features=128, out_features=8, bias=True)
            )
            (pos_embedding_proj): Linear(132.1 K = 0.06% Params, 28.45 GMACs = 5.27% MACs, 1.84 ms = 1.01% latency, 30.91 TFLOPS, in_features=128, out_features=1024, bias=True)
          )
        )
        (3): GraphormerSentenceEncoderLayer(
          12.75 M = 5.59% Params, 38.01 GMACs = 7.04% MACs, 10.93 ms = 5.98% latency, 6.96 TFLOPS
          (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 92.27 us = 0.05% latency, 0 FLOPS)
          (activation_dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
          (pre_attn_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 121.36 us = 0.07% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (pre_mlp_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 107.05 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            4.19 M = 1.84% Params, 2.22 GMACs = 0.41% MACs, 984.19 us = 0.54% latency, 4.52 TFLOPS
            (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 52.93 us = 0.03% latency, 0 FLOPS)
            (k_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 78.92 us = 0.04% latency, 11.27 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 75.1 us = 0.04% latency, 11.84 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (q_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 98.23 us = 0.05% latency, 9.05 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (out_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 86.78 us = 0.05% latency, 10.25 TFLOPS, in_features=1024, out_features=1024, bias=False)
          )
          (fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 184.54 us = 0.1% latency, 19.27 TFLOPS, in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 225.54 us = 0.12% latency, 15.77 TFLOPS, in_features=4096, out_features=1024, bias=True)
          (attn_bias): PSMBias(
            152.97 K = 0.07% Params, 32.23 GMACs = 5.97% MACs, 8.76 ms = 4.79% latency, 7.37 TFLOPS
            (gbf): GaussianLayer(
              3.33 K = 0% Params, 0 MACs = 0% MACs, 1.25 ms = 0.68% latency, 0 FLOPS
              (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 51.5 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
              (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 48.16 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
            )
            (gbf_proj): NonLinear(
              17.54 K = 0.01% Params, 3.78 GMACs = 0.7% MACs, 652.07 us = 0.36% latency, 11.63 TFLOPS
              (layer1): Linear(16.51 K = 0.01% Params, 3.56 GMACs = 0.66% MACs, 325.44 us = 0.18% latency, 21.86 TFLOPS, in_features=128, out_features=128, bias=True)
              (layer2): Linear(1.03 K = 0% Params, 222.3 MMACs = 0.04% MACs, 147.82 us = 0.08% latency, 3.01 TFLOPS, in_features=128, out_features=8, bias=True)
            )
            (pos_embedding_proj): Linear(132.1 K = 0.06% Params, 28.45 GMACs = 5.27% MACs, 1.84 ms = 1.01% latency, 30.87 TFLOPS, in_features=128, out_features=1024, bias=True)
          )
        )
        (4): GraphormerSentenceEncoderLayer(
          12.75 M = 5.59% Params, 38.01 GMACs = 7.04% MACs, 10.91 ms = 5.97% latency, 6.97 TFLOPS
          (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 93.46 us = 0.05% latency, 0 FLOPS)
          (activation_dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
          (pre_attn_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 122.31 us = 0.07% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (pre_mlp_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 103.95 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            4.19 M = 1.84% Params, 2.22 GMACs = 0.41% MACs, 1.01 ms = 0.55% latency, 4.4 TFLOPS
            (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 53.64 us = 0.03% latency, 0 FLOPS)
            (k_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 76.77 us = 0.04% latency, 11.58 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 73.91 us = 0.04% latency, 12.03 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (q_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 100.61 us = 0.06% latency, 8.84 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (out_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 84.64 us = 0.05% latency, 10.51 TFLOPS, in_features=1024, out_features=1024, bias=False)
          )
          (fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 181.44 us = 0.1% latency, 19.6 TFLOPS, in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 226.26 us = 0.12% latency, 15.72 TFLOPS, in_features=4096, out_features=1024, bias=True)
          (attn_bias): PSMBias(
            152.97 K = 0.07% Params, 32.23 GMACs = 5.97% MACs, 8.71 ms = 4.77% latency, 7.4 TFLOPS
            (gbf): GaussianLayer(
              3.33 K = 0% Params, 0 MACs = 0% MACs, 1.22 ms = 0.67% latency, 0 FLOPS
              (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 49.83 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
              (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 39.82 us = 0.02% latency, 0 FLOPS, 1536, 1, padding_idx=0)
            )
            (gbf_proj): NonLinear(
              17.54 K = 0.01% Params, 3.78 GMACs = 0.7% MACs, 649.69 us = 0.36% latency, 11.68 TFLOPS
              (layer1): Linear(16.51 K = 0.01% Params, 3.56 GMACs = 0.66% MACs, 326.63 us = 0.18% latency, 21.78 TFLOPS, in_features=128, out_features=128, bias=True)
              (layer2): Linear(1.03 K = 0% Params, 222.3 MMACs = 0.04% MACs, 140.43 us = 0.08% latency, 3.17 TFLOPS, in_features=128, out_features=8, bias=True)
            )
            (pos_embedding_proj): Linear(132.1 K = 0.06% Params, 28.45 GMACs = 5.27% MACs, 1.83 ms = 1% latency, 31.15 TFLOPS, in_features=128, out_features=1024, bias=True)
          )
        )
        (5): GraphormerSentenceEncoderLayer(
          12.75 M = 5.59% Params, 38.01 GMACs = 7.04% MACs, 10.9 ms = 5.97% latency, 6.98 TFLOPS
          (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 92.51 us = 0.05% latency, 0 FLOPS)
          (activation_dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
          (pre_attn_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 117.78 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (pre_mlp_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 103.95 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            4.19 M = 1.84% Params, 2.22 GMACs = 0.41% MACs, 981.57 us = 0.54% latency, 4.53 TFLOPS
            (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 55.07 us = 0.03% latency, 0 FLOPS)
            (k_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 77.25 us = 0.04% latency, 11.51 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 74.86 us = 0.04% latency, 11.88 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (q_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 100.37 us = 0.05% latency, 8.86 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (out_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 85.59 us = 0.05% latency, 10.39 TFLOPS, in_features=1024, out_features=1024, bias=False)
          )
          (fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 181.2 us = 0.1% latency, 19.63 TFLOPS, in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 227.69 us = 0.12% latency, 15.62 TFLOPS, in_features=4096, out_features=1024, bias=True)
          (attn_bias): PSMBias(
            152.97 K = 0.07% Params, 32.23 GMACs = 5.97% MACs, 8.73 ms = 4.78% latency, 7.39 TFLOPS
            (gbf): GaussianLayer(
              3.33 K = 0% Params, 0 MACs = 0% MACs, 1.22 ms = 0.67% latency, 0 FLOPS
              (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 49.83 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
              (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 37.43 us = 0.02% latency, 0 FLOPS, 1536, 1, padding_idx=0)
            )
            (gbf_proj): NonLinear(
              17.54 K = 0.01% Params, 3.78 GMACs = 0.7% MACs, 645.16 us = 0.35% latency, 11.76 TFLOPS
              (layer1): Linear(16.51 K = 0.01% Params, 3.56 GMACs = 0.66% MACs, 326.16 us = 0.18% latency, 21.81 TFLOPS, in_features=128, out_features=128, bias=True)
              (layer2): Linear(1.03 K = 0% Params, 222.3 MMACs = 0.04% MACs, 142.81 us = 0.08% latency, 3.11 TFLOPS, in_features=128, out_features=8, bias=True)
            )
            (pos_embedding_proj): Linear(132.1 K = 0.06% Params, 28.45 GMACs = 5.27% MACs, 1.84 ms = 1.01% latency, 30.85 TFLOPS, in_features=128, out_features=1024, bias=True)
          )
        )
        (6): GraphormerSentenceEncoderLayer(
          12.75 M = 5.59% Params, 38.01 GMACs = 7.04% MACs, 10.9 ms = 5.97% latency, 6.98 TFLOPS
          (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 90.84 us = 0.05% latency, 0 FLOPS)
          (activation_dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
          (pre_attn_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 125.17 us = 0.07% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (pre_mlp_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 104.43 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            4.19 M = 1.84% Params, 2.22 GMACs = 0.41% MACs, 977.52 us = 0.54% latency, 4.55 TFLOPS
            (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 56.74 us = 0.03% latency, 0 FLOPS)
            (k_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 76.29 us = 0.04% latency, 11.65 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 72.48 us = 0.04% latency, 12.27 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (q_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 99.42 us = 0.05% latency, 8.94 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (out_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 83.68 us = 0.05% latency, 10.63 TFLOPS, in_features=1024, out_features=1024, bias=False)
          )
          (fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 179.53 us = 0.1% latency, 19.81 TFLOPS, in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 225.54 us = 0.12% latency, 15.77 TFLOPS, in_features=4096, out_features=1024, bias=True)
          (attn_bias): PSMBias(
            152.97 K = 0.07% Params, 32.23 GMACs = 5.97% MACs, 8.74 ms = 4.78% latency, 7.38 TFLOPS
            (gbf): GaussianLayer(
              3.33 K = 0% Params, 0 MACs = 0% MACs, 1.22 ms = 0.67% latency, 0 FLOPS
              (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 50.07 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
              (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 37.67 us = 0.02% latency, 0 FLOPS, 1536, 1, padding_idx=0)
            )
            (gbf_proj): NonLinear(
              17.54 K = 0.01% Params, 3.78 GMACs = 0.7% MACs, 647.78 us = 0.35% latency, 11.71 TFLOPS
              (layer1): Linear(16.51 K = 0.01% Params, 3.56 GMACs = 0.66% MACs, 327.11 us = 0.18% latency, 21.75 TFLOPS, in_features=128, out_features=128, bias=True)
              (layer2): Linear(1.03 K = 0% Params, 222.3 MMACs = 0.04% MACs, 142.81 us = 0.08% latency, 3.11 TFLOPS, in_features=128, out_features=8, bias=True)
            )
            (pos_embedding_proj): Linear(132.1 K = 0.06% Params, 28.45 GMACs = 5.27% MACs, 1.84 ms = 1.01% latency, 30.92 TFLOPS, in_features=128, out_features=1024, bias=True)
          )
        )
        (7): GraphormerSentenceEncoderLayer(
          12.75 M = 5.59% Params, 38.01 GMACs = 7.04% MACs, 10.86 ms = 5.95% latency, 7 TFLOPS
          (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 91.31 us = 0.05% latency, 0 FLOPS)
          (activation_dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
          (pre_attn_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 118.73 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (pre_mlp_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 103.47 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            4.19 M = 1.84% Params, 2.22 GMACs = 0.41% MACs, 969.65 us = 0.53% latency, 4.59 TFLOPS
            (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 0 FLOPS)
            (k_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 77.96 us = 0.04% latency, 11.41 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 73.91 us = 0.04% latency, 12.03 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (q_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 101.8 us = 0.06% latency, 8.73 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (out_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 84.64 us = 0.05% latency, 10.51 TFLOPS, in_features=1024, out_features=1024, bias=False)
          )
          (fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 178.81 us = 0.1% latency, 19.89 TFLOPS, in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 223.88 us = 0.12% latency, 15.89 TFLOPS, in_features=4096, out_features=1024, bias=True)
          (attn_bias): PSMBias(
            152.97 K = 0.07% Params, 32.23 GMACs = 5.97% MACs, 8.72 ms = 4.77% latency, 7.39 TFLOPS
            (gbf): GaussianLayer(
              3.33 K = 0% Params, 0 MACs = 0% MACs, 1.22 ms = 0.67% latency, 0 FLOPS
              (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 50.78 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
              (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 36.24 us = 0.02% latency, 0 FLOPS, 1536, 1, padding_idx=0)
            )
            (gbf_proj): NonLinear(
              17.54 K = 0.01% Params, 3.78 GMACs = 0.7% MACs, 646.59 us = 0.35% latency, 11.73 TFLOPS
              (layer1): Linear(16.51 K = 0.01% Params, 3.56 GMACs = 0.66% MACs, 327.59 us = 0.18% latency, 21.71 TFLOPS, in_features=128, out_features=128, bias=True)
              (layer2): Linear(1.03 K = 0% Params, 222.3 MMACs = 0.04% MACs, 142.1 us = 0.08% latency, 3.13 TFLOPS, in_features=128, out_features=8, bias=True)
            )
            (pos_embedding_proj): Linear(132.1 K = 0.06% Params, 28.45 GMACs = 5.27% MACs, 1.84 ms = 1.01% latency, 30.89 TFLOPS, in_features=128, out_features=1024, bias=True)
          )
        )
        (8): GraphormerSentenceEncoderLayer(
          12.75 M = 5.59% Params, 38.01 GMACs = 7.04% MACs, 10.95 ms = 6% latency, 6.94 TFLOPS
          (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 92.74 us = 0.05% latency, 0 FLOPS)
          (activation_dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
          (pre_attn_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 122.31 us = 0.07% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (pre_mlp_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 104.43 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            4.19 M = 1.84% Params, 2.22 GMACs = 0.41% MACs, 985.62 us = 0.54% latency, 4.51 TFLOPS
            (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 53.64 us = 0.03% latency, 0 FLOPS)
            (k_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 77.49 us = 0.04% latency, 11.48 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 75.34 us = 0.04% latency, 11.8 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (q_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 101.8 us = 0.06% latency, 8.73 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (out_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 86.07 us = 0.05% latency, 10.33 TFLOPS, in_features=1024, out_features=1024, bias=False)
          )
          (fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 196.93 us = 0.11% latency, 18.06 TFLOPS, in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 226.5 us = 0.12% latency, 15.7 TFLOPS, in_features=4096, out_features=1024, bias=True)
          (attn_bias): PSMBias(
            152.97 K = 0.07% Params, 32.23 GMACs = 5.97% MACs, 8.74 ms = 4.79% latency, 7.38 TFLOPS
            (gbf): GaussianLayer(
              3.33 K = 0% Params, 0 MACs = 0% MACs, 1.23 ms = 0.67% latency, 0 FLOPS
              (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 51.74 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
              (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 37.67 us = 0.02% latency, 0 FLOPS, 1536, 1, padding_idx=0)
            )
            (gbf_proj): NonLinear(
              17.54 K = 0.01% Params, 3.78 GMACs = 0.7% MACs, 649.21 us = 0.36% latency, 11.68 TFLOPS
              (layer1): Linear(16.51 K = 0.01% Params, 3.56 GMACs = 0.66% MACs, 325.68 us = 0.18% latency, 21.84 TFLOPS, in_features=128, out_features=128, bias=True)
              (layer2): Linear(1.03 K = 0% Params, 222.3 MMACs = 0.04% MACs, 144.48 us = 0.08% latency, 3.08 TFLOPS, in_features=128, out_features=8, bias=True)
            )
            (pos_embedding_proj): Linear(132.1 K = 0.06% Params, 28.45 GMACs = 5.27% MACs, 1.86 ms = 1.02% latency, 30.63 TFLOPS, in_features=128, out_features=1024, bias=True)
          )
        )
        (9): GraphormerSentenceEncoderLayer(
          12.75 M = 5.59% Params, 38.01 GMACs = 7.04% MACs, 10.93 ms = 5.98% latency, 6.96 TFLOPS
          (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 96.8 us = 0.05% latency, 0 FLOPS)
          (activation_dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
          (pre_attn_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 115.16 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (pre_mlp_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 104.67 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            4.19 M = 1.84% Params, 2.22 GMACs = 0.41% MACs, 974.89 us = 0.53% latency, 4.56 TFLOPS
            (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 56.03 us = 0.03% latency, 0 FLOPS)
            (k_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 77.25 us = 0.04% latency, 11.51 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 74.86 us = 0.04% latency, 11.88 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (q_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 101.57 us = 0.06% latency, 8.75 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (out_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 86.55 us = 0.05% latency, 10.27 TFLOPS, in_features=1024, out_features=1024, bias=False)
          )
          (fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 180.96 us = 0.1% latency, 19.66 TFLOPS, in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 227.69 us = 0.12% latency, 15.62 TFLOPS, in_features=4096, out_features=1024, bias=True)
          (attn_bias): PSMBias(
            152.97 K = 0.07% Params, 32.23 GMACs = 5.97% MACs, 8.77 ms = 4.8% latency, 7.36 TFLOPS
            (gbf): GaussianLayer(
              3.33 K = 0% Params, 0 MACs = 0% MACs, 1.25 ms = 0.68% latency, 0 FLOPS
              (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 66.52 us = 0.04% latency, 0 FLOPS, 1536, 1, padding_idx=0)
              (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 38.62 us = 0.02% latency, 0 FLOPS, 1536, 1, padding_idx=0)
            )
            (gbf_proj): NonLinear(
              17.54 K = 0.01% Params, 3.78 GMACs = 0.7% MACs, 646.59 us = 0.35% latency, 11.73 TFLOPS
              (layer1): Linear(16.51 K = 0.01% Params, 3.56 GMACs = 0.66% MACs, 325.92 us = 0.18% latency, 21.83 TFLOPS, in_features=128, out_features=128, bias=True)
              (layer2): Linear(1.03 K = 0% Params, 222.3 MMACs = 0.04% MACs, 144.24 us = 0.08% latency, 3.08 TFLOPS, in_features=128, out_features=8, bias=True)
            )
            (pos_embedding_proj): Linear(132.1 K = 0.06% Params, 28.45 GMACs = 5.27% MACs, 1.84 ms = 1.01% latency, 30.98 TFLOPS, in_features=128, out_features=1024, bias=True)
          )
        )
        (10): GraphormerSentenceEncoderLayer(
          12.75 M = 5.59% Params, 38.01 GMACs = 7.04% MACs, 10.87 ms = 5.95% latency, 7 TFLOPS
          (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 92.27 us = 0.05% latency, 0 FLOPS)
          (activation_dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
          (pre_attn_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 122.55 us = 0.07% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (pre_mlp_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 103 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            4.19 M = 1.84% Params, 2.22 GMACs = 0.41% MACs, 965.83 us = 0.53% latency, 4.61 TFLOPS
            (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 54.36 us = 0.03% latency, 0 FLOPS)
            (k_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 77.96 us = 0.04% latency, 11.41 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 74.39 us = 0.04% latency, 11.95 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (q_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 100.85 us = 0.06% latency, 8.82 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (out_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 84.88 us = 0.05% latency, 10.48 TFLOPS, in_features=1024, out_features=1024, bias=False)
          )
          (fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 181.2 us = 0.1% latency, 19.63 TFLOPS, in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 226.74 us = 0.12% latency, 15.69 TFLOPS, in_features=4096, out_features=1024, bias=True)
          (attn_bias): PSMBias(
            152.97 K = 0.07% Params, 32.23 GMACs = 5.97% MACs, 8.72 ms = 4.77% latency, 7.4 TFLOPS
            (gbf): GaussianLayer(
              3.33 K = 0% Params, 0 MACs = 0% MACs, 1.22 ms = 0.67% latency, 0 FLOPS
              (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 50.07 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
              (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 38.15 us = 0.02% latency, 0 FLOPS, 1536, 1, padding_idx=0)
            )
            (gbf_proj): NonLinear(
              17.54 K = 0.01% Params, 3.78 GMACs = 0.7% MACs, 647.54 us = 0.35% latency, 11.71 TFLOPS
              (layer1): Linear(16.51 K = 0.01% Params, 3.56 GMACs = 0.66% MACs, 328.06 us = 0.18% latency, 21.68 TFLOPS, in_features=128, out_features=128, bias=True)
              (layer2): Linear(1.03 K = 0% Params, 222.3 MMACs = 0.04% MACs, 143.53 us = 0.08% latency, 3.1 TFLOPS, in_features=128, out_features=8, bias=True)
            )
            (pos_embedding_proj): Linear(132.1 K = 0.06% Params, 28.45 GMACs = 5.27% MACs, 1.84 ms = 1.01% latency, 30.92 TFLOPS, in_features=128, out_features=1024, bias=True)
          )
        )
        (11): GraphormerSentenceEncoderLayer(
          12.75 M = 5.59% Params, 38.01 GMACs = 7.04% MACs, 10.88 ms = 5.96% latency, 6.99 TFLOPS
          (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 92.74 us = 0.05% latency, 0 FLOPS)
          (activation_dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
          (pre_attn_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 118.49 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (pre_mlp_norm): FusedLayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 103.47 us = 0.06% latency, 0 FLOPS, torch.Size([1024]), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            4.19 M = 1.84% Params, 2.22 GMACs = 0.41% MACs, 972.51 us = 0.53% latency, 4.57 TFLOPS
            (dropout_module): FairseqDropout(0 = 0% Params, 0 MACs = 0% MACs, 55.07 us = 0.03% latency, 0 FLOPS)
            (k_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 77.25 us = 0.04% latency, 11.51 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (v_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 74.86 us = 0.04% latency, 11.88 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (q_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 101.33 us = 0.06% latency, 8.78 TFLOPS, in_features=1024, out_features=1024, bias=False)
            (out_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 84.4 us = 0.05% latency, 10.54 TFLOPS, in_features=1024, out_features=1024, bias=False)
          )
          (fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 179.53 us = 0.1% latency, 19.81 TFLOPS, in_features=1024, out_features=4096, bias=True)
          (fc2): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 228.17 us = 0.12% latency, 15.59 TFLOPS, in_features=4096, out_features=1024, bias=True)
          (attn_bias): PSMBias(
            152.97 K = 0.07% Params, 32.23 GMACs = 5.97% MACs, 8.73 ms = 4.78% latency, 7.39 TFLOPS
            (gbf): GaussianLayer(
              3.33 K = 0% Params, 0 MACs = 0% MACs, 1.22 ms = 0.67% latency, 0 FLOPS
              (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
              (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 50.07 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
              (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 37.19 us = 0.02% latency, 0 FLOPS, 1536, 1, padding_idx=0)
            )
            (gbf_proj): NonLinear(
              17.54 K = 0.01% Params, 3.78 GMACs = 0.7% MACs, 648.5 us = 0.35% latency, 11.7 TFLOPS
              (layer1): Linear(16.51 K = 0.01% Params, 3.56 GMACs = 0.66% MACs, 326.16 us = 0.18% latency, 21.81 TFLOPS, in_features=128, out_features=128, bias=True)
              (layer2): Linear(1.03 K = 0% Params, 222.3 MMACs = 0.04% MACs, 143.53 us = 0.08% latency, 3.1 TFLOPS, in_features=128, out_features=8, bias=True)
            )
            (pos_embedding_proj): Linear(132.1 K = 0.06% Params, 28.45 GMACs = 5.27% MACs, 1.84 ms = 1.01% latency, 30.91 TFLOPS, in_features=128, out_features=1024, bias=True)
          )
        )
      )
      (graph_2d_attention_bias): GraphAttnBias(
        1.82 M = 0.8% Params, 0 MACs = 0% MACs, 963.93 us = 0.53% latency, 0 FLOPS
        (edge_encoder): Embedding(196.74 K = 0.09% Params, 0 MACs = 0% MACs, 39.58 us = 0.02% latency, 0 FLOPS, 1537, 128, padding_idx=0)
        (edge_dis_encoder): Embedding(1.57 M = 0.69% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1572864, 1)
        (spatial_pos_encoder): Embedding(50.11 K = 0.02% Params, 0 MACs = 0% MACs, 114.68 us = 0.06% latency, 0 FLOPS, 522, 96, padding_idx=0)
        (graph_token_virtual_distance): Embedding(96 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 96)
      )
    )
    (decoder): EquivariantDecoder(
      63.14 M = 27.69% Params, 56.35 GMACs = 10.43% MACs, 21.46 ms = 11.75% latency, 5.25 TFLOPS
      (model): GeomFormer(
        63.14 M = 27.69% Params, 56.35 GMACs = 10.43% MACs, 21.43 ms = 11.73% latency, 5.26 TFLOPS
        (unified_encoder_layers): ModuleList(
          (0): EncoderLayer(
            29.39 M = 12.89% Params, 25.34 GMACs = 4.69% MACs, 8.72 ms = 4.78% latency, 5.81 TFLOPS
            (invariant_self_attention): InvariantSelfAttention(
              4.2 M = 1.84% Params, 2.22 GMACs = 0.41% MACs, 1.1 ms = 0.6% latency, 4.03 TFLOPS
              (q_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 122.07 us = 0.07% latency, 7.28 TFLOPS, in_features=1024, out_features=1024, bias=True)
              (k_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 89.65 us = 0.05% latency, 9.92 TFLOPS, in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 84.88 us = 0.05% latency, 10.48 TFLOPS, in_features=1024, out_features=1024, bias=True)
              (invariant_attention): InvariantAttention(
                1.05 M = 0.46% Params, 889.19 MMACs = 0.16% MACs, 715.49 us = 0.39% latency, 2.49 TFLOPS
                (out_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 99.42 us = 0.05% latency, 8.94 TFLOPS, in_features=1024, out_features=1024, bias=True)
                (attn_ln): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 58.89 us = 0.03% latency, 36.86 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
            )
            (equivariant_self_attention): EquivariantSelfAttention(
              4.2 M = 1.84% Params, 6.73 GMACs = 1.25% MACs, 2.29 ms = 1.26% latency, 5.87 TFLOPS
              (q_proj): Linear(1.05 M = 0.46% Params, 1.33 GMACs = 0.25% MACs, 172.38 us = 0.09% latency, 15.48 TFLOPS, in_features=1024, out_features=1024, bias=False)
              (k_proj): Linear(1.05 M = 0.46% Params, 1.33 GMACs = 0.25% MACs, 146.39 us = 0.08% latency, 18.22 TFLOPS, in_features=1024, out_features=1024, bias=False)
              (v_proj): Linear(1.05 M = 0.46% Params, 1.33 GMACs = 0.25% MACs, 143.77 us = 0.08% latency, 18.55 TFLOPS, in_features=1024, out_features=1024, bias=False)
              (equiariant_attention): EquivariantAttention(
                1.05 M = 0.46% Params, 2.73 GMACs = 0.51% MACs, 1.75 ms = 0.96% latency, 3.13 TFLOPS
                (out_proj): Linear(1.05 M = 0.46% Params, 1.33 GMACs = 0.25% MACs, 162.36 us = 0.09% latency, 16.43 TFLOPS, in_features=1024, out_features=1024, bias=False)
                (attn_ln): EquivariantLayerNorm(1.02 K = 0% Params, 62.61 MMACs = 0.01% MACs, 972.75 us = 0.53% latency, 128.73 GFLOPS, (1024,), elementwise_linear=True)
              )
            )
            (invariant_attn_layer_norm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 70.81 us = 0.04% latency, 30.66 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
            (equivariant_attn_layer_norm): EquivariantLayerNorm(1.02 K = 0% Params, 62.61 MMACs = 0.01% MACs, 1.04 ms = 0.57% latency, 120 GFLOPS, (1024,), elementwise_linear=True)
            (invariant_fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 179.05 us = 0.1% latency, 19.86 TFLOPS, in_features=1024, out_features=4096, bias=True)
            (invariant_fc2): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 229.6 us = 0.13% latency, 15.49 TFLOPS, in_features=4096, out_features=1024, bias=True)
            (equivariant_fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 169.52 us = 0.09% latency, 20.98 TFLOPS, in_features=1024, out_features=4096, bias=True)
            (equivariant_fc2): Linear(4.19 M = 1.84% Params, 5.34 GMACs = 0.99% MACs, 451.09 us = 0.25% latency, 23.65 TFLOPS, in_features=1024, out_features=4096, bias=False)
            (equivariant_fc3): Linear(4.19 M = 1.84% Params, 5.34 GMACs = 0.99% MACs, 397.68 us = 0.22% latency, 26.83 TFLOPS, in_features=4096, out_features=1024, bias=False)
            (invariant_ffn_layer_norm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 55.07 us = 0.03% latency, 39.42 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
            (equivariant_ffn_layer_norm): EquivariantLayerNorm(1.02 K = 0% Params, 62.61 MMACs = 0.01% MACs, 910.52 us = 0.5% latency, 137.53 GFLOPS, (1024,), elementwise_linear=True)
            (invariant_ffn_layer_norm_2): LayerNorm(8.19 K = 0% Params, 0 MACs = 0% MACs, 53.41 us = 0.03% latency, 162.6 GFLOPS, (4096,), eps=1e-05, elementwise_affine=True)
            (equivariant_ffn_layer_norm_2): EquivariantLayerNorm(4.1 K = 0% Params, 250.18 MMACs = 0.05% MACs, 1.26 ms = 0.69% latency, 398.3 GFLOPS, (4096,), elementwise_linear=True)
          )
          (1): EncoderLayer(
            33.59 M = 14.73% Params, 30.67 GMACs = 5.68% MACs, 9.19 ms = 5.03% latency, 6.68 TFLOPS
            (invariant2equivariant_attention): Invariant2EquivariantAttention(
              6.3 M = 2.76% Params, 6.67 GMACs = 1.23% MACs, 1.59 ms = 0.87% latency, 8.39 TFLOPS
              (q_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 107.29 us = 0.06% latency, 8.29 TFLOPS, in_features=1024, out_features=1024, bias=True)
              (k1_proj): Linear(1.05 M = 0.46% Params, 1.33 GMACs = 0.25% MACs, 148.3 us = 0.08% latency, 17.99 TFLOPS, in_features=1024, out_features=1024, bias=False)
              (k2_proj): Linear(1.05 M = 0.46% Params, 1.33 GMACs = 0.25% MACs, 146.15 us = 0.08% latency, 18.25 TFLOPS, in_features=1024, out_features=1024, bias=False)
              (v1_proj): Linear(1.05 M = 0.46% Params, 1.33 GMACs = 0.25% MACs, 153.3 us = 0.08% latency, 17.4 TFLOPS, in_features=1024, out_features=1024, bias=False)
              (v2_proj): Linear(1.05 M = 0.46% Params, 1.33 GMACs = 0.25% MACs, 146.39 us = 0.08% latency, 18.22 TFLOPS, in_features=1024, out_features=1024, bias=False)
              (invariant_attention): InvariantAttention(
                1.05 M = 0.46% Params, 889.19 MMACs = 0.16% MACs, 659.23 us = 0.36% latency, 2.7 TFLOPS
                (out_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 98.71 us = 0.05% latency, 9.01 TFLOPS, in_features=1024, out_features=1024, bias=True)
                (attn_ln): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 57.46 us = 0.03% latency, 37.78 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
            )
            (equivaiant2invariant_attention): Equivariant2InvariantAttention(
              6.29 M = 2.76% Params, 7.62 GMACs = 1.41% MACs, 2.46 ms = 1.35% latency, 6.2 TFLOPS
              (q_proj): Linear(1.05 M = 0.46% Params, 1.33 GMACs = 0.25% MACs, 164.51 us = 0.09% latency, 16.22 TFLOPS, in_features=1024, out_features=1024, bias=False)
              (k1_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 93.94 us = 0.05% latency, 9.47 TFLOPS, in_features=1024, out_features=1024, bias=True)
              (v1_proj): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 86.78 us = 0.05% latency, 10.25 TFLOPS, in_features=1024, out_features=1024, bias=True)
              (k2_proj): Linear(1.05 M = 0.46% Params, 1.33 GMACs = 0.25% MACs, 143.77 us = 0.08% latency, 18.55 TFLOPS, in_features=1024, out_features=1024, bias=False)
              (v2_proj): Linear(1.05 M = 0.46% Params, 1.33 GMACs = 0.25% MACs, 148.77 us = 0.08% latency, 17.93 TFLOPS, in_features=1024, out_features=1024, bias=False)
              (equiariant_attention): EquivariantAttention(
                1.05 M = 0.46% Params, 2.73 GMACs = 0.51% MACs, 1.66 ms = 0.91% latency, 3.3 TFLOPS
                (out_proj): Linear(1.05 M = 0.46% Params, 1.33 GMACs = 0.25% MACs, 159.26 us = 0.09% latency, 16.75 TFLOPS, in_features=1024, out_features=1024, bias=False)
                (attn_ln): EquivariantLayerNorm(1.02 K = 0% Params, 62.61 MMACs = 0.01% MACs, 930.55 us = 0.51% latency, 134.57 GFLOPS, (1024,), elementwise_linear=True)
              )
            )
            (invariant_attn_layer_norm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 55.55 us = 0.03% latency, 39.08 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
            (equivariant_attn_layer_norm): EquivariantLayerNorm(1.02 K = 0% Params, 62.61 MMACs = 0.01% MACs, 933.17 us = 0.51% latency, 134.19 GFLOPS, (1024,), elementwise_linear=True)
            (invariant_fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 180.48 us = 0.1% latency, 19.71 TFLOPS, in_features=1024, out_features=4096, bias=True)
            (invariant_fc2): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 227.21 us = 0.12% latency, 15.65 TFLOPS, in_features=4096, out_features=1024, bias=True)
            (equivariant_fc1): Linear(4.2 M = 1.84% Params, 1.78 GMACs = 0.33% MACs, 171.9 us = 0.09% latency, 20.69 TFLOPS, in_features=1024, out_features=4096, bias=True)
            (equivariant_fc2): Linear(4.19 M = 1.84% Params, 5.34 GMACs = 0.99% MACs, 433.44 us = 0.24% latency, 24.62 TFLOPS, in_features=1024, out_features=4096, bias=False)
            (equivariant_fc3): Linear(4.19 M = 1.84% Params, 5.34 GMACs = 0.99% MACs, 380.52 us = 0.21% latency, 28.04 TFLOPS, in_features=4096, out_features=1024, bias=False)
            (invariant_ffn_layer_norm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 54.12 us = 0.03% latency, 40.11 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
            (equivariant_ffn_layer_norm): EquivariantLayerNorm(1.02 K = 0% Params, 62.61 MMACs = 0.01% MACs, 906.23 us = 0.5% latency, 138.18 GFLOPS, (1024,), elementwise_linear=True)
            (invariant_ffn_layer_norm_2): LayerNorm(8.19 K = 0% Params, 0 MACs = 0% MACs, 52.69 us = 0.03% latency, 164.8 GFLOPS, (4096,), eps=1e-05, elementwise_affine=True)
            (equivariant_ffn_layer_norm_2): EquivariantLayerNorm(4.1 K = 0% Params, 250.18 MMACs = 0.05% MACs, 1.24 ms = 0.68% latency, 404.44 GFLOPS, (4096,), elementwise_linear=True)
          )
        )
        (unified_gbf_attn_bias): GaussianLayer(
          3.33 K = 0% Params, 0 MACs = 0% MACs, 1.22 ms = 0.67% latency, 0 FLOPS
          (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
          (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
          (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 51.26 us = 0.03% latency, 0 FLOPS, 1536, 1, padding_idx=0)
          (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 37.43 us = 0.02% latency, 0 FLOPS, 1536, 1, padding_idx=0)
        )
        (unified_gbf_pos): NodeGaussianLayer(
          9.47 K = 0% Params, 0 MACs = 0% MACs, 401.5 us = 0.22% latency, 0 FLOPS
          (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
          (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
          (mul): Embedding(4.61 K = 0% Params, 0 MACs = 0% MACs, 50.54 us = 0.03% latency, 0 FLOPS, 4608, 1, padding_idx=0)
          (bias): Embedding(4.61 K = 0% Params, 0 MACs = 0% MACs, 37.91 us = 0.02% latency, 0 FLOPS, 4608, 1, padding_idx=0)
        )
        (unified_gbf_vec): GaussianLayer(
          3.33 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS
          (means): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
          (stds): Embedding(128 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1, 128)
          (mul): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1536, 1, padding_idx=0)
          (bias): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, 1536, 1, padding_idx=0)
        )
        (unified_bias_proj): Linear(1.03 K = 0% Params, 222.3 MMACs = 0.04% MACs, 155.69 us = 0.09% latency, 2.86 TFLOPS, in_features=128, out_features=8, bias=True)
        (unified_vec_proj): Linear(132.1 K = 0.06% Params, 55.57 MMACs = 0.01% MACs, 106.1 us = 0.06% latency, 1.05 TFLOPS, in_features=128, out_features=1024, bias=True)
        (unified_final_equivariant_ln): EquivariantLayerNorm(1.02 K = 0% Params, 62.61 MMACs = 0.01% MACs, 899.08 us = 0.49% latency, 139.28 GFLOPS, (1024,), elementwise_linear=True)
        (unified_final_invariant_ln): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 55.07 us = 0.03% latency, 39.42 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
        (unified_final_feature_ln): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS, (1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (molecule_energy_head): Sequential(
      1.05 M = 0.46% Params, 445.03 MMACs = 0.08% MACs, 295.64 us = 0.16% latency, 3.01 TFLOPS
      (0): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 104.19 us = 0.06% latency, 8.53 TFLOPS, in_features=1024, out_features=1024, bias=True)
      (1): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 46.73 us = 0.03% latency, 9.29 GFLOPS)
      (2): Linear(1.02 K = 0% Params, 434.18 KMACs = 0% MACs, 77.01 us = 0.04% latency, 11.28 GFLOPS, in_features=1024, out_features=1, bias=True)
    )
    (periodic_energy_head): Sequential(
      1.05 M = 0.46% Params, 445.03 MMACs = 0.08% MACs, 290.87 us = 0.16% latency, 3.06 TFLOPS
      (0): Linear(1.05 M = 0.46% Params, 444.6 MMACs = 0.08% MACs, 109.2 us = 0.06% latency, 8.14 TFLOPS, in_features=1024, out_features=1024, bias=True)
      (1): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 37.67 us = 0.02% latency, 11.53 GFLOPS)
      (2): Linear(1.02 K = 0% Params, 434.18 KMACs = 0% MACs, 73.91 us = 0.04% latency, 11.75 GFLOPS, in_features=1024, out_features=1, bias=True)
    )
    (molecule_force_head): Linear(1.02 K = 0% Params, 1.3 MMACs = 0% MACs, 54.36 us = 0.03% latency, 47.92 GFLOPS, in_features=1024, out_features=1, bias=False)
    (periodic_force_head): Linear(1.02 K = 0% Params, 1.3 MMACs = 0% MACs, 43.87 us = 0.02% latency, 59.38 GFLOPS, in_features=1024, out_features=1, bias=False)
    (molecule_noise_head): Linear(1.02 K = 0% Params, 1.3 MMACs = 0% MACs, 46.97 us = 0.03% latency, 55.46 GFLOPS, in_features=1024, out_features=1, bias=False)
    (periodic_noise_head): Linear(1.02 K = 0% Params, 1.3 MMACs = 0% MACs, 41.72 us = 0.02% latency, 62.44 GFLOPS, in_features=1024, out_features=1, bias=False)
    (protein_noise_head): Linear(1.02 K = 0% Params, 1.3 MMACs = 0% MACs, 41.25 us = 0.02% latency, 63.16 GFLOPS, in_features=1024, out_features=1, bias=False)
    (aa_mask_head): Linear(163.84 K = 0.07% Params, 69.47 MMACs = 0.01% MACs, 164.51 us = 0.09% latency, 844.55 GFLOPS, in_features=1024, out_features=160, bias=False)
  )
  (diffnoise): DiffNoise(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
  (loss_fn): DiffMAE3dCriterions(
    0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS
    (energy_loss): L1Loss(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
    (force_loss): L1Loss(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
    (noise_loss): L1Loss(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
    (aa_mlm_loss): CrossEntropyLoss(0 = 0% Params, 0 MACs = 0% MACs, 0 s = 0% latency, 0 FLOPS)
  )
)
------------------------------------------------------------------------------
