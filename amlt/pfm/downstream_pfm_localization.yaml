description: bfm-finetune

target:
    service: aml
    # name: gcrarca100cl1
    # name: sfm-nd96amsra100v4-uksouth
    name: sfm-nc96-westus3

# target:
#     service: amlk8s
#     name: itphyperdgx2cl1
#     vc: hai1

# target:
#     service: amlk8s
#     name: itplabrr1cl1
#     vc: mprr3


environment:
    image: pj/sfm:cu117
    registry: msrmoldyn.azurecr.io
    username: msrmoldyn


storage:
    blob:
        storage_account_name: hai1data # Storage account
        container_name: pfm # Container name
        mount_dir: /blob

code:
    local_dir: ../SFM_framework

search:
    job_template:
        name: "finetune-binary-{task_name:s}_seed{seed:s}_lr{lr:s}_E{epoch:s}_bs{batch_size:s}_{run:s}"
        tags:
        - 'ProjectID: PRJ-0209-A40'
        sku: G4
        mpi: true
        process_count_per_node: 1
        command:
        - export seed={seed}
        - export layers=33
        - export hidden_size=1280
        - export ffn_size=5120
        - export num_head=20
        # - export layers=36
        # - export hidden_size=2560
        # - export ffn_size=10240
        # - export num_head=40
        - export atom_loss_coeff=1.0
        - export pos_loss_coeff=1.0
        - export sandwich_ln="true"
        - export dropout={dropout}
        - export attn_dropout={dropout}
        - export act_dropout={dropout}
        - export weight_decay=0.0005
        - export droppath_prob=0.0
        - export max_num_aa=1024
        - export noise_mode=diff
        - export noise_scale=0.2
        - export mask_ratio=0.2
        - export mode_prob=1.0,0.0,0.0
        - export d_tilde=1.0
        - export max_lr={lr}
        - export strategy=DDP
        - export pipeline_model_parallel_size=0
        - export train_batch_size={batch_size}
        - export val_batch_size=4
        - export max_tokens=6400
        - export max_length=2048
        - export gradient_accumulation_steps=1
        - export log_interval=10
        - export epochs=50
        - export wandb_group=localization_E{epoch}_bs{batch_size}
        - export wandb_team=icuppjin
        - export wandb_project=ds_mfmpre
        - export WANDB_RUN_NAME="finetune-{task_name}_seed{seed}_Drop{dropout}_lr{lr}_E{epoch}_bs{batch_size}_{run}"
        - export train_data_path="None"
        - export valid_data_path="None"
        - export data_basepath="/blob/data/bfm_benchmark"
        - export task_name={task_name}
        # - export loadcheck_path=/blob/pfmexp/output/bfm650m_bpe3_maskspan3_ddp8e5d16mask020drop1_ln_pairv3_bert2_64A100_adam2/checkpoints/checkpoint_E39.pt
        - export loadcheck_path=/blob/pfmexp/output/{run}/checkpoints/checkpoint_E{epoch}.pt
        - export save_dir=/blob/pfmexp/output/finetune/finetune-{task_name}_seed{seed}_lr{lr}_E{epoch}_bs{batch_size}_{run}
        - export early_stopping=true
        - export early_stopping_patience=5
        - export early_stopping_metric='accuracy'
        - export early_stopping_mode='max'
        - export head_dropout=0.5
        - export label_normalize={label_normalize}
        - eval "$$(conda shell.bash hook)" && conda create -n sfm python=3.9 && conda activate sfm
        - bash ./install/install.sh
        # - bash ./install/install_megatron.sh
        - bash ./scripts/pfm/finetune_pfm.sh
        submit_args:
            container_args:
                shm_size: 1024g

    type: grid
    max_trials: 999
    parallel_trials: 6
    params:
        - name: task_name
          spec: discrete
          values: ['subcellular_localization'] # ['solubility', 'subcellular_localization_2', 'human_ppi', 'yeast_ppi']
        - name: seed
          spec: discrete
          values: ["13", "21", "42",]
        - name: lr
          spec: discrete
          values: ["1e-5"]
        - name: dropout
          spec: discrete
          values: ["0.1"]
        - name: run
          spec: discrete
          # values: ["bfm3B_data2_maskspan3_ddp2e5d16mask030drop1L1536B2k_bpev2pairv4_bert2_128A100_adam2", ]
          values: ["bfm650m_data3_maskspan3_ddp4e5d16mask020drop1L1536B2k_bpev2pairv4_bert2_128A100_adam2"]
        - name: epoch
          spec: discrete
          values: ["80", ]
        - name: batch_size
          spec: discrete
          values: ["16", "32"] # ["32", "64", "128"]
        - name: label_normalize
          spec: discrete
          values: ["false", ]
