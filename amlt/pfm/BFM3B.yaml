 # Steps to run:
# 1. copy data folder https://itpeus4data.blob.core.windows.net/shuz/data/pcq-pos-custom/ to your Azure storage blob container
# 2. copy script mae_3d.sh in FoundationModelProp/ to your Azure storage blob container
# 3. Specify the Azure storage account and container to use
# 4. Submit the job with this yaml file

description: pfmtest

# target:
#     service: amlk8s
#     name: itphyperdgx2cl1
#     vc: hai1

# target:
#     service: amlk8s
#     name: itphyperdgx2cl2
#     vc: hcrr08

target:
    service: aml
    name: gcrarca100cl1

environment:
  image: pj/mfmds:20230207_b
  registry: itpeus4cr.azurecr.io
  username: itpeus4cr

storage:
  blob:
    storage_account_name: hai1data # Storage account
    container_name: pfm # Container name
    mount_dir: /blob

code:
  local_dir: ../SFM_framework

jobs:
- name: pfmtestamlt
  tags:
  - 'ProjectID: PRJ-0209-A40'
  sku: 14xG8
  mpi: true
  process_count_per_node: 1
  command:
  - mkdir /blob/pfmexp/output
  # - mkdir /blob/pfmexp/output/pfmdiff50M_dyn_mode_prob550
  - mkdir /blob/pfmexp/output/bfm3B_maskspan3_ddp2e5d16mask030drop1L1536B2k_bpepairv4_bert2_128A100_adam2
  - mkdir /blob/pfmexp/output/bfm3B_maskspan3_ddp2e5d16mask030drop1L1536B2k_bpepairv4_bert2_128A100_adam2/checkpoints
  - mkdir ./output
  - export path=run.sh
  - export layers=36
  - export num_pred_attn_layer=2
  - export hidden_size=2560
  - export ffn_size=10240
  - export num_head=40
  - export atom_loss_coeff=1.0
  - export pos_loss_coeff=1.0
  - export sandwich_ln="true"
  - export dropout=0.1
  - export attn_dropout=0.1
  - export act_dropout=0.0
  - export weight_decay=0.0
  - export droppath_prob=0.0
  - export max_num_aa=1024
  - export noise_mode=diff
  - export noise_scale=0.2
  - export mask_prob=0.30
  - export mode_prob=1.0,0.0,0.0
  - export d_tilde=1.0
  - export max_lr=2e-5
  - export strategy=DDP
  - export pipeline_model_parallel_size=0
  - export total_num_steps=600000
  - export warmup_num_steps=30000
  - export train_batch_size=896
  - export val_batch_size=896
  - export max_tokens=3072
  - export max_length=1536
  - export gradient_accumulation_steps=2
  - export log_interval=100
  - export wandb_group=BFM
  - export wandb_project=ds_mfmpre
  - export loadcheck_path=/blob/pfmexp/output/bfm3B_maskspan3_ddp2e5d16mask030drop1L1536B2k_bpepairv4_bert2_128A100_adam2/checkpoints
  # - export train_data_path=/blob/data/afdb/uniref50_msa_ppi_pack1536_train.lmdb/
  # - export valid_data_path=/blob/data/afdb/uniref50_pack1024_valid.lmdb/
  # - export train_data_path=/nfs/uniref50_msa_ppi_pack1536_train.lmdb/
  # - export valid_data_path=/nfs/uniref50_pack1024_valid.lmdb/
  - export train_data_path=/mnt/amlt_code/ur50_msa_ppi_bpe_pack1536_train.lmdb/
  - export valid_data_path=/mnt/amlt_code/uniref50_pack1024_valid.lmdb/
  - bash ./scripts/pfm/copy_from_nfs.sh
  - export save_dir=/blob/pfmexp/output/bfm3B_maskspan3_ddp2e5d16mask030drop1L1536B2k_bpepairv4_bert2_128A100_adam2/checkpoints
  - eval "$$(conda shell.bash hook)" && conda create -n sfm python=3.9 && conda activate sfm
  - bash ./install/install.sh
  - bash ./install/install_megatron.sh
  - bash ./scripts/pfm/pretrain_pfm.sh
  submit_args:
    container_args:
      shm_size: 1024g
