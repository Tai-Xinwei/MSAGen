# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

# train a chemical generalist

description: chemical-copilot

target:
    service: amlk8s
    name: itphyperdgx2cl1
    vc: hai1

# target:
    # service: amlk8s
    # name: itphyperdgx2cl2
    # vc: hcrr08

environment:
  image: pj/mfmds:20230207_b
  registry: itpeus4cr.azurecr.io
  username: itpeus4cr

storage:
  blob:
    storage_account_name: hai1data # Storage account
    container_name: mfm # Container name
    mount_dir: /blob

code:
  local_dir: ../SFM_framework

jobs:
- name: ft_22BMFM_70Bppllama_graphqformer1_moldesc+funcgroup+funcgroup-desc+chebi+chemcop-adaptor+graphormer-special-tokens-2e-5-pp16-nonprem
  tags:
  - 'ProjectID: PRJ-0209-A40'
  sku: 16xG16
  mpi: true
  process_count_per_node: 1
  command:
  # molecule model parameters
  - export layers=42
  - export num_pred_attn_layer=4
  - export hidden_size=6144
  - export ffn_size=24576
  - export num_head=32
  - export num_3d_bias_kernel=128

  # molecule model training parameters
  - export dropout=0.0
  - export act_dropout=0.0
  - export attn_dropout=0.0
  - export weight_decay=0.0
  - export sandwich_ln="true"
  - export droppath_prob=0.0

  # general training parameters
  - export d_tilde=16.0
  - export max_lr=1e-5
  - export total_num_steps=200000
  - export warmup_num_steps=12000
  - export seed=12345

  # generalist dataset settings
  - export data_path=/tmp/chemical-copilot-special-token/
  # - export dataset_names=mol-instruction-mol-desc,functional-group,chebi,func-group-list-and-desc
  # - export dataset_splits=clean,all,all,all
  # - export dataset_ratios=3.0,1.0,3.0,1.0
  - export dataset_names=mol-instruction-mol-desc,chebi,functional-group,func-group-list-and-desc,chemcop-instruction,tdc/LD50_Zhu,tdc/kcnq2_potassium_channel_butkiewicz,tdc/Skin_Reaction,tdc/HIV,tdc/CYP3A4_Veith,tdc/CYP1A2_Veith,tdc/hERG_Karim,tdc/PAMPA_NCATS,tdc/hERG,tdc/CYP2C9_Substrate_CarbonMangels,tdc/HydrationFreeEnergy_FreeSolv,tdc/m1_muscarinic_receptor_agonists_butkiewicz,tdc/Bioavailability_Ma,tdc/m1_muscarinic_receptor_antagonists_butkiewicz,tdc/DILI,tdc/potassium_ion_channel_kir2.1_butkiewicz,tdc/CYP2C9_Veith,tdc/SARSCoV2_3CLPro_Diamond,tdc/Clearance_Hepatocyte_AZ,tdc/choline_transporter_butkiewicz,tdc/Half_Life_Obach,tdc/Lipophilicity_AstraZeneca,tdc/cav3_t-type_calcium_channels_butkiewicz,tdc/SARSCoV2_Vitro_Touret,tdc/Caco2_Wang,tdc/VDss_Lombardo,tdc/PPBR_AZ,tdc/Solubility_AqSolDB,tdc/tyrosyl-dna_phosphodiesterase_butkiewicz,tdc/Carcinogens_Lagunin,tdc/Pgp_Broccatelli,tdc/CYP2C19_Veith,tdc/CYP3A4_Substrate_CarbonMangels,tdc/CYP2D6_Substrate_CarbonMangels,tdc/serine_threonine_kinase_33_butkiewicz,tdc/orexin1_receptor_butkiewicz,tdc/AMES,tdc/CYP2D6_Veith,tdc/Tox21/NR-AR,tdc/Tox21/NR-PPAR-gamma,tdc/Tox21/NR-AR-LBD,tdc/Tox21/NR-Aromatase,tdc/Tox21/SR-MMP,tdc/Tox21/NR-AhR,tdc/Tox21/SR-HSE,tdc/Tox21/NR-ER,tdc/Tox21/SR-ARE,tdc/Tox21/NR-ER-LBD,tdc/Tox21/SR-p53,tdc/Tox21/SR-ATAD5,tdc/herg_central/hERG_inhib,tdc/herg_central/hERG_at_1uM,tdc/herg_central/hERG_at_10uM,tdc/USPTO_Yields,tdc/Buchwald-Hartwig
  - export dataset_splits=clean,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all,all
  - export dataset_ratios=5.0,5.0,5.0,5.0,1.0,16.93,0.41,309.60,3.04,10.14,9.52,9.30,61.46,191.20,187.27,194.55,2.02,195.31,2.02,263.85,0.41,10.34,142.05,103.09,0.41,187.97,29.76,1.24,84.25,137.36,110.62,44.82,12.52,0.37,446.43,102.77,10.34,186.92,187.97,0.39,0.57,17.18,9.52,17.21,19.38,18.50,21.47,21.51,19.09,19.33,20.19,21.43,17.97,18.45,17.67,0.41,0.41,0.41,0.15,31.61
  - export pool_mode="full"
  - export embedding_length=20
  - export model_max_length=1024

  # checkpoint and log settings
  - export save_dir=/blob/checkpoints/ft_22BMFM_70Bppllama_graphqformer1_moldesc+funcgroup+funcgroup-desc+chebi+chemcop-adaptor+graphormer-special-tokens-2e-4-pp16-nonprem
  - export save_batch_interval=500
  - export loadmfmcheck_path=/blob/ds_dataset/output/pretrain56w/pm6oc_512_pm6_oc2_acc2_4UnifiedH_22B_L46_4e4/global_step560000/
  - export llm_model_name_or_path=/blob/ds_dataset/llama2/llama-2-70b
  - export finetune_from_checkpoint_dir=/blob/checkpoints/ft_22BMFM_70Bppllama_graphqformer1_moldesc+funcgroup+funcgroup-desc+chebi+chemcop-adaptor+graphormer-special-tokens-2e-4-pp16-nonprem
  - export finetune_from_checkpoint_id=global_step11499
  - export wandb_key=5d03b7a46d10f86ff45c4aedc570660a523edc0b
  - export wandb_project_name=chemical-generalist

  # training parallelism
  - export pipeline_model_parallel_size=32
  - export tensor_model_parallel_size=1
  - export strategy=Pipeline
  - export pp_partition_layer_name="manual"
  # - export pp_part_list="[0, 43, 70, 99, 127]"
  - export pp_part_list="[0, 3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 44, 48, 52, 56, 60, 64, 68, 72, 76, 80, 84, 88, 92, 96, 100, 104, 108, 112, 115, 118, 122, 127]"
  # - export unfreeze_param_list="adapter,lm_head,num_head,mol_rep_layernorm,embed_tokens"
  - export unfreeze_param_list="adapter,lm_head,num_head,mol_rep_layernorm,embed_tokens,0.layer,1.layer,2.layer,3.layer,4.layer,5.layer,6.layer,7.layer,8.layer,9.layer,10.layer,11.layer,12.layer,13.layer,14.layer,15.layer,16.layer,17.layer,18.layer,19.layer,20.layer,21.layer,22.layer,23.layer,24.layer,25.layer,26.layer,27.layer,28.layer,29.layer,30.layer,31.layer,32.layer,33.layer,34.layer,35.layer,36.layer,37.layer,38.layer,39.layer,40.layer,41.layer,42.layer"

  # training parameters for generalist
  - export micro_batch_size=1
  - export global_batch_size=64
  - export max_position_embeddings=2048
  - export llm_hidden_size=4096 # this is only for ThreeD parallelism strategy, used by megatron

  # setup environment
  - eval "$$(conda shell.bash hook)" && conda create -n sfm python=3.9 && conda activate sfm
  - bash /blob/ds_dataset/copy_generalist_data.sh
  - bash ./install/install.sh && bash ./install/install_megatron.sh
  - bash ./scripts/generalist/ftmp_graphormer_llama_smiles.sh
  submit_args:
    container_args:
      shm_size: 1024g
  preemptible: false
