# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

# train a chemical generalist

description: chemical-copilot

target:
    service: amlk8s
    name: itphyperdgx2cl1
    vc: hai1


environment:
  image: pj/mfmds:20230207_b
  registry: itpeus4cr.azurecr.io
  username: itpeus4cr

storage:
  blob:
    storage_account_name: hai1data # Storage account
    container_name: mfm # Container name
    mount_dir: /blob

code:
  local_dir: ../../

jobs:
- name: ft_100MMFM_7Bppllama_graphqformer1_moldesc+funcgroup+funcgroup-desc+chebi+pubchem-adaptor-2e-4-pp16-layerwise-graph-attn-bias
  tags:
  - 'ProjectID: PRJ-0209-A40'
  sku: 4xG16
  mpi: true
  process_count_per_node: 1
  command:
  # molecule model parameters
  - export layers=24
  - export num_pred_attn_layer=1
  - export hidden_size=768
  - export ffn_size=768
  - export num_head=32
  - export num_3d_bias_kernel=128

  # molecule model training parameters
  - export dropout=0.0
  - export act_dropout=0.1
  - export attn_dropout=0.1
  - export weight_decay=0.0
  - export sandwich_ln="true"
  - export droppath_prob=0.0

  # generalist model parameters
  - export fused_graphormer_llama=true
  - export add_mol_attn_bias_in_llama=true
  - export mol_attn_bias_in_llama_layerwise=false
  - export path_edge_cutoff=5

  # general training parameters
  - export d_tilde=1.0
  - export max_lr=2e-4
  - export total_num_steps=200000
  - export warmup_num_steps=12000
  - export seed=12347

  # generalist dataset settings
  - export data_path=/tmp/data/chemical-copilot-special-token
  - wget https://azcopyvnext.azureedge.net/releases/release-10.20.1-20230809/azcopy_linux_amd64_10.20.1.tar.gz
  - tar -zxvf azcopy_linux_amd64_10.20.1.tar.gz
  - ./azcopy_linux_amd64_10.20.1/azcopy copy "https://hai1data.blob.core.windows.net/mfm/data/chemical-copilot-special-token<SAS>" "/tmp/data/" --recursive
  - export dataset_names=mol-instruction-mol-desc,functional-group,chebi,func-group-list-and-desc,pubchem-properties-with-name
  - export dataset_splits=clean,all,all,all,small
  - export dataset_ratios=3.0,1.0,3.0,1.0,1.0
  - export pool_mode="full"
  - export embedding_length=20
  - export model_max_length=1024

  # checkpoint and log settings
  - export save_dir=/blob/generalist-checkpoints/ft_100MMFM_7Bppllama_graphqformer1_moldesc+funcgroup+funcgroup-desc+chebi+pubchem-adaptor-special-tokens-2e-4-pp16-layerwise-graph-attn-bias-continued
  - export save_batch_interval=500
  - export loadmfmcheck_path=/blob/ds_dataset/ddpm100M/checkpoint7_new.pt
  - export llm_model_name_or_path=/blob/ds_dataset/llama2/llama-2-7b/
  - export finetune_from_checkpoint_dir="/blob/generalist-checkpoints/ft_100MMFM_7Bppllama_graphqformer1_moldesc+funcgroup+funcgroup-desc+chebi+pubchem-adaptor-special-tokens-2e-4-pp16-layerwise-graph-attn-bias"
  - export finetune_from_checkpoint_id="global_step13999"
  - export wandb_key=974f8bd90204c0396737c20ec04a3b4ca924ea9a
  - export wandb_project_name=chemical-copilot

  # training parallelism
  - export pipeline_model_parallel_size=16
  - export tensor_model_parallel_size=1
  - export strategy=Pipeline
  - export pp_partition_layer_name="LlamaDecoderLayerPP"
  - export pp_part_list="[]"
  - export unfreeze_param_list="mol_adaptor,lm_head,num_head,mol_rep_layernorm,embed_tokens,llama_node_distance_embed,llama_edge_embed,llama_edge_feature_weights"

  # training parameters for generalist
  - export micro_batch_size=1
  - export global_batch_size=64
  - export max_position_embeddings=1024
  - export llm_hidden_size=4096 # this is only for ThreeD parallelism strategy, used by megatron

  # setup environment
  - eval "$$(conda shell.bash hook)" && conda create -n sfm python=3.9 && conda activate sfm
  - bash ./install/install.sh && bash ./install/install_megatron.sh
  - bash ./scripts/generalist/ftmp_graphormer_llama_smiles.sh
  submit_args:
    container_args:
      shm_size: 1024g
  preemptible: false
