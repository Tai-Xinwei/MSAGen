description: nlm_moe_finetune

target:
  service: sing
  name: baltic02
  workspace_name: sfm-ws

environment:
  image: ai4s-sfm:20240429.081857
  registry: msroctocr.azurecr.io
  username: msroctocr

storage:
  sfmdataeastus2:
    storage_account_name: sfmdataeastus2
    container_name: nlm
    mount_dir: /mnt/sfmdataeastus2
  msralaphilly2:
    storage_account_name: msralaphilly2
    container_name: ml-la
    mount_dir: /mnt/msralaphilly2


code:
    local_dir: .

jobs:
- name: "finetune_nlm_8x7b_inst.20240524.50k.filt"
  tags:
  - 'ProjectID: PRJ-0209-A40'
  - 'Project_Name: Science_Foundation_Model'
  - 'Experiment: SFM_NLM_MOE_Model_Training'
  sku: 8xG8-IB
  mpi: true
  process_count_per_node: 1
  submit_args:
    env:
      SHARED_MEMORY_PERCENT: 1.0
      # CUDA_LAUNCH_BLOCKING: 1
      WANDB_API_KEY: local-8b231a9559eded7cef00bd550f7330ad2f3ce696
      WANDB_PROJECT: NLM_MOE
      WANDB_TEAM: ai4s-sfm
      WANDB_BASE_URL: https://microsoft-research.wandb.io
      NCCL_DEBUG: INFO
      train_data_path: /mnt/msralaphilly2/yinxia/wu2/shared/SFM/SFM.overall.data/SFMMolInstruct.20240524/overall.instructtrain.50k.filt
      valid_data_path: /mnt/msralaphilly2/yinxia/wu2/shared/SFM/SFM.overall.data/SFMMolInstruct.20240524/overall.instructval.filt
      total_num_epochs: 3
      train_batch_size: 256
      gradient_accumulation_steps: 32
  command:
    - eval "$$(conda shell.bash hook)" && conda activate sfm
    - pip install -e . --no-deps
    - bash scripts/nlm/finetune_nlm_moe_8x7b_inst.sh
- name: "finetune_nlm_8x7b_inst.20240524.full"
  tags:
  - 'ProjectID: PRJ-0209-A40'
  - 'Project_Name: Science_Foundation_Model'
  - 'Experiment: SFM_NLM_MOE_Model_Training'
  sku: 8xG8-IB
  mpi: true
  process_count_per_node: 1
  submit_args:
    env:
      SHARED_MEMORY_PERCENT: 1.0
      # CUDA_LAUNCH_BLOCKING: 1
      WANDB_API_KEY: local-8b231a9559eded7cef00bd550f7330ad2f3ce696
      WANDB_PROJECT: NLM_MOE
      WANDB_TEAM: ai4s-sfm
      WANDB_BASE_URL: https://microsoft-research.wandb.io
      NCCL_DEBUG: INFO
      train_data_path: /mnt/msralaphilly2/yinxia/wu2/shared/SFM/SFM.overall.data/SFMMolInstruct.20240524/overall.instructtrain.full.filt
      valid_data_path: /mnt/msralaphilly2/yinxia/wu2/shared/SFM/SFM.overall.data/SFMMolInstruct.20240524/overall.instructval.filt
      total_num_epochs: 3
      train_batch_size: 256
      gradient_accumulation_steps: 32
  command:
    - eval "$$(conda shell.bash hook)" && conda activate sfm
    - pip install -e . --no-deps
    - bash scripts/nlm/finetune_nlm_moe_8x7b_inst.sh
