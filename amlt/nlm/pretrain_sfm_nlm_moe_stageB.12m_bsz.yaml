description: train_sfm_nlm_moe

target:
  service: sing
  name: baltic02
  workspace_name: sfm-ws

environment:
  image: ai4s-sfm:20240429.081857
  registry: msroctocr.azurecr.io
  username: msroctocr

storage:
  sfmdataeastus2:
    storage_account_name: sfmdataeastus2
    container_name: nlm
    mount_dir: /mnt/sfmdataeastus2

code:
  local_dir: .

jobs:
- name: train_sfm_nlm_moe_stageB_pp8_acc16_total1536_12m_bsz
  sku: 32xG8
  tags:
    - 'ProjectID: PRJ-0209-A40'
    - 'Project_Name: Science_Foundation_Model'
    - 'Experiment: SFM_NLM_MOE_Model_Training'
  mpi: true
  process_count_per_node: 1
  submit_args:
    env:
      SHARED_MEMORY_PERCENT: 1.0
      WANDB_API_KEY: local-8b231a9559eded7cef00bd550f7330ad2f3ce696
      WANDB_PROJECT: NLM_MOE
      WANDB_TEAM: ai4s-sfm
      WANDB_BASE_URL: https://microsoft-research.wandb.io
      NCCL_DEBUG: INFO
      train_batch_size: 1536
      gradient_accumulation_steps: 16
      pipeline_model_parallel_size: 8
      epochs: 5
      total_num_steps: 58312 # ~ 17913508 / 1536 * 5
      save_batch_interval: 2000
      max_lr: 2e-4
      warmup_num_steps: 1000
      log_interval: 10
      save_dir: '/mnt/sfmdataeastus2/shufxi/nlm/8x7b/stageB_pp8_acc16_total1536_12m_bsz'
  command:
    - eval "$$(conda shell.bash hook)" && conda activate sfm
    - pip install -e . --no-deps
    - python scripts/pref/all_reduce_bench_v2.py
    - bash scripts/nlm/pretrain_sfm_nlm_moe_stageB.sh
