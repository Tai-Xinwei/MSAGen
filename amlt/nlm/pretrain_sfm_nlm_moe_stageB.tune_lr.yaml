description: train_sfm_nlm_moe

env_defaults:
  NODES: 32
  GPUS: 8

target:
  service: sing
  name: baltic02
  workspace_name: sfm-ws

environment:
  image: ai4s-sfm:20240429.081857
  registry: msroctocr.azurecr.io
  username: msroctocr

storage:
  sfmdataeastus2:
    storage_account_name: sfmdataeastus2
    container_name: nlm
    mount_dir: /mnt/sfmdataeastus2

code:
  local_dir: .

search:
  job_template:
    name: train_sfm_nlm_moe_stageB_pp16_acc16_total1280_lr{lr}
    sku: ${NODES}xG${GPUS}
    tags:
      - 'ProjectID: PRJ-0209-A40'
      - 'Project_Name: Science_Foundation_Model'
      - 'Experiment: SFM_NLM_MOE_Model_Training'
    mpi: true
    process_count_per_node: 1
    submit_args:
      env:
        SHARED_MEMORY_PERCENT: 1.0
        WANDB_API_KEY: local-8b231a9559eded7cef00bd550f7330ad2f3ce696
        WANDB_PROJECT: NLM_MOE
        WANDB_TEAM: ai4s-sfm
        WANDB_BASE_URL: https://microsoft-research.wandb.io
        NCCL_DEBUG: INFO
        train_batch_size: 1280
        gradient_accumulation_steps: 16
        pipeline_model_parallel_size: 16
        epochs: 1
        total_num_steps: 300
        save_batch_interval: 5000
        log_interval: 1
        warmup_num_steps: 50
    command:
      - eval "$$(conda shell.bash hook)" && conda activate sfm
      - python setup_cython.py build_ext --inplace
      - pip install -e . --no-deps
      - export max_lr={lr}
      - export save_dir='/mnt/sfmdataeastus2/shufxi/nlm/8x7b/stageB_pp16_acc32_total1280_lr{lr}'
      - bash scripts/nlm/pretrain_sfm_nlm_moe_stageB.sh
  type: grid
  max_trials: 999
  parallel_trials: 1
  params:
    - name: lr
      spec: discrete
      values: [0.0001, 0.0002, 0.0003]
