description: "evaluate NLM MoE model"

target:
  service: aml
  name: sfm-nd96amsra100v4-uksouth

environment:
  image: ai4s-sfm:20240429.081857
  registry: msroctocr.azurecr.io
  username: msroctocr

storage:
  sfmdataeastus2:
    storage_account_name: sfmdataeastus2
    container_name: nlm
    mount_dir: /mnt/sfmdataeastus2
  msralaphilly2:
    storage_account_name: msralaphilly2
    container_name: ml-la
    mount_dir: /mnt/msralaphilly2

code:
  local_dir: .

jobs:
- name: eval_moe_base
  sku: G8
  mpi: true
  process_count_per_node: 8
  tags:
  - 'ProjectID: PRJ-0209-A40'
  - 'Project_Name: Science_Foundation_Model'
  - 'Experiment: SFM_NLM_MOE_Model_Training'
  submit_args:
    env:
      SHARED_MEMORY_PERCENT: 1.0
      test_file_folder: '/mnt/sfmdataeastus2/yinxia/all_valid_set/1k'
      mixtral_path: '/mnt/sfmdataeastus2/Mixtral-8x7B-v0.1'
      model_path: '/mnt/sfmdataeastus2/shufxi/nlm/8x7b/stageA'
      steps: '3999'
    container_args:
      shm_size: 1024g
  command:
    - eval "$$(conda shell.bash hook)" && conda activate sfm
    - pip install git+https://github.com/TorchMoE/MoE-Infinity
    - pip install -e . --no-deps
    - bash scripts/nlm/evaluate_moe_ppl.sh
- name: eval_moe_0
  sku: G8
  mpi: true
  process_count_per_node: 8
  tags:
  - 'ProjectID: PRJ-0209-A40'
  - 'Project_Name: Science_Foundation_Model'
  - 'Experiment: SFM_NLM_MOE_Model_Training'
  submit_args:
    env:
      SHARED_MEMORY_PERCENT: 1.0
      test_file_folder: '/mnt/sfmdataeastus2/yinxia/all_valid_set/1k'
      mixtral_path: '/mnt/sfmdataeastus2/Mixtral-8x7B-v0.1'
      model_path: '/mnt/sfmdataeastus2/shufxi/nlm/8x7b/stageB_pp8_acc16_total1536_12m_bsz'
      steps: '2000,4000,6000,8000,10000,12000,14000,16000'
    container_args:
      shm_size: 1024g
  command:
    - eval "$$(conda shell.bash hook)" && conda activate sfm
    - pip install git+https://github.com/TorchMoE/MoE-Infinity
    - pip install -e . --no-deps
    - bash scripts/nlm/evaluate_moe_ppl.sh
- name: eval_moe_1
  sku: G8
  mpi: true
  process_count_per_node: 8
  tags:
  - 'ProjectID: PRJ-0209-A40'
  - 'Project_Name: Science_Foundation_Model'
  - 'Experiment: SFM_NLM_MOE_Model_Training'
  submit_args:
    env:
      SHARED_MEMORY_PERCENT: 1.0
      test_file_folder: '/mnt/sfmdataeastus2/yinxia/all_valid_set/1k'
      mixtral_path: '/mnt/sfmdataeastus2/Mixtral-8x7B-v0.1'
      model_path: '/mnt/sfmdataeastus2/shufxi/nlm/8x7b/stageB_pp8_acc16_total1536_12m_bsz'
      steps: '18000,20000'
    container_args:
      shm_size: 1024g
  command:
    - eval "$$(conda shell.bash hook)" && conda activate sfm
    - pip install git+https://github.com/TorchMoE/MoE-Infinity
    - pip install -e . --no-deps
    - bash scripts/nlm/evaluate_moe_ppl.sh
