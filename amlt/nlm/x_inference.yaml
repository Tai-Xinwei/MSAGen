description: nlm inference

target:
    service: aml
    name: sfm-nd96amsra100v4-uksouth

# target:
#     service: aml
#     subscription_id: 3eaeebff-de6e-4e20-9473-24de9ca067dc
#     resource_group: sfm-ws-rg
#     workspace_name: sfm-ws
#     cluster: sfm-nd96amsra100v4-uksouth

environment:
  image: ai4s-sfm:20240429.081857
  registry: msrmoldyn.azurecr.io
  username: msrmoldyn

storage:
    blob:
        storage_account_name: sfmdataeastus2
        container_name: nlm
        mount_dir: /blob

code:
    local_dir: .

jobs:
- name: "nlm_inference"
  identity: managed
  tags:
  - 'ProjectID: PRJ-0209-A40'
  sku: 1x G8
  mpi: true
  process_count_per_node: 1
  command:
  # ----- [llama2_7b v1] ------
  # - export model_name="llama2_7b"
  # - export base_model_root="/blob/lihe/hai1/ds_dataset/llama2/llama-2-7b/"
  # - export model_ckpt_home="/blob/lihe/model/yinxia/scigpt/7bv3/unifyall_full_run1/global_step22144"
  # - export aux_home="/blob/lihe/model/shufxi/data/scigpt/"
  # # - export output_dir="/blob/lihe/output/inference/sfmdata.prot.test.sampled30.tsv/llama2_7b.v1"
  # - export output_dir="/blob/lihe/output/inference/sfmdata.prot.test.sampled100.tsv/llama2_7b.v1"

  # ----- [llama2_7b v2] ------
  # - export model_name="llama2_7b"
  # - export base_model_root="/blob/lihe/hai1/ds_dataset/llama2/llama-2-7b/"
  # - export model_ckpt_home="/blob/lihe/model/yinxia/scigpt/7bv3/unifyall_v2_full_run1/global_step17984"
  # - export aux_home="/blob/lihe/model/shufxi/data/scigpt/"
  # # - export output_dir="/blob/lihe/output/inference/sfmdata.prot.test.sampled30.tsv/llama2_7b.v2"
  # - export output_dir="/blob/lihe/output/inference/sfmdata.prot.test.sampled100.tsv/llama2_7b.v2"

  # ----- [llama2_7b v3] ------
#   - export model_name="llama2_7b"
#   - export base_model_root="/blob/lihe/hai1/ds_dataset/llama2/llama-2-7b/"
#   - export model_ckpt_home="/blob/lihe/model/yinxia/scigpt/7bv3/unifyall_v3_full_run1/global_step17984"
#   - export aux_home="/blob/lihe/model/shufxi/data/scigpt/"
#   # - export output_dir="/blob/lihe/output/inference/sfmdata.prot.test.sampled30.tsv/llama2_7b.v3"
#   # - export output_dir="/blob/lihe/output/inference/sfmdata.prot.test.sampled100.tsv/llama2_7b.v3"
#   - export output_dir="/blob/lihe/output/inference/sfmdata.prot.test.tsv/llama2_7b.v3"

  # ----- [llama3_8b] ------
  - export model_name="llama3_8b"
  - export base_model_root="/blob/llama/Meta-Llama-3-8B/original/"
  - export model_ckpt_home="/blob/kaiyuan/results/nlm/inst/inst_0621_bsz256_lr2e5_0624/global_step89920/"
  - export aux_home=""
  - export output_dir="/blob/lihe/output/inference/sfmdata.prot.test.sampled100.tsv/llama3_8b"

  # ----- [mixtral_8x7b] ------
  # - export model_name="mixtral_8x7b"
  # - export base_model_root="/blob/Mixtral-8x7B-v0.1/"
  # - export model_ckpt_home="/blob/shufxi/nlm/8x7b/inst/20240611215447/global_step33216/"
  # - export aux_home="/tmp/nlm_rank"
  # # - export output_dir="/blob/lihe/output/inference/sfmdata.prot.test.sampled30.tsv/mixtral_8x7b"
  # - export output_dir="/blob/lihe/output/inference/sfmdata.prot.test.sampled100.tsv/mixtral_8x7b"

  # - export input_file="/blob/lihe/data/generated/sfmdata.prot.test.sampled30.tsv"
  - export input_file="/blob/lihe/data/generated/sfmdata.prot.test.sampled100.tsv"
  # - export input_file="/blob/lihe/data/generated/sfmdata.prot.test.tsv"

  - eval "$$(conda shell.bash hook)" && conda create -n sfm python=3.9 && conda activate sfm
  - bash ./install/install.sh && bash ./install/install_megatron.sh
  - pip install -i https://pypi.org/simple/ bitsandbytes
  - bash ./tools/nlm/x_inference.sh
  submit_args:
      container_args:
          shm_size: 1024g
