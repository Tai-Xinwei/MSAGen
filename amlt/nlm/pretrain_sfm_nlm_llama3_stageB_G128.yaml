description: train_sfm_nlm8B

env_defaults:
  NODES: 16
  GPUS: 8
  WANDB_API_KEY: "local-094f941ede8eda7a00c307f50595f054be5382f7"


target:
  service: sing
  name: baltic02
  workspace_name: sfm-ws


environment:
  image: ai4s-sfm:20240429.081857
  registry: msrmoldyn.azurecr.io
  username: msrmoldyn

storage:
  hai1:
    storage_account_name: sfmdataeastus2 # Storage account
    container_name: nlm # Container name
    mount_dir: /nlm

code:
  local_dir: ../SFM_framework

search:
  job_template:
    name: sfm_nlm_llama3_stageB
    sku: ${NODES}xG${GPUS}-IB
    tags: [Project_Name:Science_Foundation_Model,ProjectID:PRJ-0209-A40,Experiment:SFMV1]
    mpi: true
    identity: managed
    process_count_per_node: 1
    command:
      - export wandb_key=${WANDB_API_KEY}
      - export WANDB_PROJECT=nlm_llama3
      - export WANDB_TEAM=ai4s-sfm
      - export wandb_group=nlm_llama3_stageB
      - export NCCL_DEBUG=INFO
      - eval "$$(conda shell.bash hook)" && conda activate sfm
      - pip install -e . --no-deps
      - python setup_cython.py build_ext --inplace
      - pip install git+https://github.com/NVIDIA/TransformerEngine.git@v1.6
      - pip uninstall flash-attn -y
      - pip install ase
      - python tools/nlm/compute_text_data_ratio.py {text_raito} {prot_raito} {dna_raito} {other_raito}
      - export train_batch_size=4096
      - export val_batch_size=4096
      - export gradient_accumulation_steps=64
      - export tensor_model_parallel_size=1
      - export pipeline_model_parallel_size=2
      - export load_ckpt=True
      # - export weighted_dataset=True
      - export data_dir=/nlm/peiran/llama3_processed_data/lmdb/v5_train_split/
      - export train_data_path=`cat /tmp/train_data_path.txt`
      - export data_ratio=`cat /tmp/data_ratio.txt`
      - export valid_data_path=/nlm/peiran/llama3_processed_data/lmdb/v5_valid_split_dna_six/
      - export dict_path=/nlm/llama/Meta-Llama-3-8B/original
      # - export loadcheck_path=/nlm/peiran/output/llama3_stageA_G256/global_step19000
      # - export loadcheck_path=/nlm/zekun/output/base8b/stageB/global_step11000
      - export loadcheck_path=/nlm/peiran/output/llama3_stageB_G256/global_step4000/
      - export save_dir=/nlm/peiran/output/llama3_stageB_G128_ratio/
      - bash scripts/nlm/pretrain_nlm_llama3_7b_stageB.sh
    submit_args:
      env:
        SHARED_MEMORY_PERCENT: 1.0
  type: grid
  max_trials: 3
  params:
    - name: text_raito
      spec: discrete
      values: [0.35]
    - name: prot_raito
      spec: discrete
      values: [0.35]
    - name: dna_raito
      spec: discrete
      values: [0.2]
    - name: other_raito
      spec: discrete
      values: [0.1]
