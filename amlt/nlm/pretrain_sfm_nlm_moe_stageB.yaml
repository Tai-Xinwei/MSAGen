description: train_sfm_nlm_moe

env_defaults:
  NODES: 32
  GPUS: 8

target:
  service: sing
  name: baltic02
  workspace_name: sfm-ws

environment:
  image: yaosen/sfm-py39-torch2.2.2-cuda12.1:20240417_a
  registry: msroctocr.azurecr.io
  username: msroctocr

storage:
  sfmdataeastus2:
    storage_account_name: sfmdataeastus2
    container_name: nlm
    mount_dir: /mnt/sfmdataeastus2

code:
  local_dir: .

jobs:
- name: train_sfm_nlm_moe_stageB
  sku: ${NODES}xG${GPUS}
  tags:
    - 'ProjectID: PRJ-0209-A40'
    - 'Project_Name: Science_Foundation_Model'
    - 'Experiment: SFM_NLM_MOE_Model_Training'
  mpi: true
  process_count_per_node: 1
  submit_args:
    env:
      SHARED_MEMORY_PERCENT: 1.0
      WANDB_API_KEY: local-8b231a9559eded7cef00bd550f7330ad2f3ce696
      WANDB_PROJECT: NLM_MOE
      WANDB_TEAM: ai4s-sfm
      WANDB_BASE_URL: https://microsoft-research.wandb.io
      NCCL_DEBUG: INFO
      train_batch_size: 128 # 512
      gradient_accumulation_steps: 4 # 16
      pipeline_model_parallel_size: 8
      epochs: 1
      total_num_steps: 140000 # ~ 17913508 / 128
      save_batch_interval: 5000
      log_interval: 20
  command:
    - eval "$$(conda shell.bash hook)" && conda activate sfm
    - pip install -e . --no-deps
    - bash scripts/nlm/pretrain_sfm_nlm_moe_stageB.sh
