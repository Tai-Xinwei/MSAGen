 # Steps to run:
# 1. copy data folder https://itpeus4data.blob.core.windows.net/shuz/data/pcq-pos-custom/ to your Azure storage blob container
# 2. copy script mae_3d.sh in FoundationModelProp/ to your Azure storage blob container
# 3. Specify the Azure storage account and container to use
# 4. Submit the job with this yaml file

description: PSMV1

env_defaults:
  NODES: 12
  GPUS: 8
  WANDB_API_KEY: "local-094f941ede8eda7a00c307f50595f054be5382f7"

target:
  service: sing
  name: baltic02
  workspace_name: sfm-ws

environment:
  image: ai4s-sfm:20240531.170731
  registry: msrmoldyn.azurecr.io
  username: msrmoldyn

storage:
  eastus2:
    storage_account_name: sfmdataeastus2prem # Storage account
    container_name: psm # Container name
    mount_dir: /psm
  arca100:
    storage_account_name: sfmarca100 # Storage account
    container_name: sfm # Container name
    mount_dir: /sfm

code:
  local_dir: ../SFM_framework

jobs:
- name: psmv1_dit_v13_3b
  tags: [Project_Name:Science_Foundation_Model,ProjectID:PRJ-0209-A40,Experiment:SFM_PSM]
  sku: ${NODES}xG${GPUS}-IB
  mpi: true
  identity: managed
  process_count_per_node: 1
  command:
  - export WANDB_TEAM=ai4s-sfm
  - export wandb_group=psm_DiT_3B
  - export wandb_project=psm_VT
  - export WANDB_API_KEY=${WANDB_API_KEY}
  - mkdir ./output
  - export path=run.sh
  - export num_pred_attn_layer=2
  - export layers=36
  - export hidden_size=2048
  - export ffn_size=8192
  - export num_head=32
  - export dropout=0.1
  - export attn_dropout=0.1
  - export act_dropout=0.1
  - export weight_decay=0.0
  - export droppath_prob=0.0
  - export mask_ratio=0.15
  - export d_tilde=1.0
  - export max_lr=1e-4
  - export strategy=Zero1
  - export pipeline_model_parallel_size=0
  - export total_num_steps=300000
  - export warmup_num_steps=10000
  - export train_batch_size=3072
  - export val_batch_size=768
  - export max_tokens=16000
  - export max_length=384
  - export max_residue_num=512
  - export ligand_crop_size=16.0
  - export gradient_accumulation_steps=16
  - export log_interval=100
  - export data_path=/psm/data/
  - export data_path_list='matter-gen-force-filtered,matter-sim-15M-merged,PubChemQC-B3LYP-PM6,AFDB70-plddt70-CA.lmdb,20240101_PDB_Training_Data,ESM_ATLAS_reduce.lmdb,20240630_PDB_Training_Data'
  - export dataset_name_list='mattersim,mattersim,pm6-wb97xd3,esm,pdb,esm,pdbcomplexmultimer'
  - export dataset_split_raito='0.03,0.15,0.3,0.3,0.1,0.02,0.1'
  - export dataset_micro_batch_size='3,3,16,3,3,3,2
  # - export data_path_list='PubChemQC-B3LYP-PM6,AFDB70-plddt70-CA.lmdb,20240101_PDB_Training_Data,20240630_PDB_Training_Data,ESM_ATLAS_reduce.lmdb' #,ur50_23_bpe_pack512.lmdb'
  # - export dataset_name_list='pm6-wb97xd3,esm,pdb,pdbcomplexmultimer,esm' #,ur50'
  # - export dataset_split_raito='0.4,0.3,0.02,0.08,0.2'
  # - export dataset_micro_batch_size='16,2,2,1,2'
  - export fp16=False
  - export clean_sample_ratio=0.5
  - export molecule_energy_loss_ratio=1.0
  - export material_energy_loss_ratio=1.0
  - export material_force_loss_ratio=1.0
  - export AutoGradForce=False
  - export use_hard_dist_loss=True
  - export use_unified_batch_sampler=True
  - export align_x0_in_diffusion_loss=True
  - export decoder_feat4energy=False
  - export encoderfeat4noise=True
  - export only_use_rotary_embedding_for_protein=True
  - export if_total_energy=False
  - export save_batch_interval=2500
  - export loadcheck_path=/sfm/sfmexpresults/peiran/psmv1_dit_v13_3b/checkpoints/global_step5000/mp_rank_00_model_states.pt
  - export psm_finetune_mode=False
  - export save_dir=/sfm/sfmexpresults/peiran/psmv1_dit_v13_3b/checkpoints
  - eval "$$(conda shell.bash hook)" && conda activate sfm
  - python setup_cython.py build_ext --inplace
  - pip install nvidia-dali-cuda120
  - bash ./scripts/psm/pretrain_psm_dit.sh
  submit_args:
    container_args:
      shm_size: 4096g
  preemptible: false
