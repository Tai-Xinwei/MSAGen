description: train_sci_gpt_7b

env_defaults:
  NODES: 4
  GPUS: 8
  #WANDB_API_KEY:  "d34f864932245bbdf3a9396a1ebde883ad2068f3"
  WANDB_API_KEY:  "2ff6f7b6ac42d86aed79dac8b0c10cc9a5a50b5d"



target:
    # service: aml
    # subscription_id: 3f2ab3f5-468d-4ba7-bc14-9d3a9da4bcc5
    # resource_group: TownsendAML1
    # workspace_name: townsendws1
    # cluster: townsend1
    service: aml
    subscription_id: 3eaeebff-de6e-4e20-9473-24de9ca067dc
    resource_group: sfm-ws-rg
    workspace_name: sfm-ws
    cluster: sfm-prem-nd96amsr100-westeurope


environment:
  #image: shufxi/mfmds:20230807
  #registry: itpeus4cr.azurecr.io
  #username: itpeus4cr
  image: yaosen/sfm-cuda:py39-torch2.2.2-cuda12.1-20240412
  registry: msroctocr.azurecr.io
  username: msroctocr


storage:
  blob:
    storage_account_name: hai1data # Storage account
    container_name: mfm # Container name
    mount_dir: /hai1

code:
  local_dir: ../../

jobs:
- name: train_sci_gpt_7bv3_stageA_bs256
  sku: ${NODES}xG${GPUS}
  mpi: true
  process_count_per_node: 1
  command:
  - export WANDB_API_KEY=${WANDB_API_KEY}
  - export WANDB_PROJECT=scigpt7bv3
  - eval "$$(conda shell.bash hook)" && conda activate sfm
  - export PYTHONPATH=./:$${PYTHONPATH}
  - python setup_cython.py build_ext --inplace
  - export loadcheck_path=/hai1/ds_dataset/llama2/llama-2-7b
  - export pipeline_model_parallel_size=1
  - export total_num_steps=200000
  - export epochs=1
  - export learnable_cutoff=32000
  - export train_batch_size=256
  - export gradient_accumulation_steps=8
  - export save_dir=/hai1/shufxi/scigpt/7bv3/stageA_bs256_emb
  - bash scripts/scigpt/pretrain_scigpt_7bv3_stageA.sh
  submit_args:
    container_args:
      shm_size: 1024g
