description: train_sci_gpt_7b

env_defaults:
  NODES: 8
  GPUS: 8
  WANDB_API_KEY:  "local-8904f1b2b7174cdb7c0bf0b8a2c3195c1889c537"

target:
  service: sing
  name: baltic02
  workspace_name: msrresrchws

environment:
  image: yaosen/sfm-cuda:py39-torch2.2.2-cuda12.1-20240412
  registry: msroctocr.azurecr.io
  username: msroctocr


storage:
  blob:
    storage_account_name: hai1data # Storage account
    container_name: mfm # Container name
    mount_dir: /hai1

code:
  local_dir: ../../

jobs:
- name: train_sci_gpt_7bv3_stageA_prot_bs512_emb_8xG8H100
  sku: ${NODES}xG${GPUS}
  tags: [Project_Name:Science_Foundation_Model,ProjectID:PRJ-0209-A40,Experiment:SFM_NLM_PROTEIN]
  mpi: true
  process_count_per_node: 1
  command:
  - export WANDB_API_KEY=${WANDB_API_KEY}
  - export WANDB_PROJECT=Bio0
  - export WANDB_BASE_URL="https://microsoft-research.wandb.io"
  - export WANDB_TEAM=ai4s-sfm
  - export wandb_group=7bv3_prot_emb
  - export NCCL_DEBUG=INFO
  - eval "$$(conda shell.bash hook)" && conda activate sfm
  - pip install -e . --no-deps
  - python setup_cython.py build_ext --inplace
  - export loadcheck_path=/hai1/ds_dataset/llama2/llama-2-7b
  - export pipeline_model_parallel_size=1
  - export total_num_steps=200000
  - export epochs=10
  - export learnable_cutoff=32000
  - export train_batch_size=512
  - export gradient_accumulation_steps=8
  - export save_dir=/hai1/shufxi/scigpt/7bv3/stageA_prot_e10_bs512_emb_8xG8H100
  - export save_batch_interval=0
  - export train_data_path=/hai1/shufxi/data/scigpt/v3/train.prot.npy
  - bash scripts/scigpt/pretrain_scigpt_7bv3_stageA_prot.sh
  submit_args:
    env:
      SHARED_MEMORY_PERCENT: 1.0
      CUDA_LAUNCH_BLOCKING: 1
    container_args:
      shm_size: 1024g
