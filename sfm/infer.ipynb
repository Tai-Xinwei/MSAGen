{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-18 10:50:32,621] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.add_argument import add_argument\n",
    "import argparse\n",
    "import deepspeed\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"MFMds\")\n",
    "parser.add_argument(\n",
    "    \"--dataset-name\", type=str, default=\"PM6-Full-3D\", help=\"dataset name\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"./dataset\", help=\"path to dataset\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--loadcheck_path\", type=str, default=\"\", help=\"path to dataset\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-b\",\n",
    "    \"--batch_size\",\n",
    "    default=1024,\n",
    "    type=int,\n",
    "    help=\"mini-batch size (default: 32)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-e\",\n",
    "    \"--epochs\",\n",
    "    default=100,\n",
    "    type=int,\n",
    "    help=\"number of total epochs (default: 50)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local_rank\",\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help=\"local rank passed from distributed launcher\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--global_rank\",\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help=\"global rank passed from distributed launcher\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--backend\", type=str, default=\"nccl\", help=\"distributed backend\"\n",
    ")\n",
    "parser.add_argument(\"--seed\", type=int, default=666666, help=\"PRNG seed\")\n",
    "parser.add_argument(\"--node_rank\", type=int, default=-1)\n",
    "parser.add_argument(\"--rank\", type=int, default=-1)\n",
    "parser.add_argument(\"--num-classes\", type=int, default=1, help=\"number of classes\")\n",
    "parser.add_argument(\n",
    "    \"--encoder_embed_dim\", type=int, default=768, help=\"encoder embedding dimension\"\n",
    ")\n",
    "parser.add_argument(\"--encoder_ffn_embed_dim\", type=int, default=768, help=\"\")\n",
    "parser.add_argument(\n",
    "    \"--llm_hidden_size\", type=int, default=4096, help=\"encoder embedding dimension\"\n",
    ")\n",
    "parser.add_argument(\"--llm_ffn_size\", type=int, default=256, help=\"\")\n",
    "parser.add_argument(\"--encoder_attention_heads\", type=int, default=32, help=\"\")\n",
    "parser.add_argument(\"--encoder_layers\", type=int, default=24, help=\"\")\n",
    "parser.add_argument(\"--max-nodes\", type=int, default=8, help=\"\")\n",
    "parser.add_argument(\"--add-3d\", default=False, action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\"--no-2d\", default=False, action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\"--num-3d-bias-kernel\", type=int, default=128, help=\"\")\n",
    "parser.add_argument(\"--num_pred_attn_layer\", type=int, default=4, help=\"\")\n",
    "parser.add_argument(\"--droppath_prob\", type=float, default=0.0, help=\"\")\n",
    "parser.add_argument(\"--attn_dropout\", type=float, default=0.1, help=\"\")\n",
    "parser.add_argument(\"--act_dropout\", type=float, default=0.1, help=\"\")\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.0, help=\"\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"\")\n",
    "parser.add_argument(\"--sandwich_ln\", default=True, action=\"store_true\", help=\"\")\n",
    "# parser.add_argument(\"--ft\", default=False, action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\"--infer\", action=\"store_true\", default=False, help=\"\")\n",
    "parser.add_argument(\"--noise_scale\", type=float, default=0.2, help=\"\")\n",
    "parser.add_argument(\"--mask_ratio\", type=float, default=0.3, help=\"\")\n",
    "parser.add_argument(\"--log-interval\", type=int, default=100, help=\"log per n steps\")\n",
    "parser.add_argument(\n",
    "    \"--pipeline_parallelism\", type=int, default=0, help=\"log per n steps\"\n",
    ")\n",
    "parser.add_argument(\"--steps\", type=int, default=10000000, help=\"log per n steps\")\n",
    "parser.add_argument(\n",
    "    \"--output_path\", type=str, default=\"/blob/output\", help=\"log per n steps\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--d_tilde\", type=float, default=1, help=\"mu transfer multiplier\"\n",
    ")\n",
    "parser.add_argument(\"--max_lr\", type=float, default=1e-3, help=\"max lr\")\n",
    "parser.add_argument(\n",
    "    \"--total_num_steps\",\n",
    "    type=int,\n",
    "    default=1000000,\n",
    ")\n",
    "parser.add_argument(\"--warmup_num_steps\", type=int, default=60000)\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--smiles_dict_path\",\n",
    "    type=str,\n",
    "    default=\"/home/peiran/FMproj/moleculenet_data/data/mol2idx_dict.jsonl\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--llm_model_name_or_path\",\n",
    "    type=str,\n",
    "    default=\"/home/peiran/FMproj/MetaLLM-converted\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--loadmfmcheck_path\",\n",
    "    type=str,\n",
    "    default=\"/home/peiran/FMproj/DiffTM100M/checkpoint7.pt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--loadllmcheck_path\", type=str, default=\"/home/peiran/FMproj/MetaLLM-converted\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset_names\", type=str, default=\"hiv,clintox,sider,tox21,bbbp,bace\"\n",
    ")\n",
    "parser.add_argument(\"--dataset_ratios\", type=str, default=\"\")\n",
    "parser.add_argument(\"--dataset_splits\", type=str, default=\"\")\n",
    "parser.add_argument(\"--mol2idx_dict_path\", type=str, default=\"\")\n",
    "parser.add_argument(\"--in_memory\", type=bool, default=False)\n",
    "parser.add_argument(\"--mol_size_path\", type=str, default=\"\")\n",
    "parser.add_argument(\"--pool_mode\", type=str, default=\"multimol\")\n",
    "parser.add_argument(\"--embedding_length\", type=int, default=20)\n",
    "parser.add_argument(\"--btn_adaptor\", type=bool, default=False)\n",
    "parser.add_argument(\"--mfm_lora\", type=bool, default=False)\n",
    "parser.add_argument(\"--model_max_length\", type=int, default=2048)\n",
    "\n",
    "parser = deepspeed.add_config_arguments(parser)\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "global_step = \"global_step6000\"\n",
    "\n",
    "total_size = 0\n",
    "index_map = {\"weight_map\": {}}\n",
    "model_states2 = {}\n",
    "for i in range(36):\n",
    "    ckpt_path = \"/home/peiran/FMproj/output/\" + global_step + f\"/layer_{i:02d}-model_states.pt\"\n",
    "    model_states = torch.load(ckpt_path, map_location='cpu')\n",
    "    # print(ckpt_path)\n",
    "    all_keys = list(model_states.keys())\n",
    "    # print(all_keys)\n",
    "    for key in all_keys:\n",
    "        if key.find('lora') != -1:\n",
    "            continue\n",
    "        if key.find(\"dummy\") != -1:\n",
    "            continue\n",
    "\n",
    "        weight = model_states.pop(key)\n",
    "        key = key.replace(\"base_model.model.\", \"\")\n",
    "\n",
    "        if i == 0:\n",
    "            key = \"graphormer_encoder.\" + key\n",
    "        elif i == 1:\n",
    "            if key.startswith(\"embed_tokens.weight\"):\n",
    "                key = \"decoder.model.\" + key\n",
    "            else:\n",
    "                key = \"adaptor.\" + key\n",
    "        elif i < 34:\n",
    "            key = \"decoder.model.layers.{}.\".format(i-2) + key\n",
    "        elif i == 34:\n",
    "            key = \"decoder.model.\" + key\n",
    "        elif i == 35:\n",
    "            key = \"decoder.\" + key\n",
    "\n",
    "        # index_map[\"weight_map\"][key] = f\"layer_{i:02d}-model_states.bin\"\n",
    "        model_states2[key] = weight\n",
    "        total_size += weight.nelement() * weight.element_size()\n",
    "        index_map[\"weight_map\"][key] = f\"graphormerllama.bin\"\n",
    "    del model_states\n",
    "\n",
    "    # torch.save(model_states2, \"/home/peiran/FMproj/output/\" + global_step + f\"/layer_{i:02d}-model_states.bin\")\n",
    "torch.save(model_states2, \"/home/peiran/FMproj/output/\" + global_step + f\"/graphormerllama.bin\")\n",
    "del model_states2\n",
    "index_map[\"total_size\"] = total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/home/peiran/FMproj/output/\" +  global_step + \"/graphormerlamma.bin.index.json\", \"w\") as out_file:\n",
    "    json.dump(index_map, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:FoundationModel:Namespace(dataset_name='PM6-Full-3D', data_path='./dataset', loadcheck_path='', batch_size=1024, epochs=100, local_rank=-1, global_rank=-1, backend='nccl', seed=666666, node_rank=-1, rank=-1, num_classes=1, encoder_embed_dim=768, encoder_ffn_embed_dim=768, llm_hidden_size=4096, llm_ffn_size=256, encoder_attention_heads=32, encoder_layers=24, max_nodes=8, add_3d=False, no_2d=False, num_3d_bias_kernel=128, num_pred_attn_layer=4, droppath_prob=0.0, attn_dropout=0.1, act_dropout=0.1, dropout=0.0, weight_decay=0.0, sandwich_ln=True, infer=False, noise_scale=0.2, mask_ratio=0.3, log_interval=100, pipeline_parallelism=0, steps=10000000, output_path='/blob/output', d_tilde=1, max_lr=0.001, total_num_steps=1000000, warmup_num_steps=60000, smiles_dict_path='/home/peiran/FMproj/moleculenet_data/data/mol2idx_dict.jsonl', llm_model_name_or_path='/home/peiran/FMproj/MetaLLM-converted/7B', loadmfmcheck_path='/home/peiran/FMproj/DiffTM100M/checkpoint7.pt', loadllmcheck_path='/home/peiran/FMproj/MetaLLM-converted', dataset_names='hiv,clintox,sider,tox21,bbbp,bace', dataset_ratios='', dataset_splits='', mol2idx_dict_path='', in_memory=False, mol_size_path='', pool_mode='multimol', embedding_length=20, btn_adaptor=False, mfm_lora=False, model_max_length=2048, deepspeed=False, deepspeed_config=None, deepscale=False, deepscale_config=None, deepspeed_mpi=False, attention_dropout=0.1, activation_fn='gelu', encoder_normalize_before=True, apply_graphormer_init=True, share_encoder_input_output_embed=False, no_token_positional_embeddings=False, pre_layernorm=False, encoder_learned_pos=False, num_segment=2, sentence_class_num=2, sent_loss=False, apply_bert_init=False, pooler_activation_fn='tanh', atom_loss_coeff=1.0, pos_loss_coeff=1.0, max_positions=512, num_atoms=4608, num_edges=1536, num_in_degree=512, num_out_degree=512, num_spatial=512, num_edge_dis=128, multi_hop_max_dist=5, edge_type='multi_hop')\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "from models.generalist.graphormer_llama import GraphormerLlamaModel\n",
    "\n",
    "args.llm_model_name_or_path = \"/home/peiran/FMproj/MetaLLM-converted/7B_insft\"\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = GraphormerLlamaModel(args, vocab_size=32011)\n",
    "    # model = LlamaForCausalLM.from_pretrained(args.llm_model_name_or_path)\n",
    "\n",
    "names = []\n",
    "for name, parameter in model.named_parameters():\n",
    "    names.append(name)\n",
    "# print(names)\n",
    "\n",
    "json.dump(names, open(\"/home/peiran/FMproj/output/\" + global_step + \"/graphormerlamma.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32011, 4096])\n"
     ]
    }
   ],
   "source": [
    "model.decoder.tie_weights()\n",
    "print(model.decoder.model.embed_tokens.weight.shape)\n",
    "# model.load_state_dict(torch.load(\"/home/peiran/FMproj/output/\" + global_step + \"/graphormerllama.bin\", map_location='cpu'), strict=True)\n",
    "# model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32011, 4096])\n",
      "torch.Size([32011, 4096])\n"
     ]
    }
   ],
   "source": [
    "from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model, \"/home/peiran/FMproj/output/\" + global_step, device_map=\"auto\", no_split_module_classes=[\"LlamaDecoderLayer\"]\n",
    ")\n",
    "\n",
    "# model = load_checkpoint_and_dispatch(\n",
    "#     model, args.llm_model_name_or_path, device_map=\"auto\", no_split_module_classes=[\"LlamaDecoderLayer\"]\n",
    "# )\n",
    "\n",
    "print(model.decoder.model.embed_tokens.weight.shape)\n",
    "print(model.decoder.lm_head.weight.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.chemical_tokens import CHEMICAL_TOKENS\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    args.llm_model_name_or_path,\n",
    "    cache_dir=False,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "special_tokens_dict = dict()\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "if tokenizer.eos_token is None:\n",
    "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "if tokenizer.bos_token is None:\n",
    "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "if tokenizer.unk_token is None:\n",
    "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "special_tokens_dict[\"additional_special_tokens\"] = CHEMICAL_TOKENS\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  1763,  2329,   263,  9391, 10348,  4742,   297,   590, 19022,\n",
      "         29892]])\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer(\"To fix a broken audio device in my laptop,\",\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=\"longest\",\n",
    "                        max_length=512,\n",
    "                        truncation=True)\n",
    "input_ids = tokenized.input_ids[0].unsqueeze(0)\n",
    "print(input_ids)\n",
    "input_ids = input_ids.cuda('cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,  1763,  2329,   263,  9391, 10348,  4742,   297,   590, 19022,\n",
      "        29892,   306,   817,   304,  1722,   701,   590, 19022, 29889,  1317,\n",
      "          727,   738,  4780,   982,   304,  1722,   372], device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "res = model.decoder.generate(input_ids, do_sample=True, temperature=0.7, max_new_tokens=16, output_scores=True, return_dict_in_generate=True)\n",
    "print(res.sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To fix a broken audio device in my laptop, I need to open up my laptop. Is there any easy way to open it'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(res.sequences[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
