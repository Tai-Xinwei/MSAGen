{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.add_argument import add_argument\n",
    "import argparse\n",
    "import deepspeed\n",
    "import json\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"MFMds\")\n",
    "parser.add_argument(\n",
    "    \"--dataset-name\", type=str, default=\"PM6-Full-3D\", help=\"dataset name\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"./dataset\", help=\"path to dataset\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--loadcheck_path\", type=str, default=\"\", help=\"path to dataset\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-b\",\n",
    "    \"--batch_size\",\n",
    "    default=1024,\n",
    "    type=int,\n",
    "    help=\"mini-batch size (default: 32)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-e\",\n",
    "    \"--epochs\",\n",
    "    default=100,\n",
    "    type=int,\n",
    "    help=\"number of total epochs (default: 50)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--local_rank\",\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help=\"local rank passed from distributed launcher\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--global_rank\",\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help=\"global rank passed from distributed launcher\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--backend\", type=str, default=\"nccl\", help=\"distributed backend\"\n",
    ")\n",
    "parser.add_argument(\"--seed\", type=int, default=666666, help=\"PRNG seed\")\n",
    "parser.add_argument(\"--node_rank\", type=int, default=-1)\n",
    "parser.add_argument(\"--rank\", type=int, default=-1)\n",
    "parser.add_argument(\"--num-classes\", type=int, default=1, help=\"number of classes\")\n",
    "parser.add_argument(\n",
    "    \"--encoder_embed_dim\", type=int, default=768, help=\"encoder embedding dimension\"\n",
    ")\n",
    "parser.add_argument(\"--encoder_ffn_embed_dim\", type=int, default=768, help=\"\")\n",
    "parser.add_argument(\n",
    "    \"--llm_hidden_size\", type=int, default=4096, help=\"encoder embedding dimension\"\n",
    ")\n",
    "parser.add_argument(\"--llm_ffn_size\", type=int, default=256, help=\"\")\n",
    "parser.add_argument(\"--encoder_attention_heads\", type=int, default=32, help=\"\")\n",
    "parser.add_argument(\"--encoder_layers\", type=int, default=24, help=\"\")\n",
    "parser.add_argument(\"--max-nodes\", type=int, default=8, help=\"\")\n",
    "parser.add_argument(\"--add-3d\", default=False, action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\"--no-2d\", default=False, action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\"--num-3d-bias-kernel\", type=int, default=128, help=\"\")\n",
    "parser.add_argument(\"--num_pred_attn_layer\", type=int, default=4, help=\"\")\n",
    "parser.add_argument(\"--droppath_prob\", type=float, default=0.0, help=\"\")\n",
    "parser.add_argument(\"--attn_dropout\", type=float, default=0.1, help=\"\")\n",
    "parser.add_argument(\"--act_dropout\", type=float, default=0.1, help=\"\")\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.0, help=\"\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"\")\n",
    "parser.add_argument(\"--sandwich_ln\", default=True, action=\"store_true\", help=\"\")\n",
    "parser.add_argument(\"--infer\", action=\"store_true\", default=False, help=\"\")\n",
    "parser.add_argument(\"--noise_scale\", type=float, default=0.2, help=\"\")\n",
    "parser.add_argument(\"--mask_ratio\", type=float, default=0.3, help=\"\")\n",
    "parser.add_argument(\"--log-interval\", type=int, default=100, help=\"log per n steps\")\n",
    "parser.add_argument(\n",
    "    \"--pipeline_parallelism\", type=int, default=0, help=\"log per n steps\"\n",
    ")\n",
    "parser.add_argument(\"--steps\", type=int, default=10000000, help=\"log per n steps\")\n",
    "parser.add_argument(\n",
    "    \"--output_path\", type=str, default=\"/blob/output\", help=\"log per n steps\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--d_tilde\", type=float, default=1, help=\"mu transfer multiplier\"\n",
    ")\n",
    "parser.add_argument(\"--max_lr\", type=float, default=1e-3, help=\"max lr\")\n",
    "parser.add_argument(\n",
    "    \"--total_num_steps\",\n",
    "    type=int,\n",
    "    default=1000000,\n",
    ")\n",
    "parser.add_argument(\"--warmup_num_steps\", type=int, default=60000)\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--smiles_dict_path\",\n",
    "    type=str,\n",
    "    default=\"/home/peiran/FMproj/moleculenet_data/data/mol2idx_dict.jsonl\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--llm_model_name_or_path\",\n",
    "    type=str,\n",
    "    default=\"/home/peiran/FMproj/MetaLLM-converted\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--loadmfmcheck_path\",\n",
    "    type=str,\n",
    "    default=\"/home/peiran/FMproj/DiffTM100M/checkpoint7.pt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--loadllmcheck_path\", type=str, default=\"/home/peiran/FMproj/MetaLLM-converted\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset_names\", type=str, default=\"hiv,clintox,sider,tox21,bbbp,bace\"\n",
    ")\n",
    "parser.add_argument(\"--dataset_ratios\", type=str, default=\"\")\n",
    "parser.add_argument(\"--dataset_splits\", type=str, default=\"\")\n",
    "parser.add_argument(\"--mol2idx_dict_path\", type=str, default=\"\")\n",
    "parser.add_argument(\"--in_memory\", type=bool, default=False)\n",
    "parser.add_argument(\"--mol_size_path\", type=str, default=\"\")\n",
    "parser.add_argument(\"--pool_mode\", type=str, default=\"full\")\n",
    "parser.add_argument(\"--embedding_length\", type=int, default=20)\n",
    "parser.add_argument(\"--btn_adaptor\", type=bool, default=False)\n",
    "parser.add_argument(\"--mfm_lora\", type=bool, default=False)\n",
    "parser.add_argument(\"--model_max_length\", type=int, default=2048)\n",
    "\n",
    "parser = deepspeed.add_config_arguments(parser)\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "global_step = \"llama2/global_step2000\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "total_size = 0\n",
    "index_map = {\"weight_map\": {}}\n",
    "model_states2 = {}\n",
    "for i in range(37):\n",
    "    ckpt_path = \"/home/peiran/FMproj/output/\" + global_step + f\"/layer_{i:02d}-model_states.pt\"\n",
    "    model_states = torch.load(ckpt_path, map_location='cpu')\n",
    "    # print(ckpt_path)\n",
    "    all_keys = list(model_states.keys())\n",
    "\n",
    "    for key in all_keys:\n",
    "        if key.find('lora') != -1:\n",
    "            continue\n",
    "        if key.find(\"dummy\") != -1:\n",
    "            continue\n",
    "        if key.find(\"partial_learnable_emb\") != -1:\n",
    "            continue\n",
    "        \n",
    "        weight = model_states.pop(key)\n",
    "        key = key.replace(\"base_model.model.\", \"\")\n",
    "\n",
    "        if i == 0:\n",
    "            key = \"graphormer_encoder.\" + key\n",
    "        elif i <= 2:\n",
    "            if key.startswith(\"embed_tokens.weight\"):\n",
    "                key = \"decoder.model.\" + key\n",
    "            else:\n",
    "                key = \"adaptor.\" + key\n",
    "        # elif i < 83: #35:\n",
    "        #     key = \"decoder.model.layers.{}.\".format(i-3) + key\n",
    "        # elif i == 83: #35:\n",
    "        #     key = \"decoder.model.\" + key\n",
    "        # elif i == 84: #36:\n",
    "        #     key = \"decoder.\" + key\n",
    "        elif i < 35:\n",
    "            key = \"decoder.model.layers.{}.\".format(i-3) + key\n",
    "        elif i == 35:\n",
    "            key = \"decoder.model.\" + key\n",
    "        elif i == 36:\n",
    "            key = \"decoder.\" + key\n",
    "\n",
    "\n",
    "        # index_map[\"weight_map\"][key] = f\"layer_{i:02d}-model_states.bin\"\n",
    "        model_states2[key] = weight\n",
    "        total_size += weight.nelement() * weight.element_size()\n",
    "        index_map[\"weight_map\"][key] = f\"graphormerllama.bin\"\n",
    "    del model_states\n",
    "\n",
    "    # torch.save(model_states2, \"/home/peiran/FMproj/output/\" + global_step + f\"/layer_{i:02d}-model_states.bin\")\n",
    "torch.save(model_states2, \"/home/peiran/FMproj/output/\" + global_step + f\"/graphormerllama.bin\")\n",
    "del model_states2\n",
    "index_map[\"total_size\"] = total_size\n",
    "\n",
    "with open(\"/home/peiran/FMproj/output/\" +  global_step + \"/graphormerlamma.bin.index.json\", \"w\") as out_file:\n",
    "    json.dump(index_map, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.generalist.graphormer_llama import GraphormerLlamaModel\n",
    "\n",
    "args.llm_model_name_or_path = \"/home/peiran/FMproj/llama2/llama-2-7b\"\n",
    "args.ft = True\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = GraphormerLlamaModel(args, vocab_size=32011)\n",
    "    # model = LlamaForCausalLM.from_pretrained(args.llm_model_name_or_path)\n",
    "\n",
    "names = []\n",
    "for name, parameter in model.named_parameters():\n",
    "    names.append(name)\n",
    "# print(names)\n",
    "\n",
    "json.dump(names, open(\"/home/peiran/FMproj/output/\" + global_step + \"/graphormerlamma.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import load_checkpoint_and_dispatch, infer_auto_device_map, dispatch_model, load_checkpoint_in_model\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    model, \n",
    "    no_split_module_classes=[\"LlamaDecoderLayer\", \"GraphormerSentenceEncoderLayer\"],\n",
    "    dtype='float16'\n",
    ")\n",
    "# print(device_map)\n",
    "full_model_device_map = {k: 0 for k, v in device_map.items()}\n",
    "\n",
    "# model = load_checkpoint_in_model(\n",
    "#     model, \"/home/peiran/FMproj/output/\" + global_step, device_map=device_mpa, dtype='float16', offload_state_dict=True \n",
    "# )\n",
    "\n",
    "# model.decoder.tie_weights()\n",
    "\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model, \n",
    "    \"/home/peiran/FMproj/output/\" + global_step, \n",
    "    device_map=full_model_device_map, \n",
    "    no_split_module_classes=[\"LlamaDecoderLayer\", \"GraphormerSentenceEncoderLayer\"],\n",
    "    offload_state_dict = True, \n",
    "    offload_buffers = True, \n",
    "    dtype='float16'\n",
    ")\n",
    "\n",
    "# full_model_device_map = {f\"model.{k}\": v for k, v in device_map.items()}\n",
    "# full_model_device_map[\"lm_head\"] = 0\n",
    "# dispatch_model(model, device_map=full_model_device_map)\n",
    "\n",
    "print(model.decoder.model.embed_tokens.weight.shape)\n",
    "print(model.decoder.lm_head.weight.shape)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.mol_data.tokenizer import MolTokenizer\n",
    "from utils.move_to_device import move_to_device\n",
    "import torch\n",
    "args.llm_model_name_or_path = \"/home/peiran/FMproj/llama2/llama-2-7b\"\n",
    "\n",
    "tokenizer = MolTokenizer(args)\n",
    "\n",
    "# batched_smile_data = move_to_device(batched_smile_data, 'cuda:1')\n",
    "# model.decoder.tie_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = [\"#Prompt: The smile of the molecule is <mol> c1ccc(C2CN=C3NCCN32)cc1 </mol>, give its description\"]\n",
    "# text = [\"### Instruction: \\nWhat can you tell me about this molecule?\\n  ###Input: <mol> CCCCCc1cccc([C@@]2(c3ccc(OC(F)F)cc3)N=C(N)N(C)C2=O)c1 </mol> ### Response:\\n\"]\n",
    "# text = [\"### Instruction:\\nWhat can you tell me about this molecule?\\n  ###Input: <mol> c1ccc(C2CN=C3NCCN32)cc1 </mol> ### Response:\\n\"]\n",
    "# text = [\"### Instruction:\\nWhat can you tell me about this molecule?\\n  ###Input: <mol> C1=C(OC(=C1)C(=O)OCCO)C(=O)[O-1] </mol> ### Response:\\n\"]\n",
    "# text = [\"### Instruction:\\nWhat can you tell me about this molecule?\\n  ###Input: <mol> CC1=CC(=NC(=C1)C)C </mol> ### Response:\\n\"]\n",
    "# text = [\"### Instruction:\\nWhat can you tell me about this molecule?\\n\\n###Input:\\n<mol>CCC(CC(C)(C)C(=O)O)C1=CC=CC=C1</mol>\\n\\n### Response:\\n\"]\n",
    "# text = [\"### Instruction:\\nProvide a brief overview of this molecule.\\n\\n###Input:\\n<mol>CC(=O)C</mol>\\n\\n### Response:\\n\"]\n",
    "\n",
    "smile_list = [\"CCCCCc1cccc([C@@]2(c3ccc(OC(F)F)cc3)N=C(N)N(C)C2=O)c1\", \n",
    "              \"c1ccc(C2CN=C3NCCN32)cc1\", \n",
    "              \"C1=C(OC(=C1)C(=O)OCCO)C(=O)[O-1]\", \n",
    "              \"CC1=CC(=NC(=C1)C)C\",\n",
    "              \"CCC(CC(C)(C)C(=O)O)C1=CC=CC=C1\", \n",
    "              \"O=C(COc1ccc(Cl)cc1)OCCNC12CC3CC(CC(C3)C1)C2\",\n",
    "              \"CC(=O)C\",\n",
    "              \"C[C@]1(Cn2ccnn2)[C@H](C(=O)O)N2C(=O)C[C@H]2S1(=O)=O\",\n",
    "              \"CC[C@@]1(O)C[C@H](O[C@H]2C[C@H](N(C)C)[C@H](O[C@H]3C[C@@H]4O[C@H]5CC(=O)[C@H](C)O[C@H]5O[C@@H]4[C@H](C)O3)[C@H](C)O2)c2c(O)c3c(c(O)c2[C@H]1O[C@H]1C[C@H](N(C)C)[C@H](O)[C@H](C)O1)C(=O)c1cccc(O)c1C3=O\", #inactive\n",
    "              \"CCCNCC(O)COc1ccccc1C(=O)CCc1ccccc1\", \n",
    "              \"CC(=O)NCCn1c(C)cc(C=O)c1C\", #active\n",
    "              \"O=C(Cc1ccc(Cl)c(Cl)c1)N1CCc2sccc2[C@H]1CN1CCCC1\", #active\n",
    "              \"CC1(C2=C(C=CC(=C2)S(=O)(=O)[O-1])[N+1](=C1/C=C/C3=C(/C(=C/C=C/4\\\\C(C5=C(N4CCCCS(=O)(=O)O)C=CC(=C5)S(=O)(=O)O)(C)C)/CCC3)OC6=CC=C(C=C6)C[C@@H1](C(=O)O)NC(=O)C7=CC=C(C=C7)NCC8=CN=C9C(=N8)C(=O)NC(=N9)N)CCCCS(=O)(=O)O)C\",\n",
    "              \"O=C(O)Cc1ccc(N2N=C(c3ccccc3)CC2=O)cc1\",\n",
    "              \"CC(=O)OCC(=O)C12OC3(CCCC3)OC1CC1C3CCC4=CC(=O)C=CC4(C)C3(F)C(O)CC12C\",\n",
    "              'O=C(O)CC1C(=O)OC2C3COC(=O)c4cc(O)c(O)c(O)c4-c4c(cc(O)c(O)c4O)C(=O)OC2C(OC(=O)c2cc(O)c(O)c4c2C1C(O)C(=O)O4)C(OC(=O)c1cc(O)c(O)c(O)c1)O3',\n",
    "              \"COc1cc(-c2nn3c(C)c(-c4ccccc4)nc3s2)cc(OC)c1OC\",\n",
    "              \"CCO\",\n",
    "              \"CC(C(C1=CC=CC=C1)O)NC.Cl\",\n",
    "              ]\n",
    "\n",
    "question_list = [\"Is this molecule toxic and why?\",\n",
    "                 \"Is the molecule easily soluble in water and why?\",\n",
    "                 \"Does the molecule has good oral bioavailability and why?\",\n",
    "                 \"Can the molecule pass the blood-brain barrier and why?\",\n",
    "                 \"Whether the molecule can pass the blood-brain barrier?\",\n",
    "                 \"Provide a brief overview of this molecule.\",\n",
    "                 \"The number of Carbon atom in this molecule is?\",\n",
    "                 \"Could you provide a description this molecule?\",\n",
    "                 \"What the most important feature of this molecule?\",\n",
    "                 \"Whether the molecule can inhibit HIV replication and why?\",\n",
    "                 \"Whether the drug can cause blood and lymphatic system disorders and why?\",\n",
    "                 \"Whether the drug can treat HIV and why?\",\n",
    "                 \"What is potential side effect of this drug and why?\",\n",
    "                 ]\n",
    "\n",
    "smile = smile_list[-1]\n",
    "question = question_list[-1]\n",
    "text = [\"### Instruction:\\n{}\\n\\n### Input: <mol>{}</mol> ### Response:\\n\".format(question, smile)]\n",
    "\n",
    "# text = [\"### Instruction:\\nIs this molecule toxic\\n\\n###Input:\\n<mol>CCCCCc1cccc([C@@]2(c3ccc(OC(F)F)cc3)N=C(N)N(C)C2=O)c1</mol>\\n\\n### Response:\\n\"]\n",
    "# text = [\"### Instruction:\\nIs this molecule able to penetrate blood-brain barrier.\\n\\n###Input:\\n<mol> CCCCCc1cccc([C@@]2(c3ccc(OC(F)F)cc3)N=C(N)N(C)C2=O)c1 </mol>\\n\\n### Response:\\n\"]\n",
    "\n",
    "# text = [\"The smile of the molecule is <mol>CCO</mol>, provide a description of this molecule: \"]\n",
    "text = [\"To fix a broken audio device in my laptop\"]\n",
    "input_ids, batched_smile_data, llm_mask = tokenizer.tokenize(text)\n",
    "# print(input_ids)\n",
    "maksed_input_ids = torch.where(input_ids>0, input_ids, 0)\n",
    "inputtext = tokenizer.text_tokenizer.decode(maksed_input_ids[0], skip_special_tokens=False)\n",
    "# print(maksed_input_ids)\n",
    "print(\"smile: \", smile)\n",
    "\n",
    "print(inputtext)\n",
    "# input_ids = input_ids.to('cuda:0')\n",
    "# llm_mask = llm_mask.to('cuda:0')\n",
    "res = model.generate(batched_smile_data, input_ids=input_ids, attention_mask=llm_mask, do_sample=True, temperature=0.9, max_new_tokens=256, output_scores=True, return_dict_in_generate=True)\n",
    "# print(res.sequences[0])\n",
    "print(text)\n",
    "res = tokenizer.text_tokenizer.decode(res.sequences[0], skip_special_tokens=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# text = [\"###Prompt\\nDoes <mol> O=S(=O)([O-])[O-].[NH4+].[NH4+] </mol> can react with <mol> NC(=O)c1ccccc1 </mol> and what is the product?\\n###Response:\\n\"]\n",
    "# smile1 = \"Cc1ccc(/C=C2\\\\C(=O)C3(C)CCC2C3(C)C)cc1\"\n",
    "# smile2 = \"C[Si](C)(C)n1ccnc1\"\n",
    "smile1 = \"CCO\"\n",
    "smile2 = \"CC(=O)O\"\n",
    "smile3 = \"CC1=CC(=NC(=C1)C)C\"\n",
    "# text = [\"### Instruction:\\nDoes two molecules react with each other and what is the product?\\n\\n### Input:\\n<mol> {} </mol>\\n <mol> {} </mol> ### Response:\\n\".format(smile1, smile2)]\n",
    "# text = [\"### Instruction:\\nWhat compounds is need to synthesis the molecule?\\n\\n### Input:\\n<mol> {} </mol> ### Response:\\n\".format(smile3)]\n",
    "text = [\"Does molecules <mol> {} </mol> react with <mol> {} </mol> and what is the product?\".format(smile1, smile2)]\n",
    "\n",
    "# text = ['### Instruction:\\n\\nThis is a reaction question.\\n\\n###Input:\\nWhat molecule can this molecule <mol> {} </mol> react with? ### Output:\\n'.format(smile1)]\n",
    "# text = [\"### Instruction:\\nDoes <mol> {} </mol> react with <mol> {} </mol> and what is the product?\\n\\n### Input:\\n<mol> {} </mol> <mol> {} </mol>\\n\\n### Response:\\n\".format(smile1, smile2, smile1, smile2)]\n",
    "\n",
    "# text = [\"###Prompt\\nHow to sythesis <mol> C=CCC#N </mol>\\n###Response:\\n\"]\n",
    "\n",
    "input_ids, batched_smile_data, llm_mask = tokenizer.tokenize(text)\n",
    "# text_list, smile_list, _ = tokenizer.split_text_and_mol(text)\n",
    "\n",
    "# print(text_list, smile_list);exit()\n",
    "print(input_ids)\n",
    "# maksed_input_ids = torch.where(input_ids>0, input_ids, 0)\n",
    "# inputtext = tokenizer.text_tokenizer.decode(maksed_input_ids[0], skip_special_tokens=False)\n",
    "# print(inputtext)\n",
    "\n",
    "# input_ids = input_ids.to('cuda:0')\n",
    "# llm_mask = llm_mask.to('cuda:0')\n",
    "# input_ids = input_ids.to('cpu')\n",
    "# llm_mask = llm_mask.to('cpu')\n",
    "res = model.generate(batched_smile_data, input_ids=input_ids, attention_mask=llm_mask, do_sample=True, temperature=0.7, max_new_tokens=512, output_scores=True, return_dict_in_generate=True)\n",
    "res = tokenizer.text_tokenizer.decode(res.sequences[0], skip_special_tokens=False)\n",
    "print(text)\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import Draw\n",
    "IPythonConsole.drawOptions.addAtomIndices = True\n",
    "IPythonConsole.molSize = 600,600\n",
    "\n",
    "mol = Chem.MolFromSmiles('O=C(COc1ccc(Cl)cc1)OCCNC12CC3CC(CC(C3)C1)C2')\n",
    "mol\n",
    "\n",
    "# def mol_with_atom_index(mol):\n",
    "#     for atom in mol.GetAtoms():\n",
    "#         atom.SetAtomMapNum(atom.GetIdx())\n",
    "#     return mol\n",
    "# mol_with_atom_index(mol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
