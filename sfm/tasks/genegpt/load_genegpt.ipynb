{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrain model use LlamaForCausalLM (file in ml-la container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100m 6mer 160k\n",
    "pretrain ckpt  `/ml-la/v-zekunguo/gene/checkpoints/100m6kmer160k`\n",
    "config path `/ml-la/v-zekunguo/gene/checkpoints/config/config_1b`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b 6mer 16k\n",
    "pretrain ckpt  1bm `/ml-la/v-zekunguo/gene/checkpoints/real_1b6kmer16k`\n",
    "config path `/ml-la/v-zekunguo/gene/checkpoints/config/config_100m`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import LlamaConfig, LlamaForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model100m(config_path, ckpt_path):\n",
    "    config = LlamaConfig.from_json_file(config_path)\n",
    "    model = LlamaForCausalLM(config)\n",
    "    model_dict = model.state_dict()\n",
    "    print(model_dict.keys())\n",
    "    flag = \"\"\n",
    "    if not os.path.exists(os.path.join(ckpt_path, \"layer_00-model_states.pt\")):\n",
    "        flag = \"_00-model\"\n",
    "    # print(model_dict.keys())\n",
    "    ckpt_dict = {}\n",
    "    layer0 = torch.load(\n",
    "        os.path.join(ckpt_path, f\"layer_00-model{flag}_states.pt\"),\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )\n",
    "    print(layer0.keys())\n",
    "    ckpt_dict[\"model.embed_tokens.weight\"] = layer0[\"word_embeddings.weight\"]\n",
    "\n",
    "    for l in range(0, config.num_hidden_layers):\n",
    "        l_index = str(l + 1).zfill(2)\n",
    "        layer = torch.load(\n",
    "            os.path.join(ckpt_path, f\"layer_{l_index}-model{flag}_states.pt\"),\n",
    "            map_location=torch.device(\"cpu\"),\n",
    "        )\n",
    "        for k in layer:\n",
    "            if \"dummy\" in k or \"rotary_emb\" in k:\n",
    "                continue\n",
    "            if k == \"self_attention.layernorm_qkv.query_weight\":\n",
    "                ckpt_dict[f\"model.layers.{l}.self_attn.q_proj.weight\"] = layer[k]\n",
    "            elif k == \"self_attention.layernorm_qkv.key_weight\":\n",
    "                ckpt_dict[f\"model.layers.{l}.self_attn.k_proj.weight\"] = layer[k]\n",
    "            elif k == \"self_attention.layernorm_qkv.value_weight\":\n",
    "                ckpt_dict[f\"model.layers.{l}.self_attn.v_proj.weight\"] = layer[k]\n",
    "            elif k == \"self_attention.proj.weight\":\n",
    "                ckpt_dict[f\"model.layers.{l}.self_attn.o_proj.weight\"] = layer[k]\n",
    "            elif k == \"self_attention.layernorm_qkv.layer_norm_weight\":\n",
    "                ckpt_dict[f\"model.layers.{l}.input_layernorm.weight\"] = layer[k]\n",
    "            elif k == \"layernorm_mlp.layer_norm_weight\":\n",
    "                ckpt_dict[f\"model.layers.{l}.post_attention_layernorm.weight\"] = layer[\n",
    "                    k\n",
    "                ]\n",
    "            elif k == \"self_attention.proj.weight\":\n",
    "                ckpt_dict[f\"model.layers.{l}.self_attn.o_proj.weight\"] = layer[k]\n",
    "            elif k == \"layernorm_mlp.fc2_weight\":\n",
    "                ckpt_dict[f\"model.layers.{l}.mlp.down_proj.weight\"] = layer[k]\n",
    "            elif k == \"layernorm_mlp.fc1_weight\":\n",
    "                splits = torch.split(layer[k], int(layer[k].size(0) / 2))\n",
    "                ckpt_dict[f\"model.layers.{l}.mlp.gate_proj.weight\"] = splits[0]\n",
    "                ckpt_dict[f\"model.layers.{l}.mlp.up_proj.weight\"] = splits[1]\n",
    "    layer = torch.load(\n",
    "        os.path.join(\n",
    "            ckpt_path,\n",
    "            f\"layer_{config.num_hidden_layers+1}-model{flag}_states.pt\",\n",
    "        ),\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )\n",
    "    ckpt_dict[\"model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "    layer = torch.load(\n",
    "        os.path.join(\n",
    "            ckpt_path,\n",
    "            f\"layer_{config.num_hidden_layers+2}-model{flag}_states.pt\",\n",
    "        ),\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )\n",
    "    ckpt_dict[\"lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "    model_dict.update(ckpt_dict)\n",
    "\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model1b(config_path, ckpt_path):\n",
    "    config = LlamaConfig.from_json_file(config_path)\n",
    "    model = LlamaForCausalLM(config)\n",
    "    model_dict = model.state_dict()\n",
    "    flag = \"\"\n",
    "    if not os.path.exists(os.path.join(ckpt_path, \"layer_00-model_states.pt\")):\n",
    "        flag = \"_00-model\"\n",
    "    # print(model_dict.keys())\n",
    "    ckpt_dict = {}\n",
    "    layer0 = torch.load(\n",
    "        os.path.join(ckpt_path, f\"layer_00-model{flag}_states.pt\"),\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )\n",
    "    print(layer0.keys())\n",
    "    ckpt_dict[\"model.embed_tokens.weight\"] = layer0[\"embed_tokens.weight\"]\n",
    "\n",
    "    for l in range(0, config.num_hidden_layers):\n",
    "        l_index = str(l + 1).zfill(2)\n",
    "        layer = torch.load(\n",
    "            os.path.join(ckpt_path, f\"layer_{l_index}-model{flag}_states.pt\"),\n",
    "            map_location=torch.device(\"cpu\"),\n",
    "        )\n",
    "        for k in layer:\n",
    "            if \"dummy\" in k or \"rotary_emb\" in k:\n",
    "                continue\n",
    "            ckpt_dict[f\"model.layers.{l}.{k}\"] = layer[k]\n",
    "    layer = torch.load(\n",
    "        os.path.join(\n",
    "            ckpt_path,\n",
    "            f\"layer_{config.num_hidden_layers+1}-model{flag}_states.pt\",\n",
    "        ),\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )\n",
    "    ckpt_dict[\"model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "    layer = torch.load(\n",
    "        os.path.join(\n",
    "            ckpt_path,\n",
    "            f\"layer_{config.num_hidden_layers+2}-model{flag}_states.pt\",\n",
    "        ),\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )\n",
    "    ckpt_dict[\"lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "    model_dict.update(ckpt_dict)\n",
    "\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
