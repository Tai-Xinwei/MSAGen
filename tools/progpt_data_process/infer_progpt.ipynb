{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fastdata/peiran/miniconda3/envs/sfm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-14 19:44:45,725] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[\u001b[32m2024-04-14 19:44:46.625\u001b[0m][\u001b[36mINFO\u001b[0m]: flash_attn not installed, use default attn\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import transformers\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sfm.models.progpt.progpt import ProGPTModel\n",
    "from sfm.models.progpt.progpt_config import ProGPTConfig\n",
    "from sfm.models.pfm.pfm_config import PFMConfig\n",
    "from sfm.data.sci_data.SFMDecTokenizer import SFMDecTokenizer\n",
    "from sfm.utils import arg_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_and_tokenizer():\n",
    "    parser = ArgumentParser()\n",
    "    cfg_classes = [PFMConfig, ProGPTConfig]\n",
    "    parser = arg_utils.add_dataclass_to_parser(cfg_classes, parser)\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.llm_model_name_or_path = \"/fastdata/peiran/scigpt/ckpt/stageB.prot/global_step224655\"\n",
    "    args.tokenizer_path = \"/fastdata/peiran/scigpt\"\n",
    "    args.save_dir = '/fastdata/peiran/nlm/checkpoints/stageB/global_step12386/'\n",
    "    args.load_ckpt = False\n",
    "    args.strategy = \"DDP\"\n",
    "    args.encoder_layers = 33\n",
    "    args.encoder_embed_dim = 1280\n",
    "    args.encoder_ffn_embed_dim = 5120\n",
    "    args.encoder_attention_heads = 20\n",
    "\n",
    "    tokenizer = SFMDecTokenizer.from_pretrained(\n",
    "        args.llm_model_name_or_path,\n",
    "        prot_spm_path=os.path.join(args.tokenizer_path, \"ur50bpe/bpe\"),\n",
    "        dna_spm_path=os.path.join(args.tokenizer_path, \"dnabpe/bpe\"),\n",
    "        rna_spm_path=os.path.join(args.tokenizer_path, \"rnabpe/bpe\"),\n",
    "    )\n",
    "    args.vocab_size = len(tokenizer)  # now we have new tokens\n",
    "    args.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return args, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[32m2024-04-14 19:44:50.345\u001b[0m][\u001b[36mWARNING\u001b[0m]: Duplicate config name: train_data_path, not added to parser\n",
      "[\u001b[32m2024-04-14 19:44:50.346\u001b[0m][\u001b[36mWARNING\u001b[0m]: Duplicate config name: valid_data_path, not added to parser\n",
      "[\u001b[32m2024-04-14 19:44:50.453\u001b[0m][\u001b[36mINFO\u001b[0m]: Loading protein sentencepiece model from /fastdata/peiran/scigpt/ur50bpe/bpe.model and /fastdata/peiran/scigpt/ur50bpe/bpe.vocab\n",
      "[\u001b[32m2024-04-14 19:44:50.456\u001b[0m][\u001b[36mINFO\u001b[0m]: Loading DNA sentencepiece model from /fastdata/peiran/scigpt/dnabpe/bpe.model and /fastdata/peiran/scigpt/dnabpe/bpe.vocab\n",
      "[\u001b[32m2024-04-14 19:44:50.456\u001b[0m][\u001b[36mINFO\u001b[0m]: Loading RNA sentencepiece model from /fastdata/peiran/scigpt/rnabpe/bpe.model and /fastdata/peiran/scigpt/rnabpe/bpe.vocab\n",
      "[\u001b[32m2024-04-14 19:44:50.689\u001b[0m][\u001b[36mINFO\u001b[0m]: Tokenizer has 40014 tokens\n",
      "[\u001b[32m2024-04-14 19:44:50.758\u001b[0m][\u001b[36mINFO\u001b[0m]: Trainer args: Namespace(num_classes=1, encoder_attention_heads=20, encoder_ffn_embed_dim=5120, encoder_embed_dim=1280, encoder_layers=33, num_pred_attn_layer=4, num_3d_bias_kernel=128, max_length=1024, pbc_expanded_token_cutoff=512, pbc_expanded_num_cell_per_direction=10, multi_hop_max_dist=20, droppath_prob=0.0, act_dropout=0.0, attn_dropout=0.0, dropout=0.0, sandwich_ln=True, noise_scale=0.2, mask_ratio=0.5, d_tilde=1.0, pbc_cutoff=40.0, data_path='', dataset_names='', loadcheck_path='', add_3d=False, no_2d=False, ft=False, infer=False, use_pbc=False, transformer_m_pretrain=True, mode_prob='0.6,0.2,0.2', num_timesteps=1000, ddpm_beta_start=0.0001, ddpm_beta_end=0.02, ddpm_schedule='linear', noise_mode='const', num_edges=1536, task_name='', data_basepath='', output_dim=1024, add_rope=True, flash_attn=False, stack_seq=False, num_residues=32, max_num_aa=1024, task='mae', mask_prob=0.15, train_data_path='', valid_data_path='', dataset_splits='', dataset_ratios='', pool_mode='full', embedding_length=20, model_max_length=512, smiles_dict_path='', loadbfmckpt_path='', llm_model_name_or_path='/fastdata/peiran/scigpt/ckpt/stageB.prot/global_step224655', mol_size_path='', tokenizer_path='/fastdata/peiran/scigpt', mfm_lora=False, llm_lora=False, btn_adaptor=False, fused_graphormer_llama=False, add_mol_attn_bias_in_llama=False, mol_attn_bias_in_llama_layerwise=False, path_edge_cutoff=0, skip_num_datasets='', num_data_loading_workers=16, use_global_padding=False, max_num_mol_per_sample=8, protein_max_size=1024, save_dir='/fastdata/peiran/nlm/checkpoints/stageB/global_step12386/', load_ckpt=False, strategy='DDP', vocab_size=40014, pad_token_id=32000, attention_dropout=0.0, activation_fn='gelu', encoder_normalize_before=True, apply_graphormer_init=True, share_encoder_input_output_embed=False, no_token_positional_embeddings=False, pre_layernorm=False, encoder_learned_pos=False, num_segments=2, sentence_class_num=2, sent_loss=False, apply_bert_init=False, pooler_activation_fn='tanh', atom_loss_coeff=1.0, pos_loss_coeff=1.0, y_2d_loss_coeff=1.0, max_positions=512, num_atoms=4608, num_in_degree=512, num_out_degree=512, num_spatial=512, num_edge_dis=128, edge_type='multi_hop', layerdrop=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'SFMDecTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'sfm.data.sci_data.SFMDecTokenizer.SFMDecTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "args, tokenizer = get_args_and_tokenizer()\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = ProGPTModel(args, len(tokenizer))\n",
    "# print(model.state_dict().keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_dict = {}\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer0.items():\n",
    "    new_k = \"pfm_encoder.\" + k\n",
    "    ckpt_dict[new_k] = v\n",
    "\n",
    "layer1 = torch.load(os.path.join(args.save_dir, \"layer_01-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict['decoder.model.embed_tokens.weight'] = layer1['embed_tokens.weight']\n",
    "\n",
    "layer2 = torch.load(os.path.join(args.save_dir, \"layer_02-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer2.items():\n",
    "    ckpt_dict[k] = v\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 3).zfill(2)\n",
    "    layer = torch.load(os.path.join(args.save_dir, f\"layer_{l_index}-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        ckpt_dict[f\"decoder.model.layers.{l}.{k}\"] = layer[k]\n",
    "    del layer\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_35-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_36-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "\n",
    "# model_dict.update(ckpt_dict)\n",
    "model.load_state_dict(model_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca_mfm_pp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
