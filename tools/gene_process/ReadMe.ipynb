{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  #READ ME ####\n",
    "\n",
    "# The folder contain 172 genome sequence files, named GCF_*.fna , each .fna file is a genome, may contain one or multiple sequences.\n",
    "# File GCF_000001405.40 is the human genome, make sure to include this sequence in your training.\n",
    "# For training, you can use the code below to first truncate the sequences into the length neede, for example, 8,000 basepairs as showed below. \n",
    "# Please keep all truncated sequneces from the human genome GCF_000001405.40, then down-sample the rest according to your need for training.You can perform dedup before down-sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split genome into specific length\n",
    "\n",
    "\n",
    "import os  \n",
    "from concurrent.futures import ProcessPoolExecutor  \n",
    "from Bio import SeqIO  \n",
    "  \n",
    "input_dir = \"/home/chuancao/genome/refseq_genome\"  # Replace with the path to your .fna files  \n",
    "output_file = \"/home/chuancao/dna/refseq_genome_8k.fasta\"  # Replace with the path to your output .fasta file  \n",
    "chunk_size = 8000  # Size of the sequence chunks  \n",
    "num_workers = 64  # Number of worker processes  \n",
    "  \n",
    "def process_file(file_path):  \n",
    "    # This function will be executed by each worker process  \n",
    "    chunks = []  \n",
    "    with open(file_path, \"r\") as handle:  \n",
    "        for record in SeqIO.parse(handle, \"fasta\"):  \n",
    "            seq = str(record.seq)  \n",
    "            for i in range(0, len(seq), chunk_size):  \n",
    "                chunk_seq = seq[i:i + chunk_size]  \n",
    "                # Make sure we don't get sequences smaller than the chunk size  \n",
    "                if len(chunk_seq) == chunk_size:  \n",
    "                    # Create a new SeqRecord  \n",
    "                    chunk_record = SeqIO.SeqRecord(Seq(chunk_seq), id=f\"{record.id}_{i//chunk_size}\", description=\"\")  \n",
    "                    chunks.append(chunk_record)  \n",
    "    return chunks  \n",
    "  \n",
    "def save_chunks_to_file(chunks, output_file):  \n",
    "    with open(output_file, \"a\") as output_handle:  # Open file in append mode  \n",
    "        for chunk in chunks:  \n",
    "            SeqIO.write(chunk, output_handle, \"fasta\")  \n",
    "  \n",
    "# List all .fna files in the input directory  \n",
    "fna_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('.fna')]  \n",
    "  \n",
    "# Process the files in parallel using ProcessPoolExecutor  \n",
    "with ProcessPoolExecutor(max_workers=num_workers) as executor:  \n",
    "    # Use a dictionary to keep track of running tasks  \n",
    "    future_to_file = {executor.submit(process_file, fna_file): fna_file for fna_file in fna_files}  \n",
    "      \n",
    "    # As tasks complete, save the resulting chunks to the output file  \n",
    "    for future in concurrent.futures.as_completed(future_to_file):  \n",
    "        chunks = future.result()  \n",
    "        save_chunks_to_file(chunks, output_file)  \n",
    "  \n",
    "print(\"Splitting complete.\")  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
