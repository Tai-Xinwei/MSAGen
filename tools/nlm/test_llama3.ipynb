{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import mmap\n",
    "import re\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import transformers\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sfm.models.scigpt.scigpt import ScigptModel\n",
    "from sfm.models.scigpt.config import ScigptConfig\n",
    "from sfm.utils import arg_utils\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import multiprocessing as mp\n",
    "from sfm.utils.science_tokens import SCIENCE_TAG_TOKENS, SCIENCE_TOKENS\n",
    "\n",
    "from sfm.logging import logger\n",
    "\n",
    "import struct\n",
    "from multiprocessing import Lock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def get_args_and_tokenizer(use_llama=False):\n",
    "    parser = ArgumentParser()\n",
    "    cfg_classes = [ScigptConfig]\n",
    "    parser = arg_utils.add_dataclass_to_parser(cfg_classes, parser)\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.load_ckpt = False\n",
    "    args.strategy = \"DDP\"\n",
    "    args.encoder_layers = 33\n",
    "    args.encoder_embed_dim = 1280\n",
    "    args.encoder_ffn_embed_dim = 5120\n",
    "    args.encoder_attention_heads = 20\n",
    "    args.infer = True\n",
    "    args.bf16 = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('/data/peiran/blob/hai1data/sfm/llama/Meta-Llama-3-8B/original')\n",
    "    # args.save_dir = \"/home/v-zekunguo/hai1data/nlm/output/llama3_stageB/global_step1600/\"\n",
    "    args.save_dir = '/data/peiran/blob/hai1data/sfm/llama/Meta-Llama-3-8B/original'\n",
    "    args.llm_model_name_or_path = '/data/peiran/blob/hai1data/sfm/llama/Meta-Llama-3-8B/original'\n",
    "\n",
    "    special_tokens_dict = dict()\n",
    "    if tokenizer.pad_token is None:\n",
    "        special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "    if tokenizer.eos_token is None:\n",
    "        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "    if tokenizer.bos_token is None:\n",
    "        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "    if tokenizer.unk_token is None:\n",
    "        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "    # special_tokens_dict[\"additional_special_tokens\"] = SCIENCE_TAG_TOKENS\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    tokenizer.tag_re = re.compile(f'{\"|\".join(SCIENCE_TAG_TOKENS)}')\n",
    "    tokenizer.smiles_re = re.compile(\n",
    "        \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    )\n",
    "\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"pad_token\": \"[PAD]\",\n",
    "            \"unk_token\":\"<unk>\",\n",
    "        },\n",
    "\n",
    "    )\n",
    "\n",
    "    tokenizer.add_tokens(SCIENCE_TAG_TOKENS)\n",
    "    tokenizer.add_tokens(SCIENCE_TOKENS)\n",
    "    extra_tokens = []\n",
    "    # protein\n",
    "    for i in range(26):\n",
    "        extra_tokens.append(f\"<a>{chr(65 + i)}\")\n",
    "\n",
    "    # DNA, RNA, including ambiguous bases\n",
    "    for c in \"ACTGURYSWKMBDHVN\":\n",
    "        extra_tokens.append(f\"<d>{c}\")\n",
    "        extra_tokens.append(f\"<r>{c}\")\n",
    "\n",
    "    # materials, non-elements\n",
    "    for c in \"0123456789()+-\":\n",
    "        extra_tokens.append(f\"<i>{c}\")\n",
    "    for i in range(26):\n",
    "        extra_tokens.append(f\"<i>{chr(65 + i)}\")\n",
    "        extra_tokens.append(f\"<i>{chr(97 + i)}\")\n",
    "\n",
    "    tokenizer.add_tokens(extra_tokens)\n",
    "    tokenizer.split_special_tokens = True  # Ensure _tokenize() can access special tokens\n",
    "\n",
    "    logger.info(f\"Tokenizer has {len(tokenizer)} tokens\")\n",
    "\n",
    "    args.vocab_size=len(tokenizer)\n",
    "\n",
    "    return args, tokenizer\n",
    "\n",
    "args, tokenizer = get_args_and_tokenizer()\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.vocab_size=130304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the extended trained model\n",
    "ckpt_dict = {}\n",
    "\n",
    "model = ScigptModel(args)\n",
    "model.decoder.resize_token_embeddings(args.vocab_size)\n",
    "model_dict = model.state_dict()\n",
    "print(f\"model_dict: {model_dict.keys()}\")\n",
    "print(model_dict['decoder.model.layers.0.mlp.gate_proj.weight'].shape)\n",
    "print(model_dict['decoder.model.layers.0.mlp.up_proj.weight'].shape)\n",
    "weight1_size=model_dict['decoder.model.layers.0.mlp.gate_proj.weight'].size(0)\n",
    "weight2_size=model_dict['decoder.model.layers.0.mlp.up_proj.weight'].size(0)\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_00-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer0.items():\n",
    "    if k=='word_embeddings.weight':\n",
    "        ckpt_dict['decoder.model.embed_tokens.weight'] = v\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 1).zfill(2)\n",
    "    layer = torch.load(os.path.join(args.save_dir, f\"layer_{l_index}-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        if k==\"self_attention.layernorm_qkv.layer_norm_weight\":\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.input_layernorm.weight\"] = layer[k]\n",
    "        elif k=='self_attention.layernorm_qkv.query_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.self_attn.q_proj.weight\"] = layer[k]\n",
    "        elif k=='self_attention.layernorm_qkv.key_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.self_attn.k_proj.weight\"] = layer[k]\n",
    "        elif k=='self_attention.layernorm_qkv.value_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.self_attn.v_proj.weight\"] = layer[k]\n",
    "        elif k=='self_attention.proj.weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.self_attn.o_proj.weight\"] = layer[k]\n",
    "        elif k=='layernorm_mlp.layer_norm_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.post_attention_layernorm.weight\"] = layer[k]\n",
    "        elif k=='layernorm_mlp.fc1_weight':\n",
    "            weight1,weight2=torch.split(layer[k], [weight1_size, weight2_size], dim=0)\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.mlp.gate_proj.weight\"] = weight1\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.mlp.up_proj.weight\"] = weight2\n",
    "        elif k=='layernorm_mlp.fc2_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.mlp.down_proj.weight\"] = layer[k]\n",
    "    del layer\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_33-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_34-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "\n",
    "print(f\"ckpt_dict: {ckpt_dict.keys()}\")\n",
    "model_dict.update(ckpt_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dict = {}\n",
    "# Load the original llama3 model\n",
    "model = ScigptModel(args)\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "print(f\"model_dict: {model_dict.keys()}\")\n",
    "\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer0.items():\n",
    "    new_k = \"decoder.model.\" + k\n",
    "    ckpt_dict[new_k] = v\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 1).zfill(2)\n",
    "    layer = torch.load(os.path.join(args.save_dir, f\"layer_{l_index}-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        ckpt_dict[f\"decoder.model.layers.{l}.{k}\"] = layer[k]\n",
    "    del layer\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_33-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_34-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "\n",
    "print(f\"ckpt_dict: {ckpt_dict.keys()}\")\n",
    "model_dict.update(ckpt_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "from sfm.data.prot_data.util import bstr2obj\n",
    "# load data\n",
    "file_path='/data/peiran/blob/hai1data/sfm/nlm/llama3_300B/valid_lmdb/valid.patent.v2.txt.lmdb'\n",
    "env = lmdb.open(\n",
    "    file_path, subdir=True, readonly=True, lock=False, readahead=False\n",
    ")\n",
    "txn = env.begin(write=False)\n",
    "\n",
    "print(env.stat())\n",
    "count=0\n",
    "metadata = bstr2obj(txn.get(\"metadata\".encode()))\n",
    "cur_len, cur_keys = metadata[\"size\"], metadata[\"keys\"]\n",
    "print(cur_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Calculate loss\n",
    "print(metadata.keys())\n",
    "loss_list=[]\n",
    "print(metadata['processed_seq_len'])\n",
    "for key in cur_keys:\n",
    "    value = txn.get(str(key).encode())\n",
    "    input_ids = np.frombuffer(value, dtype=np.uint32)\n",
    "    input_tensor = torch.from_numpy(input_ids.astype(np.int64)).unsqueeze(0).to(device)\n",
    "    labels = input_tensor.clone()\n",
    "    out = model.decoder(input_tensor, labels=labels)\n",
    "    input_tensor.to(\"cpu\")\n",
    "    labels.to(\"cpu\")\n",
    "    print(out.loss.cpu().item())\n",
    "    loss_list.append(out.loss.cpu().item())\n",
    "    out = None\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    del out\n",
    "print(sum(loss_list) / len(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(input_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"Football is a \", return_tensors=\"pt\")\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.decoder.generate(\n",
    "    input_ids=torch.tensor(input_tensor).to(device),\n",
    "    num_beams=5,\n",
    "    max_new_tokens=512,\n",
    "    num_return_sequences=1,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "res = tokenizer.decode(output.sequences[0], skip_special_tokens=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=torch.tensor(tokenizer.encode(\"Football is a \", return_tensors=\"pt\")).to(device)\n",
    "labels = input_ids.clone()\n",
    "out = model.decoder(input_ids,labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.decoder.generate(\n",
    "    input_ids=torch.tensor(tokenizer.encode(\"Football is a \", return_tensors=\"pt\")).to(device),\n",
    "    num_beams=5,\n",
    "    max_new_tokens=512,\n",
    "    num_return_sequences=1,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "res = tokenizer.decode(output.sequences[0], skip_special_tokens=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
