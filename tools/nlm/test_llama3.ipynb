{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-06 05:51:21,973] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[\u001b[32m2024-06-06 05:51:23.494\u001b[0m][\u001b[36mINFO\u001b[0m]: apex is installed, using FusedAdam with fp16 optimizer states\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import mmap\n",
    "import re\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import transformers\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sfm.models.scigpt.scigpt import ScigptModel\n",
    "from sfm.models.scigpt.config import ScigptConfig\n",
    "from sfm.utils import arg_utils\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import multiprocessing as mp\n",
    "from sfm.utils.science_tokens import SCIENCE_TAG_TOKENS, SCIENCE_TOKENS\n",
    "\n",
    "from sfm.logging import logger\n",
    "\n",
    "import struct\n",
    "from multiprocessing import Lock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def get_args_and_tokenizer(use_llama=False):\n",
    "    parser = ArgumentParser()\n",
    "    cfg_classes = [ScigptConfig]\n",
    "    parser = arg_utils.add_dataclass_to_parser(cfg_classes, parser)\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.load_ckpt = False\n",
    "    args.strategy = \"DDP\"\n",
    "    args.encoder_layers = 33\n",
    "    args.encoder_embed_dim = 1280\n",
    "    args.encoder_ffn_embed_dim = 5120\n",
    "    args.encoder_attention_heads = 20\n",
    "    args.infer = True\n",
    "    args.bf16 = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('/home/v-zekunguo/sfm/llama/Meta-Llama-3-8B/original')\n",
    "    # args.save_dir = \"/home/v-zekunguo/hai1data/nlm/output/llama3_stageB/global_step1600/\"\n",
    "    args.save_dir = '/home/v-zekunguo/nlm/peiran/output/llama3_stageB_G256/global_step8572/'\n",
    "    args.llm_model_name_or_path = '/home/v-zekunguo/sfm/llama/Meta-Llama-3-8B/original'\n",
    "\n",
    "    special_tokens_dict = dict()\n",
    "    if tokenizer.pad_token is None:\n",
    "        special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "    if tokenizer.eos_token is None:\n",
    "        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "    if tokenizer.bos_token is None:\n",
    "        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "    if tokenizer.unk_token is None:\n",
    "        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "    # special_tokens_dict[\"additional_special_tokens\"] = SCIENCE_TAG_TOKENS\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    tokenizer.tag_re = re.compile(f'{\"|\".join(SCIENCE_TAG_TOKENS)}')\n",
    "    tokenizer.smiles_re = re.compile(\n",
    "        \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    )\n",
    "\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"pad_token\": \"[PAD]\",\n",
    "            \"unk_token\":\"<unk>\",\n",
    "        },\n",
    "\n",
    "    )\n",
    "\n",
    "    tokenizer.add_tokens(SCIENCE_TAG_TOKENS)\n",
    "    tokenizer.add_tokens(SCIENCE_TOKENS)\n",
    "    extra_tokens = []\n",
    "    # protein\n",
    "    for i in range(26):\n",
    "        extra_tokens.append(f\"<a>{chr(65 + i)}\")\n",
    "\n",
    "    # DNA, RNA, including ambiguous bases\n",
    "    for c in \"ACTGURYSWKMBDHVN\":\n",
    "        extra_tokens.append(f\"<d>{c}\")\n",
    "        extra_tokens.append(f\"<r>{c}\")\n",
    "\n",
    "    # materials, non-elements\n",
    "    for c in \"0123456789()+-\":\n",
    "        extra_tokens.append(f\"<i>{c}\")\n",
    "    for i in range(26):\n",
    "        extra_tokens.append(f\"<i>{chr(65 + i)}\")\n",
    "        extra_tokens.append(f\"<i>{chr(97 + i)}\")\n",
    "\n",
    "    tokenizer.add_tokens(extra_tokens)\n",
    "    tokenizer.split_special_tokens = True  # Ensure _tokenize() can access special tokens\n",
    "\n",
    "    logger.info(f\"Tokenizer has {len(tokenizer)} tokens\")\n",
    "\n",
    "    args.vocab_size=len(tokenizer)\n",
    "\n",
    "    return args, tokenizer\n",
    "\n",
    "args, tokenizer = get_args_and_tokenizer()\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.vocab_size=130304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128264, 1950, 35, 524, 4490, 3675, 29]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<protein>ASD</protien>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dict: odict_keys(['decoder.model.embed_tokens.weight', 'decoder.model.layers.0.self_attn.q_proj.weight', 'decoder.model.layers.0.self_attn.k_proj.weight', 'decoder.model.layers.0.self_attn.v_proj.weight', 'decoder.model.layers.0.self_attn.o_proj.weight', 'decoder.model.layers.0.mlp.gate_proj.weight', 'decoder.model.layers.0.mlp.up_proj.weight', 'decoder.model.layers.0.mlp.down_proj.weight', 'decoder.model.layers.0.input_layernorm.weight', 'decoder.model.layers.0.post_attention_layernorm.weight', 'decoder.model.layers.1.self_attn.q_proj.weight', 'decoder.model.layers.1.self_attn.k_proj.weight', 'decoder.model.layers.1.self_attn.v_proj.weight', 'decoder.model.layers.1.self_attn.o_proj.weight', 'decoder.model.layers.1.mlp.gate_proj.weight', 'decoder.model.layers.1.mlp.up_proj.weight', 'decoder.model.layers.1.mlp.down_proj.weight', 'decoder.model.layers.1.input_layernorm.weight', 'decoder.model.layers.1.post_attention_layernorm.weight', 'decoder.model.layers.2.self_attn.q_proj.weight', 'decoder.model.layers.2.self_attn.k_proj.weight', 'decoder.model.layers.2.self_attn.v_proj.weight', 'decoder.model.layers.2.self_attn.o_proj.weight', 'decoder.model.layers.2.mlp.gate_proj.weight', 'decoder.model.layers.2.mlp.up_proj.weight', 'decoder.model.layers.2.mlp.down_proj.weight', 'decoder.model.layers.2.input_layernorm.weight', 'decoder.model.layers.2.post_attention_layernorm.weight', 'decoder.model.layers.3.self_attn.q_proj.weight', 'decoder.model.layers.3.self_attn.k_proj.weight', 'decoder.model.layers.3.self_attn.v_proj.weight', 'decoder.model.layers.3.self_attn.o_proj.weight', 'decoder.model.layers.3.mlp.gate_proj.weight', 'decoder.model.layers.3.mlp.up_proj.weight', 'decoder.model.layers.3.mlp.down_proj.weight', 'decoder.model.layers.3.input_layernorm.weight', 'decoder.model.layers.3.post_attention_layernorm.weight', 'decoder.model.layers.4.self_attn.q_proj.weight', 'decoder.model.layers.4.self_attn.k_proj.weight', 'decoder.model.layers.4.self_attn.v_proj.weight', 'decoder.model.layers.4.self_attn.o_proj.weight', 'decoder.model.layers.4.mlp.gate_proj.weight', 'decoder.model.layers.4.mlp.up_proj.weight', 'decoder.model.layers.4.mlp.down_proj.weight', 'decoder.model.layers.4.input_layernorm.weight', 'decoder.model.layers.4.post_attention_layernorm.weight', 'decoder.model.layers.5.self_attn.q_proj.weight', 'decoder.model.layers.5.self_attn.k_proj.weight', 'decoder.model.layers.5.self_attn.v_proj.weight', 'decoder.model.layers.5.self_attn.o_proj.weight', 'decoder.model.layers.5.mlp.gate_proj.weight', 'decoder.model.layers.5.mlp.up_proj.weight', 'decoder.model.layers.5.mlp.down_proj.weight', 'decoder.model.layers.5.input_layernorm.weight', 'decoder.model.layers.5.post_attention_layernorm.weight', 'decoder.model.layers.6.self_attn.q_proj.weight', 'decoder.model.layers.6.self_attn.k_proj.weight', 'decoder.model.layers.6.self_attn.v_proj.weight', 'decoder.model.layers.6.self_attn.o_proj.weight', 'decoder.model.layers.6.mlp.gate_proj.weight', 'decoder.model.layers.6.mlp.up_proj.weight', 'decoder.model.layers.6.mlp.down_proj.weight', 'decoder.model.layers.6.input_layernorm.weight', 'decoder.model.layers.6.post_attention_layernorm.weight', 'decoder.model.layers.7.self_attn.q_proj.weight', 'decoder.model.layers.7.self_attn.k_proj.weight', 'decoder.model.layers.7.self_attn.v_proj.weight', 'decoder.model.layers.7.self_attn.o_proj.weight', 'decoder.model.layers.7.mlp.gate_proj.weight', 'decoder.model.layers.7.mlp.up_proj.weight', 'decoder.model.layers.7.mlp.down_proj.weight', 'decoder.model.layers.7.input_layernorm.weight', 'decoder.model.layers.7.post_attention_layernorm.weight', 'decoder.model.layers.8.self_attn.q_proj.weight', 'decoder.model.layers.8.self_attn.k_proj.weight', 'decoder.model.layers.8.self_attn.v_proj.weight', 'decoder.model.layers.8.self_attn.o_proj.weight', 'decoder.model.layers.8.mlp.gate_proj.weight', 'decoder.model.layers.8.mlp.up_proj.weight', 'decoder.model.layers.8.mlp.down_proj.weight', 'decoder.model.layers.8.input_layernorm.weight', 'decoder.model.layers.8.post_attention_layernorm.weight', 'decoder.model.layers.9.self_attn.q_proj.weight', 'decoder.model.layers.9.self_attn.k_proj.weight', 'decoder.model.layers.9.self_attn.v_proj.weight', 'decoder.model.layers.9.self_attn.o_proj.weight', 'decoder.model.layers.9.mlp.gate_proj.weight', 'decoder.model.layers.9.mlp.up_proj.weight', 'decoder.model.layers.9.mlp.down_proj.weight', 'decoder.model.layers.9.input_layernorm.weight', 'decoder.model.layers.9.post_attention_layernorm.weight', 'decoder.model.layers.10.self_attn.q_proj.weight', 'decoder.model.layers.10.self_attn.k_proj.weight', 'decoder.model.layers.10.self_attn.v_proj.weight', 'decoder.model.layers.10.self_attn.o_proj.weight', 'decoder.model.layers.10.mlp.gate_proj.weight', 'decoder.model.layers.10.mlp.up_proj.weight', 'decoder.model.layers.10.mlp.down_proj.weight', 'decoder.model.layers.10.input_layernorm.weight', 'decoder.model.layers.10.post_attention_layernorm.weight', 'decoder.model.layers.11.self_attn.q_proj.weight', 'decoder.model.layers.11.self_attn.k_proj.weight', 'decoder.model.layers.11.self_attn.v_proj.weight', 'decoder.model.layers.11.self_attn.o_proj.weight', 'decoder.model.layers.11.mlp.gate_proj.weight', 'decoder.model.layers.11.mlp.up_proj.weight', 'decoder.model.layers.11.mlp.down_proj.weight', 'decoder.model.layers.11.input_layernorm.weight', 'decoder.model.layers.11.post_attention_layernorm.weight', 'decoder.model.layers.12.self_attn.q_proj.weight', 'decoder.model.layers.12.self_attn.k_proj.weight', 'decoder.model.layers.12.self_attn.v_proj.weight', 'decoder.model.layers.12.self_attn.o_proj.weight', 'decoder.model.layers.12.mlp.gate_proj.weight', 'decoder.model.layers.12.mlp.up_proj.weight', 'decoder.model.layers.12.mlp.down_proj.weight', 'decoder.model.layers.12.input_layernorm.weight', 'decoder.model.layers.12.post_attention_layernorm.weight', 'decoder.model.layers.13.self_attn.q_proj.weight', 'decoder.model.layers.13.self_attn.k_proj.weight', 'decoder.model.layers.13.self_attn.v_proj.weight', 'decoder.model.layers.13.self_attn.o_proj.weight', 'decoder.model.layers.13.mlp.gate_proj.weight', 'decoder.model.layers.13.mlp.up_proj.weight', 'decoder.model.layers.13.mlp.down_proj.weight', 'decoder.model.layers.13.input_layernorm.weight', 'decoder.model.layers.13.post_attention_layernorm.weight', 'decoder.model.layers.14.self_attn.q_proj.weight', 'decoder.model.layers.14.self_attn.k_proj.weight', 'decoder.model.layers.14.self_attn.v_proj.weight', 'decoder.model.layers.14.self_attn.o_proj.weight', 'decoder.model.layers.14.mlp.gate_proj.weight', 'decoder.model.layers.14.mlp.up_proj.weight', 'decoder.model.layers.14.mlp.down_proj.weight', 'decoder.model.layers.14.input_layernorm.weight', 'decoder.model.layers.14.post_attention_layernorm.weight', 'decoder.model.layers.15.self_attn.q_proj.weight', 'decoder.model.layers.15.self_attn.k_proj.weight', 'decoder.model.layers.15.self_attn.v_proj.weight', 'decoder.model.layers.15.self_attn.o_proj.weight', 'decoder.model.layers.15.mlp.gate_proj.weight', 'decoder.model.layers.15.mlp.up_proj.weight', 'decoder.model.layers.15.mlp.down_proj.weight', 'decoder.model.layers.15.input_layernorm.weight', 'decoder.model.layers.15.post_attention_layernorm.weight', 'decoder.model.layers.16.self_attn.q_proj.weight', 'decoder.model.layers.16.self_attn.k_proj.weight', 'decoder.model.layers.16.self_attn.v_proj.weight', 'decoder.model.layers.16.self_attn.o_proj.weight', 'decoder.model.layers.16.mlp.gate_proj.weight', 'decoder.model.layers.16.mlp.up_proj.weight', 'decoder.model.layers.16.mlp.down_proj.weight', 'decoder.model.layers.16.input_layernorm.weight', 'decoder.model.layers.16.post_attention_layernorm.weight', 'decoder.model.layers.17.self_attn.q_proj.weight', 'decoder.model.layers.17.self_attn.k_proj.weight', 'decoder.model.layers.17.self_attn.v_proj.weight', 'decoder.model.layers.17.self_attn.o_proj.weight', 'decoder.model.layers.17.mlp.gate_proj.weight', 'decoder.model.layers.17.mlp.up_proj.weight', 'decoder.model.layers.17.mlp.down_proj.weight', 'decoder.model.layers.17.input_layernorm.weight', 'decoder.model.layers.17.post_attention_layernorm.weight', 'decoder.model.layers.18.self_attn.q_proj.weight', 'decoder.model.layers.18.self_attn.k_proj.weight', 'decoder.model.layers.18.self_attn.v_proj.weight', 'decoder.model.layers.18.self_attn.o_proj.weight', 'decoder.model.layers.18.mlp.gate_proj.weight', 'decoder.model.layers.18.mlp.up_proj.weight', 'decoder.model.layers.18.mlp.down_proj.weight', 'decoder.model.layers.18.input_layernorm.weight', 'decoder.model.layers.18.post_attention_layernorm.weight', 'decoder.model.layers.19.self_attn.q_proj.weight', 'decoder.model.layers.19.self_attn.k_proj.weight', 'decoder.model.layers.19.self_attn.v_proj.weight', 'decoder.model.layers.19.self_attn.o_proj.weight', 'decoder.model.layers.19.mlp.gate_proj.weight', 'decoder.model.layers.19.mlp.up_proj.weight', 'decoder.model.layers.19.mlp.down_proj.weight', 'decoder.model.layers.19.input_layernorm.weight', 'decoder.model.layers.19.post_attention_layernorm.weight', 'decoder.model.layers.20.self_attn.q_proj.weight', 'decoder.model.layers.20.self_attn.k_proj.weight', 'decoder.model.layers.20.self_attn.v_proj.weight', 'decoder.model.layers.20.self_attn.o_proj.weight', 'decoder.model.layers.20.mlp.gate_proj.weight', 'decoder.model.layers.20.mlp.up_proj.weight', 'decoder.model.layers.20.mlp.down_proj.weight', 'decoder.model.layers.20.input_layernorm.weight', 'decoder.model.layers.20.post_attention_layernorm.weight', 'decoder.model.layers.21.self_attn.q_proj.weight', 'decoder.model.layers.21.self_attn.k_proj.weight', 'decoder.model.layers.21.self_attn.v_proj.weight', 'decoder.model.layers.21.self_attn.o_proj.weight', 'decoder.model.layers.21.mlp.gate_proj.weight', 'decoder.model.layers.21.mlp.up_proj.weight', 'decoder.model.layers.21.mlp.down_proj.weight', 'decoder.model.layers.21.input_layernorm.weight', 'decoder.model.layers.21.post_attention_layernorm.weight', 'decoder.model.layers.22.self_attn.q_proj.weight', 'decoder.model.layers.22.self_attn.k_proj.weight', 'decoder.model.layers.22.self_attn.v_proj.weight', 'decoder.model.layers.22.self_attn.o_proj.weight', 'decoder.model.layers.22.mlp.gate_proj.weight', 'decoder.model.layers.22.mlp.up_proj.weight', 'decoder.model.layers.22.mlp.down_proj.weight', 'decoder.model.layers.22.input_layernorm.weight', 'decoder.model.layers.22.post_attention_layernorm.weight', 'decoder.model.layers.23.self_attn.q_proj.weight', 'decoder.model.layers.23.self_attn.k_proj.weight', 'decoder.model.layers.23.self_attn.v_proj.weight', 'decoder.model.layers.23.self_attn.o_proj.weight', 'decoder.model.layers.23.mlp.gate_proj.weight', 'decoder.model.layers.23.mlp.up_proj.weight', 'decoder.model.layers.23.mlp.down_proj.weight', 'decoder.model.layers.23.input_layernorm.weight', 'decoder.model.layers.23.post_attention_layernorm.weight', 'decoder.model.layers.24.self_attn.q_proj.weight', 'decoder.model.layers.24.self_attn.k_proj.weight', 'decoder.model.layers.24.self_attn.v_proj.weight', 'decoder.model.layers.24.self_attn.o_proj.weight', 'decoder.model.layers.24.mlp.gate_proj.weight', 'decoder.model.layers.24.mlp.up_proj.weight', 'decoder.model.layers.24.mlp.down_proj.weight', 'decoder.model.layers.24.input_layernorm.weight', 'decoder.model.layers.24.post_attention_layernorm.weight', 'decoder.model.layers.25.self_attn.q_proj.weight', 'decoder.model.layers.25.self_attn.k_proj.weight', 'decoder.model.layers.25.self_attn.v_proj.weight', 'decoder.model.layers.25.self_attn.o_proj.weight', 'decoder.model.layers.25.mlp.gate_proj.weight', 'decoder.model.layers.25.mlp.up_proj.weight', 'decoder.model.layers.25.mlp.down_proj.weight', 'decoder.model.layers.25.input_layernorm.weight', 'decoder.model.layers.25.post_attention_layernorm.weight', 'decoder.model.layers.26.self_attn.q_proj.weight', 'decoder.model.layers.26.self_attn.k_proj.weight', 'decoder.model.layers.26.self_attn.v_proj.weight', 'decoder.model.layers.26.self_attn.o_proj.weight', 'decoder.model.layers.26.mlp.gate_proj.weight', 'decoder.model.layers.26.mlp.up_proj.weight', 'decoder.model.layers.26.mlp.down_proj.weight', 'decoder.model.layers.26.input_layernorm.weight', 'decoder.model.layers.26.post_attention_layernorm.weight', 'decoder.model.layers.27.self_attn.q_proj.weight', 'decoder.model.layers.27.self_attn.k_proj.weight', 'decoder.model.layers.27.self_attn.v_proj.weight', 'decoder.model.layers.27.self_attn.o_proj.weight', 'decoder.model.layers.27.mlp.gate_proj.weight', 'decoder.model.layers.27.mlp.up_proj.weight', 'decoder.model.layers.27.mlp.down_proj.weight', 'decoder.model.layers.27.input_layernorm.weight', 'decoder.model.layers.27.post_attention_layernorm.weight', 'decoder.model.layers.28.self_attn.q_proj.weight', 'decoder.model.layers.28.self_attn.k_proj.weight', 'decoder.model.layers.28.self_attn.v_proj.weight', 'decoder.model.layers.28.self_attn.o_proj.weight', 'decoder.model.layers.28.mlp.gate_proj.weight', 'decoder.model.layers.28.mlp.up_proj.weight', 'decoder.model.layers.28.mlp.down_proj.weight', 'decoder.model.layers.28.input_layernorm.weight', 'decoder.model.layers.28.post_attention_layernorm.weight', 'decoder.model.layers.29.self_attn.q_proj.weight', 'decoder.model.layers.29.self_attn.k_proj.weight', 'decoder.model.layers.29.self_attn.v_proj.weight', 'decoder.model.layers.29.self_attn.o_proj.weight', 'decoder.model.layers.29.mlp.gate_proj.weight', 'decoder.model.layers.29.mlp.up_proj.weight', 'decoder.model.layers.29.mlp.down_proj.weight', 'decoder.model.layers.29.input_layernorm.weight', 'decoder.model.layers.29.post_attention_layernorm.weight', 'decoder.model.layers.30.self_attn.q_proj.weight', 'decoder.model.layers.30.self_attn.k_proj.weight', 'decoder.model.layers.30.self_attn.v_proj.weight', 'decoder.model.layers.30.self_attn.o_proj.weight', 'decoder.model.layers.30.mlp.gate_proj.weight', 'decoder.model.layers.30.mlp.up_proj.weight', 'decoder.model.layers.30.mlp.down_proj.weight', 'decoder.model.layers.30.input_layernorm.weight', 'decoder.model.layers.30.post_attention_layernorm.weight', 'decoder.model.layers.31.self_attn.q_proj.weight', 'decoder.model.layers.31.self_attn.k_proj.weight', 'decoder.model.layers.31.self_attn.v_proj.weight', 'decoder.model.layers.31.self_attn.o_proj.weight', 'decoder.model.layers.31.mlp.gate_proj.weight', 'decoder.model.layers.31.mlp.up_proj.weight', 'decoder.model.layers.31.mlp.down_proj.weight', 'decoder.model.layers.31.input_layernorm.weight', 'decoder.model.layers.31.post_attention_layernorm.weight', 'decoder.model.norm.weight', 'decoder.lm_head.weight'])\n",
      "torch.Size([14336, 4096])\n",
      "torch.Size([14336, 4096])\n",
      "ckpt_dict: dict_keys(['decoder.model.embed_tokens.weight', 'decoder.model.layers.0.input_layernorm.weight', 'decoder.model.layers.0.self_attn.q_proj.weight', 'decoder.model.layers.0.self_attn.k_proj.weight', 'decoder.model.layers.0.self_attn.v_proj.weight', 'decoder.model.layers.0.self_attn.o_proj.weight', 'decoder.model.layers.0.post_attention_layernorm.weight', 'decoder.model.layers.0.mlp.gate_proj.weight', 'decoder.model.layers.0.mlp.up_proj.weight', 'decoder.model.layers.0.mlp.down_proj.weight', 'decoder.model.layers.1.input_layernorm.weight', 'decoder.model.layers.1.self_attn.q_proj.weight', 'decoder.model.layers.1.self_attn.k_proj.weight', 'decoder.model.layers.1.self_attn.v_proj.weight', 'decoder.model.layers.1.self_attn.o_proj.weight', 'decoder.model.layers.1.post_attention_layernorm.weight', 'decoder.model.layers.1.mlp.gate_proj.weight', 'decoder.model.layers.1.mlp.up_proj.weight', 'decoder.model.layers.1.mlp.down_proj.weight', 'decoder.model.layers.2.input_layernorm.weight', 'decoder.model.layers.2.self_attn.q_proj.weight', 'decoder.model.layers.2.self_attn.k_proj.weight', 'decoder.model.layers.2.self_attn.v_proj.weight', 'decoder.model.layers.2.self_attn.o_proj.weight', 'decoder.model.layers.2.post_attention_layernorm.weight', 'decoder.model.layers.2.mlp.gate_proj.weight', 'decoder.model.layers.2.mlp.up_proj.weight', 'decoder.model.layers.2.mlp.down_proj.weight', 'decoder.model.layers.3.input_layernorm.weight', 'decoder.model.layers.3.self_attn.q_proj.weight', 'decoder.model.layers.3.self_attn.k_proj.weight', 'decoder.model.layers.3.self_attn.v_proj.weight', 'decoder.model.layers.3.self_attn.o_proj.weight', 'decoder.model.layers.3.post_attention_layernorm.weight', 'decoder.model.layers.3.mlp.gate_proj.weight', 'decoder.model.layers.3.mlp.up_proj.weight', 'decoder.model.layers.3.mlp.down_proj.weight', 'decoder.model.layers.4.input_layernorm.weight', 'decoder.model.layers.4.self_attn.q_proj.weight', 'decoder.model.layers.4.self_attn.k_proj.weight', 'decoder.model.layers.4.self_attn.v_proj.weight', 'decoder.model.layers.4.self_attn.o_proj.weight', 'decoder.model.layers.4.post_attention_layernorm.weight', 'decoder.model.layers.4.mlp.gate_proj.weight', 'decoder.model.layers.4.mlp.up_proj.weight', 'decoder.model.layers.4.mlp.down_proj.weight', 'decoder.model.layers.5.input_layernorm.weight', 'decoder.model.layers.5.self_attn.q_proj.weight', 'decoder.model.layers.5.self_attn.k_proj.weight', 'decoder.model.layers.5.self_attn.v_proj.weight', 'decoder.model.layers.5.self_attn.o_proj.weight', 'decoder.model.layers.5.post_attention_layernorm.weight', 'decoder.model.layers.5.mlp.gate_proj.weight', 'decoder.model.layers.5.mlp.up_proj.weight', 'decoder.model.layers.5.mlp.down_proj.weight', 'decoder.model.layers.6.input_layernorm.weight', 'decoder.model.layers.6.self_attn.q_proj.weight', 'decoder.model.layers.6.self_attn.k_proj.weight', 'decoder.model.layers.6.self_attn.v_proj.weight', 'decoder.model.layers.6.self_attn.o_proj.weight', 'decoder.model.layers.6.post_attention_layernorm.weight', 'decoder.model.layers.6.mlp.gate_proj.weight', 'decoder.model.layers.6.mlp.up_proj.weight', 'decoder.model.layers.6.mlp.down_proj.weight', 'decoder.model.layers.7.input_layernorm.weight', 'decoder.model.layers.7.self_attn.q_proj.weight', 'decoder.model.layers.7.self_attn.k_proj.weight', 'decoder.model.layers.7.self_attn.v_proj.weight', 'decoder.model.layers.7.self_attn.o_proj.weight', 'decoder.model.layers.7.post_attention_layernorm.weight', 'decoder.model.layers.7.mlp.gate_proj.weight', 'decoder.model.layers.7.mlp.up_proj.weight', 'decoder.model.layers.7.mlp.down_proj.weight', 'decoder.model.layers.8.input_layernorm.weight', 'decoder.model.layers.8.self_attn.q_proj.weight', 'decoder.model.layers.8.self_attn.k_proj.weight', 'decoder.model.layers.8.self_attn.v_proj.weight', 'decoder.model.layers.8.self_attn.o_proj.weight', 'decoder.model.layers.8.post_attention_layernorm.weight', 'decoder.model.layers.8.mlp.gate_proj.weight', 'decoder.model.layers.8.mlp.up_proj.weight', 'decoder.model.layers.8.mlp.down_proj.weight', 'decoder.model.layers.9.input_layernorm.weight', 'decoder.model.layers.9.self_attn.q_proj.weight', 'decoder.model.layers.9.self_attn.k_proj.weight', 'decoder.model.layers.9.self_attn.v_proj.weight', 'decoder.model.layers.9.self_attn.o_proj.weight', 'decoder.model.layers.9.post_attention_layernorm.weight', 'decoder.model.layers.9.mlp.gate_proj.weight', 'decoder.model.layers.9.mlp.up_proj.weight', 'decoder.model.layers.9.mlp.down_proj.weight', 'decoder.model.layers.10.input_layernorm.weight', 'decoder.model.layers.10.self_attn.q_proj.weight', 'decoder.model.layers.10.self_attn.k_proj.weight', 'decoder.model.layers.10.self_attn.v_proj.weight', 'decoder.model.layers.10.self_attn.o_proj.weight', 'decoder.model.layers.10.post_attention_layernorm.weight', 'decoder.model.layers.10.mlp.gate_proj.weight', 'decoder.model.layers.10.mlp.up_proj.weight', 'decoder.model.layers.10.mlp.down_proj.weight', 'decoder.model.layers.11.input_layernorm.weight', 'decoder.model.layers.11.self_attn.q_proj.weight', 'decoder.model.layers.11.self_attn.k_proj.weight', 'decoder.model.layers.11.self_attn.v_proj.weight', 'decoder.model.layers.11.self_attn.o_proj.weight', 'decoder.model.layers.11.post_attention_layernorm.weight', 'decoder.model.layers.11.mlp.gate_proj.weight', 'decoder.model.layers.11.mlp.up_proj.weight', 'decoder.model.layers.11.mlp.down_proj.weight', 'decoder.model.layers.12.input_layernorm.weight', 'decoder.model.layers.12.self_attn.q_proj.weight', 'decoder.model.layers.12.self_attn.k_proj.weight', 'decoder.model.layers.12.self_attn.v_proj.weight', 'decoder.model.layers.12.self_attn.o_proj.weight', 'decoder.model.layers.12.post_attention_layernorm.weight', 'decoder.model.layers.12.mlp.gate_proj.weight', 'decoder.model.layers.12.mlp.up_proj.weight', 'decoder.model.layers.12.mlp.down_proj.weight', 'decoder.model.layers.13.input_layernorm.weight', 'decoder.model.layers.13.self_attn.q_proj.weight', 'decoder.model.layers.13.self_attn.k_proj.weight', 'decoder.model.layers.13.self_attn.v_proj.weight', 'decoder.model.layers.13.self_attn.o_proj.weight', 'decoder.model.layers.13.post_attention_layernorm.weight', 'decoder.model.layers.13.mlp.gate_proj.weight', 'decoder.model.layers.13.mlp.up_proj.weight', 'decoder.model.layers.13.mlp.down_proj.weight', 'decoder.model.layers.14.input_layernorm.weight', 'decoder.model.layers.14.self_attn.q_proj.weight', 'decoder.model.layers.14.self_attn.k_proj.weight', 'decoder.model.layers.14.self_attn.v_proj.weight', 'decoder.model.layers.14.self_attn.o_proj.weight', 'decoder.model.layers.14.post_attention_layernorm.weight', 'decoder.model.layers.14.mlp.gate_proj.weight', 'decoder.model.layers.14.mlp.up_proj.weight', 'decoder.model.layers.14.mlp.down_proj.weight', 'decoder.model.layers.15.input_layernorm.weight', 'decoder.model.layers.15.self_attn.q_proj.weight', 'decoder.model.layers.15.self_attn.k_proj.weight', 'decoder.model.layers.15.self_attn.v_proj.weight', 'decoder.model.layers.15.self_attn.o_proj.weight', 'decoder.model.layers.15.post_attention_layernorm.weight', 'decoder.model.layers.15.mlp.gate_proj.weight', 'decoder.model.layers.15.mlp.up_proj.weight', 'decoder.model.layers.15.mlp.down_proj.weight', 'decoder.model.layers.16.input_layernorm.weight', 'decoder.model.layers.16.self_attn.q_proj.weight', 'decoder.model.layers.16.self_attn.k_proj.weight', 'decoder.model.layers.16.self_attn.v_proj.weight', 'decoder.model.layers.16.self_attn.o_proj.weight', 'decoder.model.layers.16.post_attention_layernorm.weight', 'decoder.model.layers.16.mlp.gate_proj.weight', 'decoder.model.layers.16.mlp.up_proj.weight', 'decoder.model.layers.16.mlp.down_proj.weight', 'decoder.model.layers.17.input_layernorm.weight', 'decoder.model.layers.17.self_attn.q_proj.weight', 'decoder.model.layers.17.self_attn.k_proj.weight', 'decoder.model.layers.17.self_attn.v_proj.weight', 'decoder.model.layers.17.self_attn.o_proj.weight', 'decoder.model.layers.17.post_attention_layernorm.weight', 'decoder.model.layers.17.mlp.gate_proj.weight', 'decoder.model.layers.17.mlp.up_proj.weight', 'decoder.model.layers.17.mlp.down_proj.weight', 'decoder.model.layers.18.input_layernorm.weight', 'decoder.model.layers.18.self_attn.q_proj.weight', 'decoder.model.layers.18.self_attn.k_proj.weight', 'decoder.model.layers.18.self_attn.v_proj.weight', 'decoder.model.layers.18.self_attn.o_proj.weight', 'decoder.model.layers.18.post_attention_layernorm.weight', 'decoder.model.layers.18.mlp.gate_proj.weight', 'decoder.model.layers.18.mlp.up_proj.weight', 'decoder.model.layers.18.mlp.down_proj.weight', 'decoder.model.layers.19.input_layernorm.weight', 'decoder.model.layers.19.self_attn.q_proj.weight', 'decoder.model.layers.19.self_attn.k_proj.weight', 'decoder.model.layers.19.self_attn.v_proj.weight', 'decoder.model.layers.19.self_attn.o_proj.weight', 'decoder.model.layers.19.post_attention_layernorm.weight', 'decoder.model.layers.19.mlp.gate_proj.weight', 'decoder.model.layers.19.mlp.up_proj.weight', 'decoder.model.layers.19.mlp.down_proj.weight', 'decoder.model.layers.20.input_layernorm.weight', 'decoder.model.layers.20.self_attn.q_proj.weight', 'decoder.model.layers.20.self_attn.k_proj.weight', 'decoder.model.layers.20.self_attn.v_proj.weight', 'decoder.model.layers.20.self_attn.o_proj.weight', 'decoder.model.layers.20.post_attention_layernorm.weight', 'decoder.model.layers.20.mlp.gate_proj.weight', 'decoder.model.layers.20.mlp.up_proj.weight', 'decoder.model.layers.20.mlp.down_proj.weight', 'decoder.model.layers.21.input_layernorm.weight', 'decoder.model.layers.21.self_attn.q_proj.weight', 'decoder.model.layers.21.self_attn.k_proj.weight', 'decoder.model.layers.21.self_attn.v_proj.weight', 'decoder.model.layers.21.self_attn.o_proj.weight', 'decoder.model.layers.21.post_attention_layernorm.weight', 'decoder.model.layers.21.mlp.gate_proj.weight', 'decoder.model.layers.21.mlp.up_proj.weight', 'decoder.model.layers.21.mlp.down_proj.weight', 'decoder.model.layers.22.input_layernorm.weight', 'decoder.model.layers.22.self_attn.q_proj.weight', 'decoder.model.layers.22.self_attn.k_proj.weight', 'decoder.model.layers.22.self_attn.v_proj.weight', 'decoder.model.layers.22.self_attn.o_proj.weight', 'decoder.model.layers.22.post_attention_layernorm.weight', 'decoder.model.layers.22.mlp.gate_proj.weight', 'decoder.model.layers.22.mlp.up_proj.weight', 'decoder.model.layers.22.mlp.down_proj.weight', 'decoder.model.layers.23.input_layernorm.weight', 'decoder.model.layers.23.self_attn.q_proj.weight', 'decoder.model.layers.23.self_attn.k_proj.weight', 'decoder.model.layers.23.self_attn.v_proj.weight', 'decoder.model.layers.23.self_attn.o_proj.weight', 'decoder.model.layers.23.post_attention_layernorm.weight', 'decoder.model.layers.23.mlp.gate_proj.weight', 'decoder.model.layers.23.mlp.up_proj.weight', 'decoder.model.layers.23.mlp.down_proj.weight', 'decoder.model.layers.24.input_layernorm.weight', 'decoder.model.layers.24.self_attn.q_proj.weight', 'decoder.model.layers.24.self_attn.k_proj.weight', 'decoder.model.layers.24.self_attn.v_proj.weight', 'decoder.model.layers.24.self_attn.o_proj.weight', 'decoder.model.layers.24.post_attention_layernorm.weight', 'decoder.model.layers.24.mlp.gate_proj.weight', 'decoder.model.layers.24.mlp.up_proj.weight', 'decoder.model.layers.24.mlp.down_proj.weight', 'decoder.model.layers.25.input_layernorm.weight', 'decoder.model.layers.25.self_attn.q_proj.weight', 'decoder.model.layers.25.self_attn.k_proj.weight', 'decoder.model.layers.25.self_attn.v_proj.weight', 'decoder.model.layers.25.self_attn.o_proj.weight', 'decoder.model.layers.25.post_attention_layernorm.weight', 'decoder.model.layers.25.mlp.gate_proj.weight', 'decoder.model.layers.25.mlp.up_proj.weight', 'decoder.model.layers.25.mlp.down_proj.weight', 'decoder.model.layers.26.input_layernorm.weight', 'decoder.model.layers.26.self_attn.q_proj.weight', 'decoder.model.layers.26.self_attn.k_proj.weight', 'decoder.model.layers.26.self_attn.v_proj.weight', 'decoder.model.layers.26.self_attn.o_proj.weight', 'decoder.model.layers.26.post_attention_layernorm.weight', 'decoder.model.layers.26.mlp.gate_proj.weight', 'decoder.model.layers.26.mlp.up_proj.weight', 'decoder.model.layers.26.mlp.down_proj.weight', 'decoder.model.layers.27.input_layernorm.weight', 'decoder.model.layers.27.self_attn.q_proj.weight', 'decoder.model.layers.27.self_attn.k_proj.weight', 'decoder.model.layers.27.self_attn.v_proj.weight', 'decoder.model.layers.27.self_attn.o_proj.weight', 'decoder.model.layers.27.post_attention_layernorm.weight', 'decoder.model.layers.27.mlp.gate_proj.weight', 'decoder.model.layers.27.mlp.up_proj.weight', 'decoder.model.layers.27.mlp.down_proj.weight', 'decoder.model.layers.28.input_layernorm.weight', 'decoder.model.layers.28.self_attn.q_proj.weight', 'decoder.model.layers.28.self_attn.k_proj.weight', 'decoder.model.layers.28.self_attn.v_proj.weight', 'decoder.model.layers.28.self_attn.o_proj.weight', 'decoder.model.layers.28.post_attention_layernorm.weight', 'decoder.model.layers.28.mlp.gate_proj.weight', 'decoder.model.layers.28.mlp.up_proj.weight', 'decoder.model.layers.28.mlp.down_proj.weight', 'decoder.model.layers.29.input_layernorm.weight', 'decoder.model.layers.29.self_attn.q_proj.weight', 'decoder.model.layers.29.self_attn.k_proj.weight', 'decoder.model.layers.29.self_attn.v_proj.weight', 'decoder.model.layers.29.self_attn.o_proj.weight', 'decoder.model.layers.29.post_attention_layernorm.weight', 'decoder.model.layers.29.mlp.gate_proj.weight', 'decoder.model.layers.29.mlp.up_proj.weight', 'decoder.model.layers.29.mlp.down_proj.weight', 'decoder.model.layers.30.input_layernorm.weight', 'decoder.model.layers.30.self_attn.q_proj.weight', 'decoder.model.layers.30.self_attn.k_proj.weight', 'decoder.model.layers.30.self_attn.v_proj.weight', 'decoder.model.layers.30.self_attn.o_proj.weight', 'decoder.model.layers.30.post_attention_layernorm.weight', 'decoder.model.layers.30.mlp.gate_proj.weight', 'decoder.model.layers.30.mlp.up_proj.weight', 'decoder.model.layers.30.mlp.down_proj.weight', 'decoder.model.layers.31.input_layernorm.weight', 'decoder.model.layers.31.self_attn.q_proj.weight', 'decoder.model.layers.31.self_attn.k_proj.weight', 'decoder.model.layers.31.self_attn.v_proj.weight', 'decoder.model.layers.31.self_attn.o_proj.weight', 'decoder.model.layers.31.post_attention_layernorm.weight', 'decoder.model.layers.31.mlp.gate_proj.weight', 'decoder.model.layers.31.mlp.up_proj.weight', 'decoder.model.layers.31.mlp.down_proj.weight', 'decoder.model.norm.weight', 'decoder.lm_head.weight'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the extended trained model\n",
    "ckpt_dict = {}\n",
    "\n",
    "model = ScigptModel(args)\n",
    "model.decoder.resize_token_embeddings(args.vocab_size)\n",
    "model_dict = model.state_dict()\n",
    "print(f\"model_dict: {model_dict.keys()}\")\n",
    "print(model_dict['decoder.model.layers.0.mlp.gate_proj.weight'].shape)\n",
    "print(model_dict['decoder.model.layers.0.mlp.up_proj.weight'].shape)\n",
    "weight1_size=model_dict['decoder.model.layers.0.mlp.gate_proj.weight'].size(0)\n",
    "weight2_size=model_dict['decoder.model.layers.0.mlp.up_proj.weight'].size(0)\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_00-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer0.items():\n",
    "    if k=='word_embeddings.weight':\n",
    "        ckpt_dict['decoder.model.embed_tokens.weight'] = v\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 1).zfill(2)\n",
    "    layer = torch.load(os.path.join(args.save_dir, f\"layer_{l_index}-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        if k==\"self_attention.layernorm_qkv.layer_norm_weight\":\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.input_layernorm.weight\"] = layer[k]\n",
    "        elif k=='self_attention.layernorm_qkv.query_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.self_attn.q_proj.weight\"] = layer[k]\n",
    "        elif k=='self_attention.layernorm_qkv.key_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.self_attn.k_proj.weight\"] = layer[k]\n",
    "        elif k=='self_attention.layernorm_qkv.value_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.self_attn.v_proj.weight\"] = layer[k]\n",
    "        elif k=='self_attention.proj.weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.self_attn.o_proj.weight\"] = layer[k]\n",
    "        elif k=='layernorm_mlp.layer_norm_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.post_attention_layernorm.weight\"] = layer[k]\n",
    "        elif k=='layernorm_mlp.fc1_weight':\n",
    "            weight1,weight2=torch.split(layer[k], [weight1_size, weight2_size], dim=0)\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.mlp.gate_proj.weight\"] = weight1\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.mlp.up_proj.weight\"] = weight2\n",
    "        elif k=='layernorm_mlp.fc2_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.mlp.down_proj.weight\"] = layer[k]\n",
    "    del layer\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_33-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_34-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "\n",
    "print(f\"ckpt_dict: {ckpt_dict.keys()}\")\n",
    "model_dict.update(ckpt_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dict = {}\n",
    "# Load the original llama3 model\n",
    "model = ScigptModel(args)\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "print(f\"model_dict: {model_dict.keys()}\")\n",
    "\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer0.items():\n",
    "    new_k = \"decoder.model.\" + k\n",
    "    ckpt_dict[new_k] = v\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 1).zfill(2)\n",
    "    layer = torch.load(os.path.join(args.save_dir, f\"layer_{l_index}-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        ckpt_dict[f\"decoder.model.layers.{l}.{k}\"] = layer[k]\n",
    "    del layer\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_33-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_34-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "\n",
    "print(f\"ckpt_dict: {ckpt_dict.keys()}\")\n",
    "model_dict.update(ckpt_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ScigptModel(\n",
       "  (loss): AutoregressiveCriterion(\n",
       "    (cross_entropy): CrossEntropyLoss()\n",
       "  )\n",
       "  (decoder): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(130304, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaSdpaAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=130304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'psize': 4096, 'depth': 2, 'branch_pages': 1, 'leaf_pages': 38, 'overflow_pages': 33698, 'entries': 3745}\n",
      "3744\n"
     ]
    }
   ],
   "source": [
    "import lmdb\n",
    "from sfm.data.prot_data.util import bstr2obj\n",
    "# load data\n",
    "file_path='/home/v-zekunguo/sfm/nlm/valid_lmdb/new_valid.patent.v2.txt.lmdb'\n",
    "env = lmdb.open(\n",
    "    file_path, subdir=True, readonly=True, lock=False, readahead=False\n",
    ")\n",
    "txn = env.begin(write=False)\n",
    "\n",
    "print(env.stat())\n",
    "count=0\n",
    "metadata = bstr2obj(txn.get(\"metadata\".encode()))\n",
    "cur_len, cur_keys = metadata[\"size\"], metadata[\"keys\"]\n",
    "print(cur_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' treatment time for step (iii) is from about 30 minutes to 18 hours, particularly from about 1 to 6 hours, more particularly from about 2 to 3 hours. Typically, the fabric is washed between each of the steps (i) to (iii) of the method of the present invention. For example, the fabric may be washed with water, for example with distilled water. The washing step substantially removes residual reagents present from the previous reaction step(s). Typically, after step the fibrous catalyst is dried before use. The catalyst may be dried using any conventional means, for example at temperatures up to 105° C. Any fabric comprising PAN fibres may be used in the present invention. The references herein to a fabric may refer simply to an arrangement of one or more PAN fibres. In one aspect of the invention, the fabric that comprises PAN fibres is a knitted fabric, such as a fibrous knitted mesh. Thus, in this aspect, the PAN fibres/yarn must be capable of being knitted. The knitted fabric may be prepared by any suitable method known in the art. For example, the fabric may be knitted using conventional equipment using the “polufang” (half-cardigan structure) knitting method, which method is identifiable by British Standard 5441:1998 and would be well known to a person skilled in the art. The PAN fibres may be complex PAN fibres and may be prepared by any suitable method known to a person skilled in the art. For example, the PAN fibres may be prepared according to Russian Standard 6-0602-80. The fabric may comprise one or more inert mono-fibres in addition to the PAN fibres. The inert mono-fibres preferably should be capable of being knitted and act as a support for the PAN fibres, so as to provide a fibrous catalyst that is self-supporting. Without wishing to be bound by any theory, the inert mono-fibres are not believed to participate directly in any reaction(s) in which the fibrous catalysts are used. The inert mono-fibres may be any suitable fibres known to a person skilled in the art. For example, suitable inert mono-fibres include polypropylene fibres (such as polypropylene fibres made according to Russian Standard 6-06-537-87). When the knitted fabric comprises one or more inert mono-fibres, any suitable weight ratio of PAN fibres to inert mono-fibres may be used. It is preferred that the knitted fabric comprises a higher proportion of PAN fibres than inert mono-fibres. This is because the inert mono-fibres are not believed to participate in any reaction(s) in which the fibrous catalysts are used. For example, the knitted fabric may comprise the PAN fibres and the inert mono-fibres in a weight ratio in the range of from 90:10 to 10:90, particularly 75:25 to 25:75 and more particularly 60:40 to 40:60. The method of the present invention may be conducted in any suitable reactor. Preferably, the method is conducted in a suitable dye bath reactor (i.e. a reactor typically used for dying fabric). In particular, the method may be conducted in a commercial dye bath with rollers arranged for passing the fabric through the dye bath in which the treatment(s) are conducted. An example of a suitable dye bath reactor is a URGNANO-BERGAMO MCS, WRT 3 reactor (made in Italy). Typically, a dye bath reactor comprises a cylindrical drum lying on its side and the body of the reactor is made of stainless steel (such as stainless steel 316). Typically, a dye bath reactor is equipped with suitable means for monitoring a reaction, such as a temperature sensor and regulator, a pressure sensor and a flow meter. A dye bath reactor may also include a programmable interface (for example for defining temperature programmes and/or automating the dosage of reagents) and may also include one or more heat exchangers for cooling the reactor when required. References herein to aqueous solutions are intended to refer to solutions in a suitable solvent or diluent comprising at least 40%, particularly at least 50%, by volume of water. In particular, the solvent or diluent is water. Additionally, the skilled person would appreciate that such solutions include the component(s) thereof (for example metal salt(s)) substantially dissolved therein but that minor amounts of the component(s) may be present as a suspension in the solvent or diluent. Additionally, a proportion of the component(s) may become suspended in the solvent or diluent as the method step(s) of the present invention are conducted. According to another aspect of the present invention there is provided a fibrous catalyst obtainable by the method of the present invention. According to yet another aspect of the present invention there is provided a fibrous catalyst obtained by the method of the present invention. The fibrous catalysts made according to the method of the present invention are suitable for treating a waste stream. The references to the treatment of a waste stream are intended to refer to the conversion of an undesired, potentially harmful, “waste” compound contained in the waste stream into a new derivative that typically is at least less harmful and/or easier to dispose of. In some cases, the new derivative formed may be useful in another process and/or application and may be isolated and/or collected for further use. When this is not the case, the new derivative that is formed typically will be collected and/or disposed of in any suitable manner. Typically, the fibrous catalysts do not convert 100% of the “waste” compound into the new derivative. As the skilled person would appreciate, the percentage conversion will depend on a number of factors, including the particular fibrous catalyst selected and the composition of the waste stream being treated. It is expected that the fibrous catalysts prepared according to the method of the present invention will generally convert from about 50% to about 100% by weight of the “waste” compound into the new derivative. As discussed above, the method of the present invention provides a fibrous catalyst having increased loading of iron and/or nickel cations fixed to the PAN fibres. It is believed that this increase in loading should increase the number of active sites on the catalyst and hence improve catalytic activity and/or extend the lifetime of the catalyst. The fibrous catalysts prepared according to the method of the present invention are suitable for treating a wide variety of waste streams comprising virtually any undesired organic compound(s). For example, they may be used to treat waste streams originating from the chemical, pharmaceutical, petroleum chemical, textile, pulp, leather, agro-chemical, furniture manufacturing and photo-processing industries, for example comprising virtually any undesired organic compound(s). In particular, the fibrous catalysts may be used to treat waste streams containing one or more dyes, for example waste streams containing one or more dyes and which originate from the textile or chemical industries. The kit includes those components that are required to provide a fibrous catalyst for the treatment of a waste stream. The PAN fibres, hydrazine salt, hydroxylamine salt, base, sulfate salt of the first metal cation and salt of the second metal cation are as defined above in relation to the method of the present invention. The base is selected so as to provide the desired pH for each step of the method, as necessary. The fibrous catalysts may be prepared from the components of the kit using any suitable method, for example using a method as hereinbefore defined. In one aspect, the kit comprises a sulfate salt of an iron cation. In another aspect, the kit comprises a sulfate salt of a nickel cation. In one aspect, the kit comprises a hydrazine salt selected from hydrazine dihydrochloride, hydrazine monohydrochloride, hydrazine hydrate, hydrazine monohydrobromide, hydrazine acetate, hydrazine sulfate and dihydrazine sulfate, and mixtures thereof. More especially, the hydrazine salt is selected from hydrazine sulfate and dihydrazine sulfate, and mixtures thereof (even more especially the hydrazine salt is dihydrazine sulfate). In one aspect, the kit comprises a hydroxylamine salt selected from hydroxylamine monohydrochloride, hydroxylamine sulfate and hydroxylamine phosphate, and mixtures thereof. More especially, the hydroxylamine salt is hydroxylamine sulfate. In one aspect, the kit comprises a base selected from sodium hydroxide, potassium hydroxide and sodium carbonate, and mixtures thereof. In one aspect, the kit comprises a salt of a second metal cation, wherein the second metal cation is selected from a sodium, lithium, potassium, calcium, magnesium and zinc cation, and mixtures thereof. Particularly, the second metal cation is selected from a sodium, lithium, potassium and calcium cation, more particularly from a sodium, potassium and calcium cation, even more particularly from a sodium and calcium cation, and mixtures thereof. In another aspect, the second metal cation may be selected from a sodium, lithium and potassium cation, especially from a sodium and potassium cation, and mixtures thereof. In one aspect, the second metal cation is a calcium cation. In another aspect, the second metal cation is a sodium cation. The salts of the second metal cation(s) may comprise any suitable anion, such as chlorides, iodides, bromides, fluorides, sulfates, hydrogen sulfates, carboxylates, thiosulfates, thiocyanates, perchlorates, nitrates and nitrites, particularly chlorides, sulfates, nitrates and nitrites, more particularly chlorides, sulfates and nitrates, even more particularly chlorides and sulfates, and mixtures thereof.<|end_of_text|><|begin_of_text|>1. Structure. In various aspects, the present disclosure relates to methods for preparing a 2,2-bis(3-phenyl-4-hydroxyphenyl) propane (PHP) monomer. In further aspects, the PHP monomer has a structure represented by the formula: In various aspects, this general class of monomers is known as aromatic dihydroxy compounds. In various aspects, the present disclosure provides a method for preparing an ortho aryl substituted bisphenol monomers, the method comprising: a) providing a phenol; b) providing a ketone; and c) reacting the phenol and ketone under conditions effective to provide a reaction product comprising an ortho aryl substituted bisphenol monomers, wherein the monomer exhibits a lower binding energy than estradiol for estradiol related receptor (ERR)-α obtained from the computational method described herein. In further aspects, the phenol comprises 2-phenylphenol. In some aspects, the 2-phenylphenol is a bio-based material. In further aspects, the phenol derived from a biological material. In some aspects, the phenol is wholly derived from a biological material. In other aspects, the phenol is partly derived from a biological material. In further aspects, the phenol not derived from organic material that has been transformed by geological processes into petroleum, petrochemicals, and combinations thereof. In further aspects, the phenol is from greater than 0 wt. % to about 100 wt. % derived from bio-based material, including exemplary values of 1 wt. %, 5 wt. %, 10 wt. %, 20 wt. %, 30 wt. %, 40 wt. %, 50 wt. %, 60 wt. %, 70 wt. %, 80 wt. %, 90 wt. %, and 99 wt. % derived from bio-based material. In still further aspects, the bio-based material comprises at least one lignocellulosic material, plant material, or a combination thereof. In further aspects, the ketone is acetone. In various aspects, the 2-phenylphenol and acetone are provided at a ratio of at least about 2:1. In further aspects, the 2-phenylphenol and acetone are provided at a ratio of at least about 3:1. In still further aspects, the 2-phenylphenol and acetone are provided at a ratio of from about 2:1 to about 6:1, including exemplary ratios of 3:1, 4:1, and 5:1. In further aspects, the phenol and ketone are provided at a ratio of at least about 2:1. In still further aspects, the phenol and ketone are provided at a ratio of from about 2:1 to about 6:1, including exemplary ratios of 3:1, 4:1, and 5:1. In further aspects, conditions effective comprise reacting the phenol and ketone in the presence of a catalyst. In still further aspects, the catalyst can comprise H2SO4 an cationic acidic ion exchange resin or sulfonated polystyrene resin cross linked with divinyl benzene catalyst, dodecyl benzene sulfonic acid (DBSA), trifluoro acetic acid, toluene sulfonic acid, trifluromethane sulfonic acid, HCl, or a combination thereof. In yet further aspects, conditions effective comprise reacting the phenol and acetone in the presence of 3-mercaptopropionic acid (3-MPA) as promoter. In some aspects, the catalyst is H2SO4 an cationic acidic ion exchange resin or sulfonated polystyrene resin cross linked with divinyl benzene, C1-C24 alkyl or alkyl aryl sulfonic acid, HCl, or a combination thereof. In other aspects, the catalyst is concentrated HCl. In further aspects, conditions effective comprise reacting the 2-phenylphenol and acetone in the presence of a catalyst. In still further aspects, the catalyst can comprise H2SO4 an cationic acidic ion exchange resin or sulfonated polystyrene resin cross linked with divinyl benzene catalyst, dodecyl benzene sulfonic acid (DBSA), trifluoro acetic acid, toluene sulfonic acid, trifluromethane sulfonic acid, HCl, or a combination thereof. In some aspects, the catalyst is H2SO4 an cationic acidic ion exchange resin or sulfonated polystyrene resin cross linked with divinyl benzene, C1-C24 alkyl or alkyl aryl sulfonic acid, HCl, or a combination thereof. In other aspects, the catalyst is HCl gas. In some other aspects the catalyst is a solid acid catalyst such as sulfonated Zirconia, Acidified clays, or combination thereof.<|end_of_text|><|begin_of_text|>Y is H or CH3, and. Rc and Rb are independently selected from the group consisting of H, halo and CF3. In one embodiment, the compound of Formula (IA), wherein Z is —CH2-phenyl, wherein phenyl is optionally substituted with one or more substituents independently selected from the group consisting of CN, halo and —OH. In one embodiment, the compound of Formula (IA), wherein Z is —CH2-pyrimidinyl, wherein pyrimidinyl is optionally substituted with one substituent selected from CH3 or OCH3. In one embodiment, the compound of Formula (IA), wherein Z is —CH2-unsubstituted pyrimidinyl. In one embodiment, the compound of Formula (IA), wherein Z is —CH2-pyrimidinyl substituted with one substituent selected from CH3 or OCH3. In one embodiment, the compound of Formula (IA), wherein Z is —CH2-pyrimidinyl optionally substituted with one OCH3. In one embodiment, the compound of Formula (IA), wherein Z is —CH2-unsubstituted phenyl. In one embodiment, the compound of Formula (IA), wherein Z is —CH2-phenyl optionally substituted with one or more substituents independently selected from the group consisting of CN, halo and —OH. In one embodiment, this invention also relates to compounds of any of the above embodiments related to Formula (IA), wherein X is O. In one embodiment, this invention also relates to compounds of any of the above embodiments related to Formula (IA), wherein X is absent. In one embodiment, this invention also relates to compounds of any of the above embodiments related to Formula (IA), wherein X is NH or NCH3. In one embodiment, this invention also relates to compounds of any of the above embodiments related to Formula (IA), wherein Y is H. In one embodiment, this invention also relates to compounds of any of the above embodiments related to Formula (IA), wherein Y is CH3. In one embodiment, this invention also relates to compounds of any of the above embodiments related to Formula (IA), Rc and Rb are independently halo or CF3. In one embodiment, this invention also relates to compounds of any of the above embodiments related to Formula (IA), Rc and Rb are independently Cl or CF3. In one embodiment, the compound of Formula (I) has the structure of Formula (IA), or pharmaceutically acceptable salts thereof, wherein Z is —CH2-pyrimidinyl, wherein pyrimidinyl is optionally substituted with one OCH3; X is O; Y is H; Rc and Rb are independently halo or CF3.<|end_of_text|><|begin_of_text|>Method for producing optically active diamine derivative. The problem to be solved is to provide an important intermediate for production of an FXa inhibitor. The solution thereto is a method for industrially producing a compound (1) or a compound (4), comprising: [Step 1]: adding a quaternary ammonium salt and a metal azide salt to water to prepare an aqueous solution of an azidification reagent complex comprising quaternary ammonium salt-metal azide salt, and subsequently dehydrating the aqueous solution using an aromatic hydrocarbon solvent to form a mixed solution of the azidification reagent complex comprising quaternary ammonium salt-metal azide salt and the aromatic hydrocarbon solvent with a water content of 0.2% or less; and [Step 2]: adding, to the mixed solution prepared in [Step 1], a compound (2) wherein L represents a leaving group. This application is a continuation of International Application No. PCT/JP2010/053976, filed on Mar. 10, 2010, entitled “METHOD FOR PRODUCING OPTICALLY ACTIVE DIAMINE DERIVATIVE”, which claims the benefit of Japanese Patent Application Number JP 2009-061708, filed on Mar. 13, 2009, all of which are hereby incorporated by reference. The present invention relates to a method for industrially producing an optically active diamine derivative that is important for production of a compound (A) shown below as an activated blood coagulation factor X (FXa) inhibitor or a pharmacologically acceptable salt thereof, or a hydrate thereof. Compound (A) or a pharmacologically acceptable salt thereof, or a hydrate thereof is a compound that exhibits an FXa inhibitory effect, as disclosed in Patent Literatures 1 to 3, and is useful as preventive and/or therapeutic drugs for thrombotic and/or embolic diseases. The pamphlet of International Publication No. WO 2007/032498 discloses a method for producing compound (A) using an azide derivative compound (1) and an optically active diamine derivative compound (3) as production intermediates. This pamphlet discloses a method comprising treating a compound (2a) with sodium azide and 1-dodecylpyridinium chloride in an amide solvent such as N,N-dimethylacetamide (DMAC) to produce compound (1). The method for producing compound (1), an optically active azide derivative, disclosed in the pamphlet of International Publication No. WO 2007/032498 results in yields as low as 30% or less. This method although using a reduced amount of a dangerous reagent, the metal azide salt, may be available on an industrial scale as long as its yield can be improved. Moreover, the highly polar amide solvent used as a reaction solvent in the conventional method is probably responsible for reduction in yield caused by loss of the product compound (1) during extraction. Thus, an object of the present invention is to solve these problems and to provide a novel method for industrially producing compound (1) that is an important synthetic intermediate for production of compound (A). The present inventors have conducted diligent studies over the years with the aim of improving the yield of compound (1). As a result, the present inventors have completed the present invention by finding that the yield of compound (1) is drastically improved by dissolving a quaternary ammonium salt and a metal azide salt in water to prepare an aqueous solution of an azidification reagent complex comprising quaternary ammonium salt-metal azide salt, subsequently subjecting the aqueous solution of the azidification reagent complex to azeotropic dehydration using an aromatic hydrocarbon solvent to prepare a mixed solution of the azidification reagent complex and the aromatic hydrocarbon solvent with a water content of 0.2% or less, and treating this mixed solution with a compound (2). The present invention is useful because optically active azide derivative compound (1) that is an intermediate for production of compound (A) known as an FXa inhibitor can be obtained at a yield of 71 to 75%. Specifically, the production method of the present invention is useful because the yield can be improved drastically compared with the conventional method. Thus, the production method of the present invention is useful as a method for producing compound (A) useful as an FXa inhibitor. Hereinafter, the present invention will be described in detail. A halogeno group according to the present invention means a fluoro, chloro, or bromo group. A C1 to C20 alkyl group according to the present invention means a linear or branched alkyl group having 1 to 20 carbon atoms. Examples thereof can include methyl, ethyl, n-propyl, isopropyl, n-butyl, pentyl, hexyl, heptyl, octyl, nonyl, decyl, dodecyl, tridecyl, tetradecyl, pentadecyl, hexadecyl, heptadecyl, octadecyl, nonadecyl, and icosyl groups. A specific FXa inhibitor according to the present specification is preferably, for example, compound (A) described above. Compound (A) may be a free form (free base) or a hydrate thereof or may be a pharmacologically acceptable salt or a hydrate of the salt. Examples of the salt of compound (A) include hydrochloride, sulfate, hydrobromide, hydroiodide, phosphate, nitrate, benzoate, methanesulfonate, 2-hydroxyethanesulfonate, p-toluenesulfonate, acetate, propanoate, oxalate, malonate, succinate, glutarate, adipate, tartrate, maleate, fumarate, malate, and mandelate. The salt of compound (A) is preferably hydrochloride or p-toluenesulfonate, particularly preferably p-toluenesulfonate. Hereinafter, the production method of the present invention will be described in detail. The optically active azide derivative compound (1) of the present invention can be produced from compound (2) by the following [Step 1] and [Step 2]: After the completion of the reaction, the reaction mixture of [Step 2] is treated with an aqueous alkali solution as a usual work-up procedure, followed by extraction with an aromatic hydrocarbon solvent and washing of the extracts with water to obtain compound (1). The leaving group represented by L in compound (2) according to the present invention is preferably a (C1 to C2 alkyl)sulfonyloxy group (wherein the C1 to C2 alkyl group may have one or more identical or different halogeno groups as substituents) and a benzenesulfonyloxy group (wherein the benzene ring may have one or more identical or different groups as substituents selected from a halogeno group, a methyl group, and a nitro group), more preferably a methanesulfonyloxy, ethanesulfonyloxy, trifluoromethanesulfonyloxy, 2-chloroethanesulfonyloxy, benzenesulfonyloxy, p-toluenesulfonyloxy, 4-chlorobenzenesulfonyloxy, 2-chlorobenzenesulfonyloxy, 4-nitrobenzenesulfonyloxy, or 2-nitrobenzenesulfonyloxy group. L is even more preferably a methanesulfonyloxy or ethanesulfonyloxy group, particularly preferably a methanesulfonyloxy group. Preferable examples of the quaternary ammonium salt according to the present invention can include quaternary ammonium salts of alkylamines, and pyridinium salts. In this context, examples of the quaternary ammonium salts of alkylamines include tetramethylammonium chloride, tetrabutylammonium chloride, trioctylmethylammonium chloride, benzyltrimethylammonium chloride, and benzyltributylammonium chloride, and those in which chloride ions in these salts are substituted by other anions (e.g., bromide, iodide, and bisulfate ions). Examples of the pyridinium salts can include: N-alkylpyridinium chlorides such as N-butylpyridinium chloride, N-hexylpyridinium chloride, N-octylpyridinium chloride, N-dodecylpyridinium chloride (N-laurylpyridinium chloride), and N-cetylpyridinium chloride; N-alkylpicolinium chlorides such as N-lauryl-2-picolinium chloride, N-cetyl-2-picolinium chloride, N-lauryl-3-picolinium chloride, N-cetyl-3-picolinium chloride, N-lauryl-4-picolinium chloride, and N-cetyl-4-picolinium chloride; N-alkyl-4-phenylpropylpyridinium chlorides such as N-butyl-4-phenylpropylpyridinium chloride, N-hexyl-4-phenylpropylpyridinium chloride, N-octyl-4-phenylpropylpyridinium chloride, and N-lauryl-4-phenylpropylpyridinium chloride; and those in which chloride ions in these salts are substituted by other anions (e.g., bromide, iodide, and bisulfate ions). The quaternary ammonium salt according to the present invention is preferably a pyridinium salt, more preferably a 1-(C4 to C20 alkyl)pyridinium salt. The 1-(C4 to C20 alkyl)pyridinium salt is preferably a 1-dodecylpyridinium salt, more preferably a 1-(C4 to C20 alkyl)pyridinium halide, particularly preferably 1-dodecylpyridinium chloride (also known as 1-laurylpyridinium chloride). The amount of the quaternary ammonium salt used in [Step 1] of the present invention is preferably 0.45 to 0.55 molar equivalent with respect to compound (2). The quaternary ammonium salt can be used in an amount of 0.55 molar equivalent or more with respect to compound (2). Since the quaternary ammonium salt is a phase-transfer catalyst, a possible loss may occur during extraction procedures. Thus, the amount of the quaternary ammonium salt used is more preferably approximately 0.5 molar equivalent with respect to compound (2). The metal azide salt used in [Step 1] of the present invention is preferably an alkali metal azide salt, more preferably sodium azide or lithium azide, particularly preferably sodium azide. The amount of the metal azide salt used is preferably in the range of 1.8 to 2.2 molar equivalents, more preferably approximately 2.0 molar equivalents, with respect to compound (2), though the amount is not limited to this range in any way. The amount of water used for preparing an azidification reagent complex from the quaternary ammonium salt and the metal azide salt is preferably in the range of 1 to 2 parts by volume [1 to 2 (v/w)], more preferably approximately 1.0 part by volume [1.0 (v/w)], with respect to 1 part by weight of the compound (2), though the amount is not limited to this range in any way. Water used as a solvent is preferably used in a small amount for removing it by the subsequent azeotropic dehydration procedure, as long as it does not impair the preparation of the azidification reagent complex. The temperature for preparing the azidification reagent complex in [Step 1] may be room temperature and is preferably in the range of 20 to 40° C. The stirring time for preparing the azidification reagent complex in [Step 1] is preferably 0.5 hour or longer, more preferably in the range of 0.5 to 1.5 hours, though the stirring time is not limited to this range in any way. Dehydration in [Step 1] means azeotropic dehydration using an organic solvent for azeotropy of water and is preferably azeotropic dehydration using an aromatic hydrocarbon solvent. Dehydration in the present invention means azeotropic dehydration using an organic solvent known for azeotropy of water and is preferably azeotropic dehydration under heating using an aromatic hydrocarbon solvent. In the azeotropic dehydration, water removal apparatus may be used. Examples of water removal apparatus can include, but are not limited to in any way, a Dean-Stark water trap. The azeotropic dehydration in the present invention can be performed by removing water in the mixed solution of the aqueous solution of the azidification reagent complex and the aromatic hydrocarbon solvent by azeotropy and may be performed by gradually adding dropwise the aqueous solution of the azidification reagent complex to the aromatic hydrocarbon solvent under heating. In this context, the temperature for performing the azeotropic dehydration must be a temperature equal to or higher than the boiling point of the aromatic hydrocarbon solvent used, under normal pressure. In the azeotropic dehydration, a high-boiling solvent generally offers a high dehydration effect but however, requires preventing decomposition of the prepared azidification reagent complex. Alternatively, azeotropic dehydration under reduced pressure can be carried out at a temperature equal to or lower than the boiling point of the aromatic hydrocarbon solvent used. Since the prepared azidification reagent complex is a mixture of salts, this is preferred to prevent soap-like foaming in the organic solvent. The aromatic hydrocarbon solvent is distilled off together with water by this azeotropic dehydration, decreasing the solvent in the reaction system. However, the reaction system can be replenished appropriately with aromatic hydrocarbon solvent. The aromatic hydrocarbon solvent according to the present invention is preferably benzene, toluene, xylene, chlorobenzene, or dichlorobenzene. These solvents may be used alone (one thereof) or as a mixed solvent in which two or more thereof are mixed. The aromatic hydrocarbon solvent is more preferably toluene. The amount of the aromatic hydrocarbon solvent used is preferably approximately 5 parts by volume with respect to 1 part by weight of compound (2) [5 (v/w)], though the amount is not limited thereto in any way. In [Step 1], the water content is preferably set to 0.2% or less, more preferably 0.1% or less. The azeotropic dehydration in [Step 1] is preferably performed by gradually adding dropwise the aqueous solution of the azidification reagent complex to the aromatic hydrocarbon solvent under heating, more preferably under reduced pressure, for preventing foaming. Hereinafter, a preferred embodiment of [Step 1] in the present invention will be described. A quaternary ammonium salt at 0.5 molar equivalent with respect to compound (2) and a metal azide salt at 2 molar equivalents with respect to compound (2) are added to water at 1 part by volume with respect to 1 part by weight of compound (2) [1 (v/w)], and the mixture is stirred for 1 hour to prepare an aqueous solution of an azidification reagent complex. Subsequently, the aqueous solution of the azidification reagent complex is added dropwise, with care to avoid foaming, into toluene at 5 parts by volume with respect to 1 part by weight of compound (2) [5 (v/w)] (toluene is heated in advance to an internal temperature of 40 to 60° C.). Water is dehydrated by azeotropy under reduced pressure. In the azeotropic dehydration, a Dean-Stark water trap or the like may be used appropriately. After completion of the dropwise addition of the aqueous solution of the azidification reagent complex, the water content in the mixed solution is confirmed to be 0.2% or less, preferably 0.1% or less, and then, toluene is added thereto at 5 parts by volume with respect to 1 part by weight of compound (2) [5 (v/w)]. The thus-prepared mixed solution of the azidification reagent complex and the aromatic hydrocarbon solvent with a water content of 0.2% or less has been confirmed to have a small risk of explosion. [Step 2] in the present invention is the step of adding a compound represented by the following formula (2): The temperature and the stirring time in [Step 2] are preferably 69 to 71° C. in terms of the internal temperature of the reaction mixture and within 18 hours. The conventional method requires adjusting the reaction temperature to 60 to 63° C. in terms of the internal temperature, for preventing compound (2) from being decomposed due to water in the reaction solvent. However, at this temperature, approximately 36 hours are necessary as a reaction time. In the present invention, it was demonstrated that even at a reaction temperature raised to 69 to 71° C. in terms of the internal temperature, decomposition of compound (2) can be prevented by adjusting the water content in the mixed solution of the azidification reagent complex and the aromatic hydrocarbon solvent to 0.2% or less, more preferably 0.1% or less. The criterion for determining the endpoint of the reaction is preferably a compound (2) residual rate of 2% or less in terms of an area ratio obtained using HPLC. The work-up procedure after completion of the reaction in the present invention is preferably an aqueous alkali solution treatment of the mixed solution obtained in [Step 2]. The aqueous alkali solution according to the present invention means an aqueous solution of the hydroxide, carbonate, or bicarbonate, or the like of an alkali metal or an alkaline earth metal. Any of these aqueous alkali solutions can be used as saturated aqueous solutions or at lower concentrations as long as they neither decompose products or the like nor impair procedures such as extraction procedures. The aqueous alkali solution according to the present invention is preferably an aqueous sodium hydroxide solution, an aqueous potassium hydroxide solution, an aqueous sodium carbonate solution, an aqueous potassium carbonate solution, an aqueous sodium bicarbonate solution, or an aqueous potassium bicarbonate solution, more preferably an aqueous sodium bicarbonate solution or an aqueous potassium bicarbonate solution. After completion of [Step 2] in the present invention, the reaction mixture is preferably allowed to cool and washed with an aqueous alkali solution, followed by extraction with an aromatic hydrocarbon solvent under heating. In this context, the phrase “allowed to cool” means that the reaction solution after completion of the reaction is cooled to room temperature in terms of its internal temperature. The aqueous alkali solution exemplified above can be used as the aqueous alkali solution. Preferable examples of the aqueous alkali solution can include 5% aqueous bicarbonate solutions. The amount of the aqueous alkali solution used is preferably 5 parts by volume with respect to 1 part by weight of compound (2) [5 (v/w)]. The washing is performed by adding the aqueous alkali solution thereto, then stirring the mixed solution, and separating the aqueous layer as the lower layer by still standing. The extraction with toluene under heating is the procedure of adding toluene to the separated aqueous layer and separating the toluene layer as the upper layer. This extraction procedure may be repeated. The heating temperature in the extraction procedure is preferably approximately 40° C. The amount of toluene used in the extraction is preferably approximately 2 parts by volume with respect to 1 part by weight of compound (2) [2 (v/w)]. The layer separated by the previous washing with the alkali solution and the extracted toluene layer are combined and used as toluene extracts of compound (2). The toluene extracts can be washed with water to prepare a solution of compound (1) in the aromatic hydrocarbon solvent. In this context, the amount of water used in the washing with water is preferably approximately 1.5 parts by volume with respect to 1 part by weight of compound (2) [1.5 (v/w)]. The washing temperature with water is preferably approximately 40° C. The toluene layer is separated and used as a solution of compound (1) in toluene. Compound (2a) wherein the leaving group L in compound (2) used in [Step 2] is a methanesulfonyloxy group can be produced, for example, as shown in [Scheme 1] shown below. Specific examples of the production can include a method described in Reference Example 1. Specifically, compound (2a) can be produced by producing a compound (6) from a compound (5) and methanesulfonylating this compound (6). Compound (5) can be produced by a method described in WO 2007/032498. The present invention also provides a method for producing a compound (4) represented by the following formula'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key=cur_keys[4]\n",
    "value = txn.get(str(key).encode())\n",
    "input_ids = np.frombuffer(value, dtype=np.uint32)\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Calculate loss\n",
    "print(metadata.keys())\n",
    "loss_list=[]\n",
    "print(metadata['processed_seq_len'])\n",
    "for key in cur_keys:\n",
    "    value = txn.get(str(key).encode())\n",
    "    input_ids = np.frombuffer(value, dtype=np.uint32)\n",
    "    input_tensor = torch.from_numpy(input_ids.astype(np.int64)).unsqueeze(0).to(device)\n",
    "    labels = input_tensor.clone()\n",
    "    out = model.decoder(input_tensor, labels=labels)\n",
    "    input_tensor.to(\"cpu\")\n",
    "    labels.to(\"cpu\")\n",
    "    print(out.loss.cpu().item())\n",
    "    loss_list.append(out.loss.cpu().item())\n",
    "    out = None\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    del out\n",
    "print(sum(loss_list) / len(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(input_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"Football is a \", return_tensors=\"pt\")\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.decoder.generate(\n",
    "    input_ids=torch.tensor(input_tensor).to(device),\n",
    "    num_beams=5,\n",
    "    max_new_tokens=512,\n",
    "    num_return_sequences=1,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "res = tokenizer.decode(output.sequences[0], skip_special_tokens=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=torch.tensor(tokenizer.encode(\"Football is a \", return_tensors=\"pt\")).to(device)\n",
    "labels = input_ids.clone()\n",
    "out = model.decoder(input_ids,labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1014160/568723776.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids=torch.tensor(tokenizer.encode(\"The capital of China is\", return_tensors=\"pt\")).to(device),\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of China is located in the east of the country. The city is one of the largest cities in the world and it is also one of the most popular tourist destinations in the world. There are many things to see and do in Beijing. You can visit the Great Wall of China, the Forbidden City, the Temple of Heaven, the Summer Palace, the Old Summer Palace, the Drum Tower, the Bell Tower, the National Museum of China, the National Art Museum of China, the National Gallery of China, the National Library of China, the National Theatre of China, the National Astronomical Observatory of China, the National Zoological Park of China, the Beijing National Stadium, the Beijing National Aquatics Center, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing National Stadium, the Beijing\n"
     ]
    }
   ],
   "source": [
    "output = model.decoder.generate(\n",
    "    input_ids=torch.tensor(tokenizer.encode(\"The capital of China is\", return_tensors=\"pt\")).to(device),\n",
    "    num_beams=5,\n",
    "    max_new_tokens=512,\n",
    "    num_return_sequences=1,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "res = tokenizer.decode(output.sequences[0], skip_special_tokens=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296\n",
      "0.4630786909444912\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "a='''0.20717960596084595\n",
    "0.29078763723373413\n",
    "0.4763331115245819\n",
    "0.1211240291595459\n",
    "0.2743508517742157\n",
    "0.18720833957195282\n",
    "0.11175647377967834\n",
    "0.3087017238140106\n",
    "0.044937893748283386\n",
    "0.11803882569074631\n",
    "0.2398451566696167\n",
    "0.15379402041435242\n",
    "0.07513220608234406\n",
    "0.21486173570156097\n",
    "0.2845916152000427\n",
    "0.26695480942726135\n",
    "0.5750867128372192\n",
    "0.2512124478816986\n",
    "0.273259699344635\n",
    "0.06262169778347015\n",
    "0.2809830605983734\n",
    "0.12392335385084152\n",
    "0.5005030632019043\n",
    "0.6178805232048035\n",
    "0.15205508470535278\n",
    "1.3323503732681274\n",
    "1.335503339767456\n",
    "0.20509064197540283\n",
    "0.13405446708202362\n",
    "0.341273695230484\n",
    "0.21496140956878662\n",
    "0.396593302488327\n",
    "0.08487134426832199\n",
    "0.281646728515625\n",
    "1.3463149070739746\n",
    "0.9981168508529663\n",
    "0.04263443499803543\n",
    "1.2692070007324219\n",
    "0.5956265330314636\n",
    "0.19295325875282288\n",
    "0.5995656847953796\n",
    "0.21511414647102356\n",
    "1.4270107746124268\n",
    "1.320778489112854\n",
    "0.15251511335372925\n",
    "0.5639299154281616\n",
    "1.3715708255767822\n",
    "1.453153371810913\n",
    "0.4727802872657776\n",
    "1.2407070398330688\n",
    "0.5323099493980408\n",
    "0.1223621666431427\n",
    "0.5076682567596436\n",
    "1.0528452396392822\n",
    "1.0112085342407227\n",
    "0.14629696309566498\n",
    "0.1310725212097168\n",
    "0.07658973336219788\n",
    "0.08177613466978073\n",
    "1.1829582452774048\n",
    "0.8916467428207397\n",
    "0.11947863548994064\n",
    "0.423902690410614\n",
    "0.14153778553009033\n",
    "0.047447025775909424\n",
    "0.20104463398456573\n",
    "0.12043419480323792\n",
    "0.0807940736413002\n",
    "0.18193954229354858\n",
    "0.47314414381980896\n",
    "0.0679519921541214\n",
    "0.12400541454553604\n",
    "1.3700897693634033\n",
    "0.9072701930999756\n",
    "0.12065614014863968\n",
    "1.7163435220718384\n",
    "0.08381613343954086\n",
    "0.0326957032084465\n",
    "0.40456461906433105\n",
    "0.692696213722229\n",
    "0.3141028583049774\n",
    "0.2531353235244751\n",
    "0.2273268848657608\n",
    "0.13474857807159424\n",
    "0.563109278678894\n",
    "0.156013622879982\n",
    "1.658959150314331\n",
    "0.09070580452680588\n",
    "1.3941181898117065\n",
    "0.4297913610935211\n",
    "0.5922026038169861\n",
    "0.22258101403713226\n",
    "0.12181457132101059\n",
    "0.16902956366539001\n",
    "0.22528114914894104\n",
    "0.9651983976364136\n",
    "0.10283296555280685\n",
    "0.28349676728248596\n",
    "0.2935939133167267\n",
    "0.22655513882637024\n",
    "0.18640880286693573\n",
    "0.2510978579521179\n",
    "0.8008484840393066\n",
    "1.3282228708267212\n",
    "0.21326211094856262\n",
    "0.14087621867656708\n",
    "0.10235189646482468\n",
    "0.11559632420539856\n",
    "0.11227940768003464\n",
    "0.2077503502368927\n",
    "1.358059287071228\n",
    "0.7917405962944031\n",
    "0.22339721024036407\n",
    "1.3450703620910645\n",
    "0.630079984664917\n",
    "0.13973328471183777\n",
    "0.5261379480361938\n",
    "0.06727326661348343\n",
    "0.23248539865016937\n",
    "0.31089815497398376\n",
    "1.3583070039749146\n",
    "0.07996442168951035\n",
    "1.3741847276687622\n",
    "0.5316010117530823\n",
    "0.34544873237609863\n",
    "0.07100202143192291\n",
    "0.12925738096237183\n",
    "1.365248680114746\n",
    "0.18263107538223267\n",
    "0.36207520961761475\n",
    "1.3990753889083862\n",
    "0.30359530448913574\n",
    "0.09579917788505554\n",
    "0.030381273478269577\n",
    "0.19169947504997253\n",
    "0.03946821019053459\n",
    "0.24850626289844513\n",
    "0.17042821645736694\n",
    "0.7291426062583923\n",
    "0.22262592613697052\n",
    "0.06581363081932068\n",
    "0.7285424470901489\n",
    "0.392994225025177\n",
    "0.18774549663066864\n",
    "0.186307892203331\n",
    "0.5569478869438171\n",
    "0.6422935724258423\n",
    "0.31806761026382446\n",
    "0.10028895735740662\n",
    "1.0810362100601196\n",
    "0.20113618671894073\n",
    "0.5040784478187561\n",
    "1.3916536569595337\n",
    "0.6414272785186768\n",
    "0.1204301044344902\n",
    "1.2862085103988647\n",
    "0.24327492713928223\n",
    "0.21865741908550262\n",
    "0.6439153552055359\n",
    "0.24383366107940674\n",
    "0.31891173124313354\n",
    "0.15836231410503387\n",
    "0.10932537913322449\n",
    "0.14307968318462372\n",
    "0.12938232719898224\n",
    "0.09723549336194992\n",
    "0.5819553136825562\n",
    "0.8949821591377258\n",
    "1.3371659517288208\n",
    "0.8995532393455505\n",
    "0.18025264143943787\n",
    "0.20069092512130737\n",
    "0.18684826791286469\n",
    "0.2974279522895813\n",
    "0.9680314660072327\n",
    "0.19020363688468933\n",
    "0.5359116196632385\n",
    "0.09533888846635818\n",
    "0.12332974374294281\n",
    "0.3095950484275818\n",
    "0.25255653262138367\n",
    "0.15088067948818207\n",
    "0.18418022990226746\n",
    "0.25024962425231934\n",
    "1.6245007514953613\n",
    "0.25705811381340027\n",
    "1.2602794170379639\n",
    "0.2825852930545807\n",
    "0.4596610963344574\n",
    "0.27140161395072937\n",
    "1.3548699617385864\n",
    "1.6391781568527222\n",
    "0.21397431194782257\n",
    "0.32775214314460754\n",
    "0.08903037756681442\n",
    "0.15756107866764069\n",
    "1.3055980205535889\n",
    "0.03688358888030052\n",
    "1.3175513744354248\n",
    "0.08926389366388321\n",
    "0.15858736634254456\n",
    "0.31540006399154663\n",
    "0.28574520349502563\n",
    "0.36452287435531616\n",
    "0.10388889163732529\n",
    "0.23763996362686157\n",
    "0.09452733397483826\n",
    "0.23180389404296875\n",
    "0.23069925606250763\n",
    "0.6979408860206604\n",
    "0.10550756007432938\n",
    "0.4723520874977112\n",
    "0.14598198235034943\n",
    "0.1557895392179489\n",
    "0.6543782949447632\n",
    "0.4490549862384796\n",
    "0.103681780397892\n",
    "0.19329038262367249\n",
    "1.2767599821090698\n",
    "0.8332330584526062\n",
    "0.10007733106613159\n",
    "0.15582333505153656\n",
    "1.249241828918457\n",
    "0.517615020275116\n",
    "0.15385372936725616\n",
    "1.3441174030303955\n",
    "0.9407714009284973\n",
    "0.12658777832984924\n",
    "0.14014221727848053\n",
    "1.3540621995925903\n",
    "0.2357904464006424\n",
    "0.8998426795005798\n",
    "0.10065589100122452\n",
    "0.41297203302383423\n",
    "0.28044211864471436\n",
    "1.3492896556854248\n",
    "0.043217238038778305\n",
    "0.21052215993404388\n",
    "0.08350750803947449\n",
    "1.1624751091003418\n",
    "1.2785097360610962\n",
    "0.20286978781223297\n",
    "0.6564274430274963\n",
    "0.6118311285972595\n",
    "0.1830492466688156\n",
    "1.2313441038131714\n",
    "0.2561013996601105\n",
    "0.0793098732829094\n",
    "0.4527606666088104\n",
    "0.30490294098854065\n",
    "0.08304128795862198\n",
    "0.26137775182724\n",
    "0.5580241680145264\n",
    "0.6373624205589294\n",
    "0.25001731514930725\n",
    "0.1880124807357788\n",
    "0.24547462165355682\n",
    "0.7612487077713013\n",
    "0.3183324933052063\n",
    "0.11884171515703201\n",
    "0.07513443380594254\n",
    "0.0670829489827156\n",
    "0.38079485297203064\n",
    "0.34268122911453247\n",
    "0.2640489935874939\n",
    "0.2765796482563019\n",
    "1.6809512376785278\n",
    "0.17735382914543152\n",
    "0.11657076328992844\n",
    "0.31248295307159424\n",
    "0.3016645312309265\n",
    "0.5279101729393005\n",
    "0.11424760520458221\n",
    "0.6875072121620178\n",
    "1.610180139541626\n",
    "1.3809417486190796\n",
    "0.3671647310256958\n",
    "1.3125965595245361\n",
    "0.3456448018550873\n",
    "0.13240839540958405\n",
    "0.17399924993515015\n",
    "0.07215277850627899\n",
    "1.142114520072937\n",
    "1.7568718194961548\n",
    "0.09248217195272446\n",
    "0.28186503052711487\n",
    "0.069955013692379\n",
    "0.38316670060157776\n",
    "0.19083179533481598\n",
    "0.5279322266578674\n",
    "0.4683115482330322\n",
    "1.3078995943069458\n",
    "0.42920079827308655\n",
    "0.1557501256465912\n",
    "0.09976639598608017\n",
    "0.17055709660053253'''\n",
    "out = a.split('\\n')\n",
    "result=[]\n",
    "for item in out:\n",
    "    result.append(float(item))\n",
    "print(len(result))\n",
    "print(sum(result)/len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
