{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import mmap\n",
    "import re\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import transformers\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sfm.models.scigpt.scigpt import ScigptModel\n",
    "from sfm.models.scigpt.config import ScigptConfig\n",
    "from sfm.utils import arg_utils\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import multiprocessing as mp\n",
    "from sfm.utils.science_tokens import SCIENCE_TAG_TOKENS, SCIENCE_TOKENS\n",
    "\n",
    "from sfm.logging import logger\n",
    "\n",
    "import struct\n",
    "from multiprocessing import Lock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def get_args_and_tokenizer(use_llama=False):\n",
    "    parser = ArgumentParser()\n",
    "    cfg_classes = [ScigptConfig]\n",
    "    parser = arg_utils.add_dataclass_to_parser(cfg_classes, parser)\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.load_ckpt = False\n",
    "    args.strategy = \"DDP\"\n",
    "    args.encoder_layers = 33\n",
    "    args.encoder_embed_dim = 1280\n",
    "    args.encoder_ffn_embed_dim = 5120\n",
    "    args.encoder_attention_heads = 20\n",
    "    args.infer = True\n",
    "    args.bf16 = True\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/home/v-zekunguo/hai1data/llama/Meta-Llama-3-8B/original\")\n",
    "    args.save_dir = \"/home/v-zekunguo/hai1data/nlm/output/llama3_stageB/global_step1600/\"\n",
    "    args.save_dir = \"/home/v-zekunguo/hai1data/llama/Meta-Llama-3-8B/original\"\n",
    "    args.llm_model_name_or_path = \"/home/v-zekunguo/hai1data/llama/Meta-Llama-3-8B/original\"\n",
    "\n",
    "    special_tokens_dict = dict()\n",
    "    if tokenizer.pad_token is None:\n",
    "        special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "    if tokenizer.eos_token is None:\n",
    "        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "    if tokenizer.bos_token is None:\n",
    "        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "    if tokenizer.unk_token is None:\n",
    "        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "    # special_tokens_dict[\"additional_special_tokens\"] = SCIENCE_TAG_TOKENS\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    tokenizer.tag_re = re.compile(f'{\"|\".join(SCIENCE_TAG_TOKENS)}')\n",
    "    tokenizer.smiles_re = re.compile(\n",
    "        \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    )\n",
    "\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"pad_token\": \"[PAD]\",\n",
    "            \"unk_token\":\"<unk>\",\n",
    "        },\n",
    "\n",
    "    )\n",
    "\n",
    "    tokenizer.add_tokens(SCIENCE_TAG_TOKENS)\n",
    "    tokenizer.add_tokens(SCIENCE_TOKENS)\n",
    "    extra_tokens = []\n",
    "    # protein\n",
    "    for i in range(26):\n",
    "        extra_tokens.append(f\"<a>{chr(65 + i)}\")\n",
    "\n",
    "    # DNA, RNA, including ambiguous bases\n",
    "    for c in \"ACTGURYSWKMBDHVN\":\n",
    "        extra_tokens.append(f\"<d>{c}\")\n",
    "        extra_tokens.append(f\"<r>{c}\")\n",
    "\n",
    "    # materials, non-elements\n",
    "    for c in \"0123456789()+-\":\n",
    "        extra_tokens.append(f\"<i>{c}\")\n",
    "    for i in range(26):\n",
    "        extra_tokens.append(f\"<i>{chr(65 + i)}\")\n",
    "        extra_tokens.append(f\"<i>{chr(97 + i)}\")\n",
    "\n",
    "    tokenizer.add_tokens(extra_tokens)\n",
    "    tokenizer.split_special_tokens = True  # Ensure _tokenize() can access special tokens\n",
    "\n",
    "    logger.info(f\"Tokenizer has {len(tokenizer)} tokens\")\n",
    "\n",
    "    args.vocab_size=len(tokenizer)\n",
    "\n",
    "    return args, tokenizer\n",
    "\n",
    "args, tokenizer = get_args_and_tokenizer()\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.vocab_size=130304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dict: odict_keys(['decoder.model.embed_tokens.weight', 'decoder.model.layers.0.self_attn.q_proj.weight', 'decoder.model.layers.0.self_attn.k_proj.weight', 'decoder.model.layers.0.self_attn.v_proj.weight', 'decoder.model.layers.0.self_attn.o_proj.weight', 'decoder.model.layers.0.mlp.gate_proj.weight', 'decoder.model.layers.0.mlp.up_proj.weight', 'decoder.model.layers.0.mlp.down_proj.weight', 'decoder.model.layers.0.input_layernorm.weight', 'decoder.model.layers.0.post_attention_layernorm.weight', 'decoder.model.layers.1.self_attn.q_proj.weight', 'decoder.model.layers.1.self_attn.k_proj.weight', 'decoder.model.layers.1.self_attn.v_proj.weight', 'decoder.model.layers.1.self_attn.o_proj.weight', 'decoder.model.layers.1.mlp.gate_proj.weight', 'decoder.model.layers.1.mlp.up_proj.weight', 'decoder.model.layers.1.mlp.down_proj.weight', 'decoder.model.layers.1.input_layernorm.weight', 'decoder.model.layers.1.post_attention_layernorm.weight', 'decoder.model.layers.2.self_attn.q_proj.weight', 'decoder.model.layers.2.self_attn.k_proj.weight', 'decoder.model.layers.2.self_attn.v_proj.weight', 'decoder.model.layers.2.self_attn.o_proj.weight', 'decoder.model.layers.2.mlp.gate_proj.weight', 'decoder.model.layers.2.mlp.up_proj.weight', 'decoder.model.layers.2.mlp.down_proj.weight', 'decoder.model.layers.2.input_layernorm.weight', 'decoder.model.layers.2.post_attention_layernorm.weight', 'decoder.model.layers.3.self_attn.q_proj.weight', 'decoder.model.layers.3.self_attn.k_proj.weight', 'decoder.model.layers.3.self_attn.v_proj.weight', 'decoder.model.layers.3.self_attn.o_proj.weight', 'decoder.model.layers.3.mlp.gate_proj.weight', 'decoder.model.layers.3.mlp.up_proj.weight', 'decoder.model.layers.3.mlp.down_proj.weight', 'decoder.model.layers.3.input_layernorm.weight', 'decoder.model.layers.3.post_attention_layernorm.weight', 'decoder.model.layers.4.self_attn.q_proj.weight', 'decoder.model.layers.4.self_attn.k_proj.weight', 'decoder.model.layers.4.self_attn.v_proj.weight', 'decoder.model.layers.4.self_attn.o_proj.weight', 'decoder.model.layers.4.mlp.gate_proj.weight', 'decoder.model.layers.4.mlp.up_proj.weight', 'decoder.model.layers.4.mlp.down_proj.weight', 'decoder.model.layers.4.input_layernorm.weight', 'decoder.model.layers.4.post_attention_layernorm.weight', 'decoder.model.layers.5.self_attn.q_proj.weight', 'decoder.model.layers.5.self_attn.k_proj.weight', 'decoder.model.layers.5.self_attn.v_proj.weight', 'decoder.model.layers.5.self_attn.o_proj.weight', 'decoder.model.layers.5.mlp.gate_proj.weight', 'decoder.model.layers.5.mlp.up_proj.weight', 'decoder.model.layers.5.mlp.down_proj.weight', 'decoder.model.layers.5.input_layernorm.weight', 'decoder.model.layers.5.post_attention_layernorm.weight', 'decoder.model.layers.6.self_attn.q_proj.weight', 'decoder.model.layers.6.self_attn.k_proj.weight', 'decoder.model.layers.6.self_attn.v_proj.weight', 'decoder.model.layers.6.self_attn.o_proj.weight', 'decoder.model.layers.6.mlp.gate_proj.weight', 'decoder.model.layers.6.mlp.up_proj.weight', 'decoder.model.layers.6.mlp.down_proj.weight', 'decoder.model.layers.6.input_layernorm.weight', 'decoder.model.layers.6.post_attention_layernorm.weight', 'decoder.model.layers.7.self_attn.q_proj.weight', 'decoder.model.layers.7.self_attn.k_proj.weight', 'decoder.model.layers.7.self_attn.v_proj.weight', 'decoder.model.layers.7.self_attn.o_proj.weight', 'decoder.model.layers.7.mlp.gate_proj.weight', 'decoder.model.layers.7.mlp.up_proj.weight', 'decoder.model.layers.7.mlp.down_proj.weight', 'decoder.model.layers.7.input_layernorm.weight', 'decoder.model.layers.7.post_attention_layernorm.weight', 'decoder.model.layers.8.self_attn.q_proj.weight', 'decoder.model.layers.8.self_attn.k_proj.weight', 'decoder.model.layers.8.self_attn.v_proj.weight', 'decoder.model.layers.8.self_attn.o_proj.weight', 'decoder.model.layers.8.mlp.gate_proj.weight', 'decoder.model.layers.8.mlp.up_proj.weight', 'decoder.model.layers.8.mlp.down_proj.weight', 'decoder.model.layers.8.input_layernorm.weight', 'decoder.model.layers.8.post_attention_layernorm.weight', 'decoder.model.layers.9.self_attn.q_proj.weight', 'decoder.model.layers.9.self_attn.k_proj.weight', 'decoder.model.layers.9.self_attn.v_proj.weight', 'decoder.model.layers.9.self_attn.o_proj.weight', 'decoder.model.layers.9.mlp.gate_proj.weight', 'decoder.model.layers.9.mlp.up_proj.weight', 'decoder.model.layers.9.mlp.down_proj.weight', 'decoder.model.layers.9.input_layernorm.weight', 'decoder.model.layers.9.post_attention_layernorm.weight', 'decoder.model.layers.10.self_attn.q_proj.weight', 'decoder.model.layers.10.self_attn.k_proj.weight', 'decoder.model.layers.10.self_attn.v_proj.weight', 'decoder.model.layers.10.self_attn.o_proj.weight', 'decoder.model.layers.10.mlp.gate_proj.weight', 'decoder.model.layers.10.mlp.up_proj.weight', 'decoder.model.layers.10.mlp.down_proj.weight', 'decoder.model.layers.10.input_layernorm.weight', 'decoder.model.layers.10.post_attention_layernorm.weight', 'decoder.model.layers.11.self_attn.q_proj.weight', 'decoder.model.layers.11.self_attn.k_proj.weight', 'decoder.model.layers.11.self_attn.v_proj.weight', 'decoder.model.layers.11.self_attn.o_proj.weight', 'decoder.model.layers.11.mlp.gate_proj.weight', 'decoder.model.layers.11.mlp.up_proj.weight', 'decoder.model.layers.11.mlp.down_proj.weight', 'decoder.model.layers.11.input_layernorm.weight', 'decoder.model.layers.11.post_attention_layernorm.weight', 'decoder.model.layers.12.self_attn.q_proj.weight', 'decoder.model.layers.12.self_attn.k_proj.weight', 'decoder.model.layers.12.self_attn.v_proj.weight', 'decoder.model.layers.12.self_attn.o_proj.weight', 'decoder.model.layers.12.mlp.gate_proj.weight', 'decoder.model.layers.12.mlp.up_proj.weight', 'decoder.model.layers.12.mlp.down_proj.weight', 'decoder.model.layers.12.input_layernorm.weight', 'decoder.model.layers.12.post_attention_layernorm.weight', 'decoder.model.layers.13.self_attn.q_proj.weight', 'decoder.model.layers.13.self_attn.k_proj.weight', 'decoder.model.layers.13.self_attn.v_proj.weight', 'decoder.model.layers.13.self_attn.o_proj.weight', 'decoder.model.layers.13.mlp.gate_proj.weight', 'decoder.model.layers.13.mlp.up_proj.weight', 'decoder.model.layers.13.mlp.down_proj.weight', 'decoder.model.layers.13.input_layernorm.weight', 'decoder.model.layers.13.post_attention_layernorm.weight', 'decoder.model.layers.14.self_attn.q_proj.weight', 'decoder.model.layers.14.self_attn.k_proj.weight', 'decoder.model.layers.14.self_attn.v_proj.weight', 'decoder.model.layers.14.self_attn.o_proj.weight', 'decoder.model.layers.14.mlp.gate_proj.weight', 'decoder.model.layers.14.mlp.up_proj.weight', 'decoder.model.layers.14.mlp.down_proj.weight', 'decoder.model.layers.14.input_layernorm.weight', 'decoder.model.layers.14.post_attention_layernorm.weight', 'decoder.model.layers.15.self_attn.q_proj.weight', 'decoder.model.layers.15.self_attn.k_proj.weight', 'decoder.model.layers.15.self_attn.v_proj.weight', 'decoder.model.layers.15.self_attn.o_proj.weight', 'decoder.model.layers.15.mlp.gate_proj.weight', 'decoder.model.layers.15.mlp.up_proj.weight', 'decoder.model.layers.15.mlp.down_proj.weight', 'decoder.model.layers.15.input_layernorm.weight', 'decoder.model.layers.15.post_attention_layernorm.weight', 'decoder.model.layers.16.self_attn.q_proj.weight', 'decoder.model.layers.16.self_attn.k_proj.weight', 'decoder.model.layers.16.self_attn.v_proj.weight', 'decoder.model.layers.16.self_attn.o_proj.weight', 'decoder.model.layers.16.mlp.gate_proj.weight', 'decoder.model.layers.16.mlp.up_proj.weight', 'decoder.model.layers.16.mlp.down_proj.weight', 'decoder.model.layers.16.input_layernorm.weight', 'decoder.model.layers.16.post_attention_layernorm.weight', 'decoder.model.layers.17.self_attn.q_proj.weight', 'decoder.model.layers.17.self_attn.k_proj.weight', 'decoder.model.layers.17.self_attn.v_proj.weight', 'decoder.model.layers.17.self_attn.o_proj.weight', 'decoder.model.layers.17.mlp.gate_proj.weight', 'decoder.model.layers.17.mlp.up_proj.weight', 'decoder.model.layers.17.mlp.down_proj.weight', 'decoder.model.layers.17.input_layernorm.weight', 'decoder.model.layers.17.post_attention_layernorm.weight', 'decoder.model.layers.18.self_attn.q_proj.weight', 'decoder.model.layers.18.self_attn.k_proj.weight', 'decoder.model.layers.18.self_attn.v_proj.weight', 'decoder.model.layers.18.self_attn.o_proj.weight', 'decoder.model.layers.18.mlp.gate_proj.weight', 'decoder.model.layers.18.mlp.up_proj.weight', 'decoder.model.layers.18.mlp.down_proj.weight', 'decoder.model.layers.18.input_layernorm.weight', 'decoder.model.layers.18.post_attention_layernorm.weight', 'decoder.model.layers.19.self_attn.q_proj.weight', 'decoder.model.layers.19.self_attn.k_proj.weight', 'decoder.model.layers.19.self_attn.v_proj.weight', 'decoder.model.layers.19.self_attn.o_proj.weight', 'decoder.model.layers.19.mlp.gate_proj.weight', 'decoder.model.layers.19.mlp.up_proj.weight', 'decoder.model.layers.19.mlp.down_proj.weight', 'decoder.model.layers.19.input_layernorm.weight', 'decoder.model.layers.19.post_attention_layernorm.weight', 'decoder.model.layers.20.self_attn.q_proj.weight', 'decoder.model.layers.20.self_attn.k_proj.weight', 'decoder.model.layers.20.self_attn.v_proj.weight', 'decoder.model.layers.20.self_attn.o_proj.weight', 'decoder.model.layers.20.mlp.gate_proj.weight', 'decoder.model.layers.20.mlp.up_proj.weight', 'decoder.model.layers.20.mlp.down_proj.weight', 'decoder.model.layers.20.input_layernorm.weight', 'decoder.model.layers.20.post_attention_layernorm.weight', 'decoder.model.layers.21.self_attn.q_proj.weight', 'decoder.model.layers.21.self_attn.k_proj.weight', 'decoder.model.layers.21.self_attn.v_proj.weight', 'decoder.model.layers.21.self_attn.o_proj.weight', 'decoder.model.layers.21.mlp.gate_proj.weight', 'decoder.model.layers.21.mlp.up_proj.weight', 'decoder.model.layers.21.mlp.down_proj.weight', 'decoder.model.layers.21.input_layernorm.weight', 'decoder.model.layers.21.post_attention_layernorm.weight', 'decoder.model.layers.22.self_attn.q_proj.weight', 'decoder.model.layers.22.self_attn.k_proj.weight', 'decoder.model.layers.22.self_attn.v_proj.weight', 'decoder.model.layers.22.self_attn.o_proj.weight', 'decoder.model.layers.22.mlp.gate_proj.weight', 'decoder.model.layers.22.mlp.up_proj.weight', 'decoder.model.layers.22.mlp.down_proj.weight', 'decoder.model.layers.22.input_layernorm.weight', 'decoder.model.layers.22.post_attention_layernorm.weight', 'decoder.model.layers.23.self_attn.q_proj.weight', 'decoder.model.layers.23.self_attn.k_proj.weight', 'decoder.model.layers.23.self_attn.v_proj.weight', 'decoder.model.layers.23.self_attn.o_proj.weight', 'decoder.model.layers.23.mlp.gate_proj.weight', 'decoder.model.layers.23.mlp.up_proj.weight', 'decoder.model.layers.23.mlp.down_proj.weight', 'decoder.model.layers.23.input_layernorm.weight', 'decoder.model.layers.23.post_attention_layernorm.weight', 'decoder.model.layers.24.self_attn.q_proj.weight', 'decoder.model.layers.24.self_attn.k_proj.weight', 'decoder.model.layers.24.self_attn.v_proj.weight', 'decoder.model.layers.24.self_attn.o_proj.weight', 'decoder.model.layers.24.mlp.gate_proj.weight', 'decoder.model.layers.24.mlp.up_proj.weight', 'decoder.model.layers.24.mlp.down_proj.weight', 'decoder.model.layers.24.input_layernorm.weight', 'decoder.model.layers.24.post_attention_layernorm.weight', 'decoder.model.layers.25.self_attn.q_proj.weight', 'decoder.model.layers.25.self_attn.k_proj.weight', 'decoder.model.layers.25.self_attn.v_proj.weight', 'decoder.model.layers.25.self_attn.o_proj.weight', 'decoder.model.layers.25.mlp.gate_proj.weight', 'decoder.model.layers.25.mlp.up_proj.weight', 'decoder.model.layers.25.mlp.down_proj.weight', 'decoder.model.layers.25.input_layernorm.weight', 'decoder.model.layers.25.post_attention_layernorm.weight', 'decoder.model.layers.26.self_attn.q_proj.weight', 'decoder.model.layers.26.self_attn.k_proj.weight', 'decoder.model.layers.26.self_attn.v_proj.weight', 'decoder.model.layers.26.self_attn.o_proj.weight', 'decoder.model.layers.26.mlp.gate_proj.weight', 'decoder.model.layers.26.mlp.up_proj.weight', 'decoder.model.layers.26.mlp.down_proj.weight', 'decoder.model.layers.26.input_layernorm.weight', 'decoder.model.layers.26.post_attention_layernorm.weight', 'decoder.model.layers.27.self_attn.q_proj.weight', 'decoder.model.layers.27.self_attn.k_proj.weight', 'decoder.model.layers.27.self_attn.v_proj.weight', 'decoder.model.layers.27.self_attn.o_proj.weight', 'decoder.model.layers.27.mlp.gate_proj.weight', 'decoder.model.layers.27.mlp.up_proj.weight', 'decoder.model.layers.27.mlp.down_proj.weight', 'decoder.model.layers.27.input_layernorm.weight', 'decoder.model.layers.27.post_attention_layernorm.weight', 'decoder.model.layers.28.self_attn.q_proj.weight', 'decoder.model.layers.28.self_attn.k_proj.weight', 'decoder.model.layers.28.self_attn.v_proj.weight', 'decoder.model.layers.28.self_attn.o_proj.weight', 'decoder.model.layers.28.mlp.gate_proj.weight', 'decoder.model.layers.28.mlp.up_proj.weight', 'decoder.model.layers.28.mlp.down_proj.weight', 'decoder.model.layers.28.input_layernorm.weight', 'decoder.model.layers.28.post_attention_layernorm.weight', 'decoder.model.layers.29.self_attn.q_proj.weight', 'decoder.model.layers.29.self_attn.k_proj.weight', 'decoder.model.layers.29.self_attn.v_proj.weight', 'decoder.model.layers.29.self_attn.o_proj.weight', 'decoder.model.layers.29.mlp.gate_proj.weight', 'decoder.model.layers.29.mlp.up_proj.weight', 'decoder.model.layers.29.mlp.down_proj.weight', 'decoder.model.layers.29.input_layernorm.weight', 'decoder.model.layers.29.post_attention_layernorm.weight', 'decoder.model.layers.30.self_attn.q_proj.weight', 'decoder.model.layers.30.self_attn.k_proj.weight', 'decoder.model.layers.30.self_attn.v_proj.weight', 'decoder.model.layers.30.self_attn.o_proj.weight', 'decoder.model.layers.30.mlp.gate_proj.weight', 'decoder.model.layers.30.mlp.up_proj.weight', 'decoder.model.layers.30.mlp.down_proj.weight', 'decoder.model.layers.30.input_layernorm.weight', 'decoder.model.layers.30.post_attention_layernorm.weight', 'decoder.model.layers.31.self_attn.q_proj.weight', 'decoder.model.layers.31.self_attn.k_proj.weight', 'decoder.model.layers.31.self_attn.v_proj.weight', 'decoder.model.layers.31.self_attn.o_proj.weight', 'decoder.model.layers.31.mlp.gate_proj.weight', 'decoder.model.layers.31.mlp.up_proj.weight', 'decoder.model.layers.31.mlp.down_proj.weight', 'decoder.model.layers.31.input_layernorm.weight', 'decoder.model.layers.31.post_attention_layernorm.weight', 'decoder.model.norm.weight', 'decoder.lm_head.weight'])\n",
      "torch.Size([14336, 4096])\n",
      "torch.Size([14336, 4096])\n",
      "ckpt_dict: dict_keys(['decoder.model.embed_tokens.weight', 'decoder.model.layers.0.input_layernorm.weight', 'decoder.model.layers.0.self_attn.q_proj.weight', 'decoder.model.layers.0.self_attn.k_proj.weight', 'decoder.model.layers.0.self_attn.v_proj.weight', 'decoder.model.layers.0.self_attn.o_proj.weight', 'decoder.model.layers.0.post_attention_layernorm.weight', 'decoder.model.layers.0.mlp.gate_proj.weight', 'decoder.model.layers.0.mlp.up_proj.weight', 'decoder.model.layers.0.mlp.down_proj.weight', 'decoder.model.layers.1.input_layernorm.weight', 'decoder.model.layers.1.self_attn.q_proj.weight', 'decoder.model.layers.1.self_attn.k_proj.weight', 'decoder.model.layers.1.self_attn.v_proj.weight', 'decoder.model.layers.1.self_attn.o_proj.weight', 'decoder.model.layers.1.post_attention_layernorm.weight', 'decoder.model.layers.1.mlp.gate_proj.weight', 'decoder.model.layers.1.mlp.up_proj.weight', 'decoder.model.layers.1.mlp.down_proj.weight', 'decoder.model.layers.2.input_layernorm.weight', 'decoder.model.layers.2.self_attn.q_proj.weight', 'decoder.model.layers.2.self_attn.k_proj.weight', 'decoder.model.layers.2.self_attn.v_proj.weight', 'decoder.model.layers.2.self_attn.o_proj.weight', 'decoder.model.layers.2.post_attention_layernorm.weight', 'decoder.model.layers.2.mlp.gate_proj.weight', 'decoder.model.layers.2.mlp.up_proj.weight', 'decoder.model.layers.2.mlp.down_proj.weight', 'decoder.model.layers.3.input_layernorm.weight', 'decoder.model.layers.3.self_attn.q_proj.weight', 'decoder.model.layers.3.self_attn.k_proj.weight', 'decoder.model.layers.3.self_attn.v_proj.weight', 'decoder.model.layers.3.self_attn.o_proj.weight', 'decoder.model.layers.3.post_attention_layernorm.weight', 'decoder.model.layers.3.mlp.gate_proj.weight', 'decoder.model.layers.3.mlp.up_proj.weight', 'decoder.model.layers.3.mlp.down_proj.weight', 'decoder.model.layers.4.input_layernorm.weight', 'decoder.model.layers.4.self_attn.q_proj.weight', 'decoder.model.layers.4.self_attn.k_proj.weight', 'decoder.model.layers.4.self_attn.v_proj.weight', 'decoder.model.layers.4.self_attn.o_proj.weight', 'decoder.model.layers.4.post_attention_layernorm.weight', 'decoder.model.layers.4.mlp.gate_proj.weight', 'decoder.model.layers.4.mlp.up_proj.weight', 'decoder.model.layers.4.mlp.down_proj.weight', 'decoder.model.layers.5.input_layernorm.weight', 'decoder.model.layers.5.self_attn.q_proj.weight', 'decoder.model.layers.5.self_attn.k_proj.weight', 'decoder.model.layers.5.self_attn.v_proj.weight', 'decoder.model.layers.5.self_attn.o_proj.weight', 'decoder.model.layers.5.post_attention_layernorm.weight', 'decoder.model.layers.5.mlp.gate_proj.weight', 'decoder.model.layers.5.mlp.up_proj.weight', 'decoder.model.layers.5.mlp.down_proj.weight', 'decoder.model.layers.6.input_layernorm.weight', 'decoder.model.layers.6.self_attn.q_proj.weight', 'decoder.model.layers.6.self_attn.k_proj.weight', 'decoder.model.layers.6.self_attn.v_proj.weight', 'decoder.model.layers.6.self_attn.o_proj.weight', 'decoder.model.layers.6.post_attention_layernorm.weight', 'decoder.model.layers.6.mlp.gate_proj.weight', 'decoder.model.layers.6.mlp.up_proj.weight', 'decoder.model.layers.6.mlp.down_proj.weight', 'decoder.model.layers.7.input_layernorm.weight', 'decoder.model.layers.7.self_attn.q_proj.weight', 'decoder.model.layers.7.self_attn.k_proj.weight', 'decoder.model.layers.7.self_attn.v_proj.weight', 'decoder.model.layers.7.self_attn.o_proj.weight', 'decoder.model.layers.7.post_attention_layernorm.weight', 'decoder.model.layers.7.mlp.gate_proj.weight', 'decoder.model.layers.7.mlp.up_proj.weight', 'decoder.model.layers.7.mlp.down_proj.weight', 'decoder.model.layers.8.input_layernorm.weight', 'decoder.model.layers.8.self_attn.q_proj.weight', 'decoder.model.layers.8.self_attn.k_proj.weight', 'decoder.model.layers.8.self_attn.v_proj.weight', 'decoder.model.layers.8.self_attn.o_proj.weight', 'decoder.model.layers.8.post_attention_layernorm.weight', 'decoder.model.layers.8.mlp.gate_proj.weight', 'decoder.model.layers.8.mlp.up_proj.weight', 'decoder.model.layers.8.mlp.down_proj.weight', 'decoder.model.layers.9.input_layernorm.weight', 'decoder.model.layers.9.self_attn.q_proj.weight', 'decoder.model.layers.9.self_attn.k_proj.weight', 'decoder.model.layers.9.self_attn.v_proj.weight', 'decoder.model.layers.9.self_attn.o_proj.weight', 'decoder.model.layers.9.post_attention_layernorm.weight', 'decoder.model.layers.9.mlp.gate_proj.weight', 'decoder.model.layers.9.mlp.up_proj.weight', 'decoder.model.layers.9.mlp.down_proj.weight', 'decoder.model.layers.10.input_layernorm.weight', 'decoder.model.layers.10.self_attn.q_proj.weight', 'decoder.model.layers.10.self_attn.k_proj.weight', 'decoder.model.layers.10.self_attn.v_proj.weight', 'decoder.model.layers.10.self_attn.o_proj.weight', 'decoder.model.layers.10.post_attention_layernorm.weight', 'decoder.model.layers.10.mlp.gate_proj.weight', 'decoder.model.layers.10.mlp.up_proj.weight', 'decoder.model.layers.10.mlp.down_proj.weight', 'decoder.model.layers.11.input_layernorm.weight', 'decoder.model.layers.11.self_attn.q_proj.weight', 'decoder.model.layers.11.self_attn.k_proj.weight', 'decoder.model.layers.11.self_attn.v_proj.weight', 'decoder.model.layers.11.self_attn.o_proj.weight', 'decoder.model.layers.11.post_attention_layernorm.weight', 'decoder.model.layers.11.mlp.gate_proj.weight', 'decoder.model.layers.11.mlp.up_proj.weight', 'decoder.model.layers.11.mlp.down_proj.weight', 'decoder.model.layers.12.input_layernorm.weight', 'decoder.model.layers.12.self_attn.q_proj.weight', 'decoder.model.layers.12.self_attn.k_proj.weight', 'decoder.model.layers.12.self_attn.v_proj.weight', 'decoder.model.layers.12.self_attn.o_proj.weight', 'decoder.model.layers.12.post_attention_layernorm.weight', 'decoder.model.layers.12.mlp.gate_proj.weight', 'decoder.model.layers.12.mlp.up_proj.weight', 'decoder.model.layers.12.mlp.down_proj.weight', 'decoder.model.layers.13.input_layernorm.weight', 'decoder.model.layers.13.self_attn.q_proj.weight', 'decoder.model.layers.13.self_attn.k_proj.weight', 'decoder.model.layers.13.self_attn.v_proj.weight', 'decoder.model.layers.13.self_attn.o_proj.weight', 'decoder.model.layers.13.post_attention_layernorm.weight', 'decoder.model.layers.13.mlp.gate_proj.weight', 'decoder.model.layers.13.mlp.up_proj.weight', 'decoder.model.layers.13.mlp.down_proj.weight', 'decoder.model.layers.14.input_layernorm.weight', 'decoder.model.layers.14.self_attn.q_proj.weight', 'decoder.model.layers.14.self_attn.k_proj.weight', 'decoder.model.layers.14.self_attn.v_proj.weight', 'decoder.model.layers.14.self_attn.o_proj.weight', 'decoder.model.layers.14.post_attention_layernorm.weight', 'decoder.model.layers.14.mlp.gate_proj.weight', 'decoder.model.layers.14.mlp.up_proj.weight', 'decoder.model.layers.14.mlp.down_proj.weight', 'decoder.model.layers.15.input_layernorm.weight', 'decoder.model.layers.15.self_attn.q_proj.weight', 'decoder.model.layers.15.self_attn.k_proj.weight', 'decoder.model.layers.15.self_attn.v_proj.weight', 'decoder.model.layers.15.self_attn.o_proj.weight', 'decoder.model.layers.15.post_attention_layernorm.weight', 'decoder.model.layers.15.mlp.gate_proj.weight', 'decoder.model.layers.15.mlp.up_proj.weight', 'decoder.model.layers.15.mlp.down_proj.weight', 'decoder.model.layers.16.input_layernorm.weight', 'decoder.model.layers.16.self_attn.q_proj.weight', 'decoder.model.layers.16.self_attn.k_proj.weight', 'decoder.model.layers.16.self_attn.v_proj.weight', 'decoder.model.layers.16.self_attn.o_proj.weight', 'decoder.model.layers.16.post_attention_layernorm.weight', 'decoder.model.layers.16.mlp.gate_proj.weight', 'decoder.model.layers.16.mlp.up_proj.weight', 'decoder.model.layers.16.mlp.down_proj.weight', 'decoder.model.layers.17.input_layernorm.weight', 'decoder.model.layers.17.self_attn.q_proj.weight', 'decoder.model.layers.17.self_attn.k_proj.weight', 'decoder.model.layers.17.self_attn.v_proj.weight', 'decoder.model.layers.17.self_attn.o_proj.weight', 'decoder.model.layers.17.post_attention_layernorm.weight', 'decoder.model.layers.17.mlp.gate_proj.weight', 'decoder.model.layers.17.mlp.up_proj.weight', 'decoder.model.layers.17.mlp.down_proj.weight', 'decoder.model.layers.18.input_layernorm.weight', 'decoder.model.layers.18.self_attn.q_proj.weight', 'decoder.model.layers.18.self_attn.k_proj.weight', 'decoder.model.layers.18.self_attn.v_proj.weight', 'decoder.model.layers.18.self_attn.o_proj.weight', 'decoder.model.layers.18.post_attention_layernorm.weight', 'decoder.model.layers.18.mlp.gate_proj.weight', 'decoder.model.layers.18.mlp.up_proj.weight', 'decoder.model.layers.18.mlp.down_proj.weight', 'decoder.model.layers.19.input_layernorm.weight', 'decoder.model.layers.19.self_attn.q_proj.weight', 'decoder.model.layers.19.self_attn.k_proj.weight', 'decoder.model.layers.19.self_attn.v_proj.weight', 'decoder.model.layers.19.self_attn.o_proj.weight', 'decoder.model.layers.19.post_attention_layernorm.weight', 'decoder.model.layers.19.mlp.gate_proj.weight', 'decoder.model.layers.19.mlp.up_proj.weight', 'decoder.model.layers.19.mlp.down_proj.weight', 'decoder.model.layers.20.input_layernorm.weight', 'decoder.model.layers.20.self_attn.q_proj.weight', 'decoder.model.layers.20.self_attn.k_proj.weight', 'decoder.model.layers.20.self_attn.v_proj.weight', 'decoder.model.layers.20.self_attn.o_proj.weight', 'decoder.model.layers.20.post_attention_layernorm.weight', 'decoder.model.layers.20.mlp.gate_proj.weight', 'decoder.model.layers.20.mlp.up_proj.weight', 'decoder.model.layers.20.mlp.down_proj.weight', 'decoder.model.layers.21.input_layernorm.weight', 'decoder.model.layers.21.self_attn.q_proj.weight', 'decoder.model.layers.21.self_attn.k_proj.weight', 'decoder.model.layers.21.self_attn.v_proj.weight', 'decoder.model.layers.21.self_attn.o_proj.weight', 'decoder.model.layers.21.post_attention_layernorm.weight', 'decoder.model.layers.21.mlp.gate_proj.weight', 'decoder.model.layers.21.mlp.up_proj.weight', 'decoder.model.layers.21.mlp.down_proj.weight', 'decoder.model.layers.22.input_layernorm.weight', 'decoder.model.layers.22.self_attn.q_proj.weight', 'decoder.model.layers.22.self_attn.k_proj.weight', 'decoder.model.layers.22.self_attn.v_proj.weight', 'decoder.model.layers.22.self_attn.o_proj.weight', 'decoder.model.layers.22.post_attention_layernorm.weight', 'decoder.model.layers.22.mlp.gate_proj.weight', 'decoder.model.layers.22.mlp.up_proj.weight', 'decoder.model.layers.22.mlp.down_proj.weight', 'decoder.model.layers.23.input_layernorm.weight', 'decoder.model.layers.23.self_attn.q_proj.weight', 'decoder.model.layers.23.self_attn.k_proj.weight', 'decoder.model.layers.23.self_attn.v_proj.weight', 'decoder.model.layers.23.self_attn.o_proj.weight', 'decoder.model.layers.23.post_attention_layernorm.weight', 'decoder.model.layers.23.mlp.gate_proj.weight', 'decoder.model.layers.23.mlp.up_proj.weight', 'decoder.model.layers.23.mlp.down_proj.weight', 'decoder.model.layers.24.input_layernorm.weight', 'decoder.model.layers.24.self_attn.q_proj.weight', 'decoder.model.layers.24.self_attn.k_proj.weight', 'decoder.model.layers.24.self_attn.v_proj.weight', 'decoder.model.layers.24.self_attn.o_proj.weight', 'decoder.model.layers.24.post_attention_layernorm.weight', 'decoder.model.layers.24.mlp.gate_proj.weight', 'decoder.model.layers.24.mlp.up_proj.weight', 'decoder.model.layers.24.mlp.down_proj.weight', 'decoder.model.layers.25.input_layernorm.weight', 'decoder.model.layers.25.self_attn.q_proj.weight', 'decoder.model.layers.25.self_attn.k_proj.weight', 'decoder.model.layers.25.self_attn.v_proj.weight', 'decoder.model.layers.25.self_attn.o_proj.weight', 'decoder.model.layers.25.post_attention_layernorm.weight', 'decoder.model.layers.25.mlp.gate_proj.weight', 'decoder.model.layers.25.mlp.up_proj.weight', 'decoder.model.layers.25.mlp.down_proj.weight', 'decoder.model.layers.26.input_layernorm.weight', 'decoder.model.layers.26.self_attn.q_proj.weight', 'decoder.model.layers.26.self_attn.k_proj.weight', 'decoder.model.layers.26.self_attn.v_proj.weight', 'decoder.model.layers.26.self_attn.o_proj.weight', 'decoder.model.layers.26.post_attention_layernorm.weight', 'decoder.model.layers.26.mlp.gate_proj.weight', 'decoder.model.layers.26.mlp.up_proj.weight', 'decoder.model.layers.26.mlp.down_proj.weight', 'decoder.model.layers.27.input_layernorm.weight', 'decoder.model.layers.27.self_attn.q_proj.weight', 'decoder.model.layers.27.self_attn.k_proj.weight', 'decoder.model.layers.27.self_attn.v_proj.weight', 'decoder.model.layers.27.self_attn.o_proj.weight', 'decoder.model.layers.27.post_attention_layernorm.weight', 'decoder.model.layers.27.mlp.gate_proj.weight', 'decoder.model.layers.27.mlp.up_proj.weight', 'decoder.model.layers.27.mlp.down_proj.weight', 'decoder.model.layers.28.input_layernorm.weight', 'decoder.model.layers.28.self_attn.q_proj.weight', 'decoder.model.layers.28.self_attn.k_proj.weight', 'decoder.model.layers.28.self_attn.v_proj.weight', 'decoder.model.layers.28.self_attn.o_proj.weight', 'decoder.model.layers.28.post_attention_layernorm.weight', 'decoder.model.layers.28.mlp.gate_proj.weight', 'decoder.model.layers.28.mlp.up_proj.weight', 'decoder.model.layers.28.mlp.down_proj.weight', 'decoder.model.layers.29.input_layernorm.weight', 'decoder.model.layers.29.self_attn.q_proj.weight', 'decoder.model.layers.29.self_attn.k_proj.weight', 'decoder.model.layers.29.self_attn.v_proj.weight', 'decoder.model.layers.29.self_attn.o_proj.weight', 'decoder.model.layers.29.post_attention_layernorm.weight', 'decoder.model.layers.29.mlp.gate_proj.weight', 'decoder.model.layers.29.mlp.up_proj.weight', 'decoder.model.layers.29.mlp.down_proj.weight', 'decoder.model.layers.30.input_layernorm.weight', 'decoder.model.layers.30.self_attn.q_proj.weight', 'decoder.model.layers.30.self_attn.k_proj.weight', 'decoder.model.layers.30.self_attn.v_proj.weight', 'decoder.model.layers.30.self_attn.o_proj.weight', 'decoder.model.layers.30.post_attention_layernorm.weight', 'decoder.model.layers.30.mlp.gate_proj.weight', 'decoder.model.layers.30.mlp.up_proj.weight', 'decoder.model.layers.30.mlp.down_proj.weight', 'decoder.model.layers.31.input_layernorm.weight', 'decoder.model.layers.31.self_attn.q_proj.weight', 'decoder.model.layers.31.self_attn.k_proj.weight', 'decoder.model.layers.31.self_attn.v_proj.weight', 'decoder.model.layers.31.self_attn.o_proj.weight', 'decoder.model.layers.31.post_attention_layernorm.weight', 'decoder.model.layers.31.mlp.gate_proj.weight', 'decoder.model.layers.31.mlp.up_proj.weight', 'decoder.model.layers.31.mlp.down_proj.weight', 'decoder.model.norm.weight', 'decoder.lm_head.weight'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the extended trained model\n",
    "ckpt_dict = {}\n",
    "\n",
    "model = ScigptModel(args)\n",
    "model.decoder.resize_token_embeddings(args.vocab_size)\n",
    "model_dict = model.state_dict()\n",
    "print(f\"model_dict: {model_dict.keys()}\")\n",
    "print(model_dict['decoder.model.layers.0.mlp.gate_proj.weight'].shape)\n",
    "print(model_dict['decoder.model.layers.0.mlp.up_proj.weight'].shape)\n",
    "weight1_size=model_dict['decoder.model.layers.0.mlp.gate_proj.weight'].size(0)\n",
    "weight2_size=model_dict['decoder.model.layers.0.mlp.up_proj.weight'].size(0)\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_00-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer0.items():\n",
    "    if k=='word_embeddings.weight':\n",
    "        ckpt_dict['decoder.model.embed_tokens.weight'] = v\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 1).zfill(2)\n",
    "    layer = torch.load(os.path.join(args.save_dir, f\"layer_{l_index}-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        if k==\"self_attention.layernorm_qkv.layer_norm_weight\":\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.input_layernorm.weight\"] = layer[k]\n",
    "        elif k=='self_attention.layernorm_qkv.query_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.self_attn.q_proj.weight\"] = layer[k]\n",
    "        elif k=='self_attention.layernorm_qkv.key_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.self_attn.k_proj.weight\"] = layer[k]\n",
    "        elif k=='self_attention.layernorm_qkv.value_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.self_attn.v_proj.weight\"] = layer[k]\n",
    "        elif k=='self_attention.proj.weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.self_attn.o_proj.weight\"] = layer[k]\n",
    "        elif k=='layernorm_mlp.layer_norm_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.post_attention_layernorm.weight\"] = layer[k]\n",
    "        elif k=='layernorm_mlp.fc1_weight':\n",
    "            weight1,weight2=torch.split(layer[k], [weight1_size, weight2_size], dim=0)\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.mlp.gate_proj.weight\"] = weight1\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.mlp.up_proj.weight\"] = weight2\n",
    "        elif k=='layernorm_mlp.fc2_weight':\n",
    "            ckpt_dict[f\"decoder.model.layers.{l}.mlp.down_proj.weight\"] = layer[k]\n",
    "    del layer\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_33-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_34-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "\n",
    "print(f\"ckpt_dict: {ckpt_dict.keys()}\")\n",
    "model_dict.update(ckpt_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dict: odict_keys(['decoder.model.embed_tokens.weight', 'decoder.model.layers.0.self_attn.q_proj.weight', 'decoder.model.layers.0.self_attn.k_proj.weight', 'decoder.model.layers.0.self_attn.v_proj.weight', 'decoder.model.layers.0.self_attn.o_proj.weight', 'decoder.model.layers.0.mlp.gate_proj.weight', 'decoder.model.layers.0.mlp.up_proj.weight', 'decoder.model.layers.0.mlp.down_proj.weight', 'decoder.model.layers.0.input_layernorm.weight', 'decoder.model.layers.0.post_attention_layernorm.weight', 'decoder.model.layers.1.self_attn.q_proj.weight', 'decoder.model.layers.1.self_attn.k_proj.weight', 'decoder.model.layers.1.self_attn.v_proj.weight', 'decoder.model.layers.1.self_attn.o_proj.weight', 'decoder.model.layers.1.mlp.gate_proj.weight', 'decoder.model.layers.1.mlp.up_proj.weight', 'decoder.model.layers.1.mlp.down_proj.weight', 'decoder.model.layers.1.input_layernorm.weight', 'decoder.model.layers.1.post_attention_layernorm.weight', 'decoder.model.layers.2.self_attn.q_proj.weight', 'decoder.model.layers.2.self_attn.k_proj.weight', 'decoder.model.layers.2.self_attn.v_proj.weight', 'decoder.model.layers.2.self_attn.o_proj.weight', 'decoder.model.layers.2.mlp.gate_proj.weight', 'decoder.model.layers.2.mlp.up_proj.weight', 'decoder.model.layers.2.mlp.down_proj.weight', 'decoder.model.layers.2.input_layernorm.weight', 'decoder.model.layers.2.post_attention_layernorm.weight', 'decoder.model.layers.3.self_attn.q_proj.weight', 'decoder.model.layers.3.self_attn.k_proj.weight', 'decoder.model.layers.3.self_attn.v_proj.weight', 'decoder.model.layers.3.self_attn.o_proj.weight', 'decoder.model.layers.3.mlp.gate_proj.weight', 'decoder.model.layers.3.mlp.up_proj.weight', 'decoder.model.layers.3.mlp.down_proj.weight', 'decoder.model.layers.3.input_layernorm.weight', 'decoder.model.layers.3.post_attention_layernorm.weight', 'decoder.model.layers.4.self_attn.q_proj.weight', 'decoder.model.layers.4.self_attn.k_proj.weight', 'decoder.model.layers.4.self_attn.v_proj.weight', 'decoder.model.layers.4.self_attn.o_proj.weight', 'decoder.model.layers.4.mlp.gate_proj.weight', 'decoder.model.layers.4.mlp.up_proj.weight', 'decoder.model.layers.4.mlp.down_proj.weight', 'decoder.model.layers.4.input_layernorm.weight', 'decoder.model.layers.4.post_attention_layernorm.weight', 'decoder.model.layers.5.self_attn.q_proj.weight', 'decoder.model.layers.5.self_attn.k_proj.weight', 'decoder.model.layers.5.self_attn.v_proj.weight', 'decoder.model.layers.5.self_attn.o_proj.weight', 'decoder.model.layers.5.mlp.gate_proj.weight', 'decoder.model.layers.5.mlp.up_proj.weight', 'decoder.model.layers.5.mlp.down_proj.weight', 'decoder.model.layers.5.input_layernorm.weight', 'decoder.model.layers.5.post_attention_layernorm.weight', 'decoder.model.layers.6.self_attn.q_proj.weight', 'decoder.model.layers.6.self_attn.k_proj.weight', 'decoder.model.layers.6.self_attn.v_proj.weight', 'decoder.model.layers.6.self_attn.o_proj.weight', 'decoder.model.layers.6.mlp.gate_proj.weight', 'decoder.model.layers.6.mlp.up_proj.weight', 'decoder.model.layers.6.mlp.down_proj.weight', 'decoder.model.layers.6.input_layernorm.weight', 'decoder.model.layers.6.post_attention_layernorm.weight', 'decoder.model.layers.7.self_attn.q_proj.weight', 'decoder.model.layers.7.self_attn.k_proj.weight', 'decoder.model.layers.7.self_attn.v_proj.weight', 'decoder.model.layers.7.self_attn.o_proj.weight', 'decoder.model.layers.7.mlp.gate_proj.weight', 'decoder.model.layers.7.mlp.up_proj.weight', 'decoder.model.layers.7.mlp.down_proj.weight', 'decoder.model.layers.7.input_layernorm.weight', 'decoder.model.layers.7.post_attention_layernorm.weight', 'decoder.model.layers.8.self_attn.q_proj.weight', 'decoder.model.layers.8.self_attn.k_proj.weight', 'decoder.model.layers.8.self_attn.v_proj.weight', 'decoder.model.layers.8.self_attn.o_proj.weight', 'decoder.model.layers.8.mlp.gate_proj.weight', 'decoder.model.layers.8.mlp.up_proj.weight', 'decoder.model.layers.8.mlp.down_proj.weight', 'decoder.model.layers.8.input_layernorm.weight', 'decoder.model.layers.8.post_attention_layernorm.weight', 'decoder.model.layers.9.self_attn.q_proj.weight', 'decoder.model.layers.9.self_attn.k_proj.weight', 'decoder.model.layers.9.self_attn.v_proj.weight', 'decoder.model.layers.9.self_attn.o_proj.weight', 'decoder.model.layers.9.mlp.gate_proj.weight', 'decoder.model.layers.9.mlp.up_proj.weight', 'decoder.model.layers.9.mlp.down_proj.weight', 'decoder.model.layers.9.input_layernorm.weight', 'decoder.model.layers.9.post_attention_layernorm.weight', 'decoder.model.layers.10.self_attn.q_proj.weight', 'decoder.model.layers.10.self_attn.k_proj.weight', 'decoder.model.layers.10.self_attn.v_proj.weight', 'decoder.model.layers.10.self_attn.o_proj.weight', 'decoder.model.layers.10.mlp.gate_proj.weight', 'decoder.model.layers.10.mlp.up_proj.weight', 'decoder.model.layers.10.mlp.down_proj.weight', 'decoder.model.layers.10.input_layernorm.weight', 'decoder.model.layers.10.post_attention_layernorm.weight', 'decoder.model.layers.11.self_attn.q_proj.weight', 'decoder.model.layers.11.self_attn.k_proj.weight', 'decoder.model.layers.11.self_attn.v_proj.weight', 'decoder.model.layers.11.self_attn.o_proj.weight', 'decoder.model.layers.11.mlp.gate_proj.weight', 'decoder.model.layers.11.mlp.up_proj.weight', 'decoder.model.layers.11.mlp.down_proj.weight', 'decoder.model.layers.11.input_layernorm.weight', 'decoder.model.layers.11.post_attention_layernorm.weight', 'decoder.model.layers.12.self_attn.q_proj.weight', 'decoder.model.layers.12.self_attn.k_proj.weight', 'decoder.model.layers.12.self_attn.v_proj.weight', 'decoder.model.layers.12.self_attn.o_proj.weight', 'decoder.model.layers.12.mlp.gate_proj.weight', 'decoder.model.layers.12.mlp.up_proj.weight', 'decoder.model.layers.12.mlp.down_proj.weight', 'decoder.model.layers.12.input_layernorm.weight', 'decoder.model.layers.12.post_attention_layernorm.weight', 'decoder.model.layers.13.self_attn.q_proj.weight', 'decoder.model.layers.13.self_attn.k_proj.weight', 'decoder.model.layers.13.self_attn.v_proj.weight', 'decoder.model.layers.13.self_attn.o_proj.weight', 'decoder.model.layers.13.mlp.gate_proj.weight', 'decoder.model.layers.13.mlp.up_proj.weight', 'decoder.model.layers.13.mlp.down_proj.weight', 'decoder.model.layers.13.input_layernorm.weight', 'decoder.model.layers.13.post_attention_layernorm.weight', 'decoder.model.layers.14.self_attn.q_proj.weight', 'decoder.model.layers.14.self_attn.k_proj.weight', 'decoder.model.layers.14.self_attn.v_proj.weight', 'decoder.model.layers.14.self_attn.o_proj.weight', 'decoder.model.layers.14.mlp.gate_proj.weight', 'decoder.model.layers.14.mlp.up_proj.weight', 'decoder.model.layers.14.mlp.down_proj.weight', 'decoder.model.layers.14.input_layernorm.weight', 'decoder.model.layers.14.post_attention_layernorm.weight', 'decoder.model.layers.15.self_attn.q_proj.weight', 'decoder.model.layers.15.self_attn.k_proj.weight', 'decoder.model.layers.15.self_attn.v_proj.weight', 'decoder.model.layers.15.self_attn.o_proj.weight', 'decoder.model.layers.15.mlp.gate_proj.weight', 'decoder.model.layers.15.mlp.up_proj.weight', 'decoder.model.layers.15.mlp.down_proj.weight', 'decoder.model.layers.15.input_layernorm.weight', 'decoder.model.layers.15.post_attention_layernorm.weight', 'decoder.model.layers.16.self_attn.q_proj.weight', 'decoder.model.layers.16.self_attn.k_proj.weight', 'decoder.model.layers.16.self_attn.v_proj.weight', 'decoder.model.layers.16.self_attn.o_proj.weight', 'decoder.model.layers.16.mlp.gate_proj.weight', 'decoder.model.layers.16.mlp.up_proj.weight', 'decoder.model.layers.16.mlp.down_proj.weight', 'decoder.model.layers.16.input_layernorm.weight', 'decoder.model.layers.16.post_attention_layernorm.weight', 'decoder.model.layers.17.self_attn.q_proj.weight', 'decoder.model.layers.17.self_attn.k_proj.weight', 'decoder.model.layers.17.self_attn.v_proj.weight', 'decoder.model.layers.17.self_attn.o_proj.weight', 'decoder.model.layers.17.mlp.gate_proj.weight', 'decoder.model.layers.17.mlp.up_proj.weight', 'decoder.model.layers.17.mlp.down_proj.weight', 'decoder.model.layers.17.input_layernorm.weight', 'decoder.model.layers.17.post_attention_layernorm.weight', 'decoder.model.layers.18.self_attn.q_proj.weight', 'decoder.model.layers.18.self_attn.k_proj.weight', 'decoder.model.layers.18.self_attn.v_proj.weight', 'decoder.model.layers.18.self_attn.o_proj.weight', 'decoder.model.layers.18.mlp.gate_proj.weight', 'decoder.model.layers.18.mlp.up_proj.weight', 'decoder.model.layers.18.mlp.down_proj.weight', 'decoder.model.layers.18.input_layernorm.weight', 'decoder.model.layers.18.post_attention_layernorm.weight', 'decoder.model.layers.19.self_attn.q_proj.weight', 'decoder.model.layers.19.self_attn.k_proj.weight', 'decoder.model.layers.19.self_attn.v_proj.weight', 'decoder.model.layers.19.self_attn.o_proj.weight', 'decoder.model.layers.19.mlp.gate_proj.weight', 'decoder.model.layers.19.mlp.up_proj.weight', 'decoder.model.layers.19.mlp.down_proj.weight', 'decoder.model.layers.19.input_layernorm.weight', 'decoder.model.layers.19.post_attention_layernorm.weight', 'decoder.model.layers.20.self_attn.q_proj.weight', 'decoder.model.layers.20.self_attn.k_proj.weight', 'decoder.model.layers.20.self_attn.v_proj.weight', 'decoder.model.layers.20.self_attn.o_proj.weight', 'decoder.model.layers.20.mlp.gate_proj.weight', 'decoder.model.layers.20.mlp.up_proj.weight', 'decoder.model.layers.20.mlp.down_proj.weight', 'decoder.model.layers.20.input_layernorm.weight', 'decoder.model.layers.20.post_attention_layernorm.weight', 'decoder.model.layers.21.self_attn.q_proj.weight', 'decoder.model.layers.21.self_attn.k_proj.weight', 'decoder.model.layers.21.self_attn.v_proj.weight', 'decoder.model.layers.21.self_attn.o_proj.weight', 'decoder.model.layers.21.mlp.gate_proj.weight', 'decoder.model.layers.21.mlp.up_proj.weight', 'decoder.model.layers.21.mlp.down_proj.weight', 'decoder.model.layers.21.input_layernorm.weight', 'decoder.model.layers.21.post_attention_layernorm.weight', 'decoder.model.layers.22.self_attn.q_proj.weight', 'decoder.model.layers.22.self_attn.k_proj.weight', 'decoder.model.layers.22.self_attn.v_proj.weight', 'decoder.model.layers.22.self_attn.o_proj.weight', 'decoder.model.layers.22.mlp.gate_proj.weight', 'decoder.model.layers.22.mlp.up_proj.weight', 'decoder.model.layers.22.mlp.down_proj.weight', 'decoder.model.layers.22.input_layernorm.weight', 'decoder.model.layers.22.post_attention_layernorm.weight', 'decoder.model.layers.23.self_attn.q_proj.weight', 'decoder.model.layers.23.self_attn.k_proj.weight', 'decoder.model.layers.23.self_attn.v_proj.weight', 'decoder.model.layers.23.self_attn.o_proj.weight', 'decoder.model.layers.23.mlp.gate_proj.weight', 'decoder.model.layers.23.mlp.up_proj.weight', 'decoder.model.layers.23.mlp.down_proj.weight', 'decoder.model.layers.23.input_layernorm.weight', 'decoder.model.layers.23.post_attention_layernorm.weight', 'decoder.model.layers.24.self_attn.q_proj.weight', 'decoder.model.layers.24.self_attn.k_proj.weight', 'decoder.model.layers.24.self_attn.v_proj.weight', 'decoder.model.layers.24.self_attn.o_proj.weight', 'decoder.model.layers.24.mlp.gate_proj.weight', 'decoder.model.layers.24.mlp.up_proj.weight', 'decoder.model.layers.24.mlp.down_proj.weight', 'decoder.model.layers.24.input_layernorm.weight', 'decoder.model.layers.24.post_attention_layernorm.weight', 'decoder.model.layers.25.self_attn.q_proj.weight', 'decoder.model.layers.25.self_attn.k_proj.weight', 'decoder.model.layers.25.self_attn.v_proj.weight', 'decoder.model.layers.25.self_attn.o_proj.weight', 'decoder.model.layers.25.mlp.gate_proj.weight', 'decoder.model.layers.25.mlp.up_proj.weight', 'decoder.model.layers.25.mlp.down_proj.weight', 'decoder.model.layers.25.input_layernorm.weight', 'decoder.model.layers.25.post_attention_layernorm.weight', 'decoder.model.layers.26.self_attn.q_proj.weight', 'decoder.model.layers.26.self_attn.k_proj.weight', 'decoder.model.layers.26.self_attn.v_proj.weight', 'decoder.model.layers.26.self_attn.o_proj.weight', 'decoder.model.layers.26.mlp.gate_proj.weight', 'decoder.model.layers.26.mlp.up_proj.weight', 'decoder.model.layers.26.mlp.down_proj.weight', 'decoder.model.layers.26.input_layernorm.weight', 'decoder.model.layers.26.post_attention_layernorm.weight', 'decoder.model.layers.27.self_attn.q_proj.weight', 'decoder.model.layers.27.self_attn.k_proj.weight', 'decoder.model.layers.27.self_attn.v_proj.weight', 'decoder.model.layers.27.self_attn.o_proj.weight', 'decoder.model.layers.27.mlp.gate_proj.weight', 'decoder.model.layers.27.mlp.up_proj.weight', 'decoder.model.layers.27.mlp.down_proj.weight', 'decoder.model.layers.27.input_layernorm.weight', 'decoder.model.layers.27.post_attention_layernorm.weight', 'decoder.model.layers.28.self_attn.q_proj.weight', 'decoder.model.layers.28.self_attn.k_proj.weight', 'decoder.model.layers.28.self_attn.v_proj.weight', 'decoder.model.layers.28.self_attn.o_proj.weight', 'decoder.model.layers.28.mlp.gate_proj.weight', 'decoder.model.layers.28.mlp.up_proj.weight', 'decoder.model.layers.28.mlp.down_proj.weight', 'decoder.model.layers.28.input_layernorm.weight', 'decoder.model.layers.28.post_attention_layernorm.weight', 'decoder.model.layers.29.self_attn.q_proj.weight', 'decoder.model.layers.29.self_attn.k_proj.weight', 'decoder.model.layers.29.self_attn.v_proj.weight', 'decoder.model.layers.29.self_attn.o_proj.weight', 'decoder.model.layers.29.mlp.gate_proj.weight', 'decoder.model.layers.29.mlp.up_proj.weight', 'decoder.model.layers.29.mlp.down_proj.weight', 'decoder.model.layers.29.input_layernorm.weight', 'decoder.model.layers.29.post_attention_layernorm.weight', 'decoder.model.layers.30.self_attn.q_proj.weight', 'decoder.model.layers.30.self_attn.k_proj.weight', 'decoder.model.layers.30.self_attn.v_proj.weight', 'decoder.model.layers.30.self_attn.o_proj.weight', 'decoder.model.layers.30.mlp.gate_proj.weight', 'decoder.model.layers.30.mlp.up_proj.weight', 'decoder.model.layers.30.mlp.down_proj.weight', 'decoder.model.layers.30.input_layernorm.weight', 'decoder.model.layers.30.post_attention_layernorm.weight', 'decoder.model.layers.31.self_attn.q_proj.weight', 'decoder.model.layers.31.self_attn.k_proj.weight', 'decoder.model.layers.31.self_attn.v_proj.weight', 'decoder.model.layers.31.self_attn.o_proj.weight', 'decoder.model.layers.31.mlp.gate_proj.weight', 'decoder.model.layers.31.mlp.up_proj.weight', 'decoder.model.layers.31.mlp.down_proj.weight', 'decoder.model.layers.31.input_layernorm.weight', 'decoder.model.layers.31.post_attention_layernorm.weight', 'decoder.model.norm.weight', 'decoder.lm_head.weight'])\n",
      "ckpt_dict: dict_keys(['decoder.model.embed_tokens.weight', 'decoder.model.layers.0.self_attn.q_proj.weight', 'decoder.model.layers.0.self_attn.k_proj.weight', 'decoder.model.layers.0.self_attn.v_proj.weight', 'decoder.model.layers.0.self_attn.o_proj.weight', 'decoder.model.layers.0.mlp.gate_proj.weight', 'decoder.model.layers.0.mlp.up_proj.weight', 'decoder.model.layers.0.mlp.down_proj.weight', 'decoder.model.layers.0.input_layernorm.weight', 'decoder.model.layers.0.post_attention_layernorm.weight', 'decoder.model.layers.1.self_attn.q_proj.weight', 'decoder.model.layers.1.self_attn.k_proj.weight', 'decoder.model.layers.1.self_attn.v_proj.weight', 'decoder.model.layers.1.self_attn.o_proj.weight', 'decoder.model.layers.1.mlp.gate_proj.weight', 'decoder.model.layers.1.mlp.up_proj.weight', 'decoder.model.layers.1.mlp.down_proj.weight', 'decoder.model.layers.1.input_layernorm.weight', 'decoder.model.layers.1.post_attention_layernorm.weight', 'decoder.model.layers.2.self_attn.q_proj.weight', 'decoder.model.layers.2.self_attn.k_proj.weight', 'decoder.model.layers.2.self_attn.v_proj.weight', 'decoder.model.layers.2.self_attn.o_proj.weight', 'decoder.model.layers.2.mlp.gate_proj.weight', 'decoder.model.layers.2.mlp.up_proj.weight', 'decoder.model.layers.2.mlp.down_proj.weight', 'decoder.model.layers.2.input_layernorm.weight', 'decoder.model.layers.2.post_attention_layernorm.weight', 'decoder.model.layers.3.self_attn.q_proj.weight', 'decoder.model.layers.3.self_attn.k_proj.weight', 'decoder.model.layers.3.self_attn.v_proj.weight', 'decoder.model.layers.3.self_attn.o_proj.weight', 'decoder.model.layers.3.mlp.gate_proj.weight', 'decoder.model.layers.3.mlp.up_proj.weight', 'decoder.model.layers.3.mlp.down_proj.weight', 'decoder.model.layers.3.input_layernorm.weight', 'decoder.model.layers.3.post_attention_layernorm.weight', 'decoder.model.layers.4.self_attn.q_proj.weight', 'decoder.model.layers.4.self_attn.k_proj.weight', 'decoder.model.layers.4.self_attn.v_proj.weight', 'decoder.model.layers.4.self_attn.o_proj.weight', 'decoder.model.layers.4.mlp.gate_proj.weight', 'decoder.model.layers.4.mlp.up_proj.weight', 'decoder.model.layers.4.mlp.down_proj.weight', 'decoder.model.layers.4.input_layernorm.weight', 'decoder.model.layers.4.post_attention_layernorm.weight', 'decoder.model.layers.5.self_attn.q_proj.weight', 'decoder.model.layers.5.self_attn.k_proj.weight', 'decoder.model.layers.5.self_attn.v_proj.weight', 'decoder.model.layers.5.self_attn.o_proj.weight', 'decoder.model.layers.5.mlp.gate_proj.weight', 'decoder.model.layers.5.mlp.up_proj.weight', 'decoder.model.layers.5.mlp.down_proj.weight', 'decoder.model.layers.5.input_layernorm.weight', 'decoder.model.layers.5.post_attention_layernorm.weight', 'decoder.model.layers.6.self_attn.q_proj.weight', 'decoder.model.layers.6.self_attn.k_proj.weight', 'decoder.model.layers.6.self_attn.v_proj.weight', 'decoder.model.layers.6.self_attn.o_proj.weight', 'decoder.model.layers.6.mlp.gate_proj.weight', 'decoder.model.layers.6.mlp.up_proj.weight', 'decoder.model.layers.6.mlp.down_proj.weight', 'decoder.model.layers.6.input_layernorm.weight', 'decoder.model.layers.6.post_attention_layernorm.weight', 'decoder.model.layers.7.self_attn.q_proj.weight', 'decoder.model.layers.7.self_attn.k_proj.weight', 'decoder.model.layers.7.self_attn.v_proj.weight', 'decoder.model.layers.7.self_attn.o_proj.weight', 'decoder.model.layers.7.mlp.gate_proj.weight', 'decoder.model.layers.7.mlp.up_proj.weight', 'decoder.model.layers.7.mlp.down_proj.weight', 'decoder.model.layers.7.input_layernorm.weight', 'decoder.model.layers.7.post_attention_layernorm.weight', 'decoder.model.layers.8.self_attn.q_proj.weight', 'decoder.model.layers.8.self_attn.k_proj.weight', 'decoder.model.layers.8.self_attn.v_proj.weight', 'decoder.model.layers.8.self_attn.o_proj.weight', 'decoder.model.layers.8.mlp.gate_proj.weight', 'decoder.model.layers.8.mlp.up_proj.weight', 'decoder.model.layers.8.mlp.down_proj.weight', 'decoder.model.layers.8.input_layernorm.weight', 'decoder.model.layers.8.post_attention_layernorm.weight', 'decoder.model.layers.9.self_attn.q_proj.weight', 'decoder.model.layers.9.self_attn.k_proj.weight', 'decoder.model.layers.9.self_attn.v_proj.weight', 'decoder.model.layers.9.self_attn.o_proj.weight', 'decoder.model.layers.9.mlp.gate_proj.weight', 'decoder.model.layers.9.mlp.up_proj.weight', 'decoder.model.layers.9.mlp.down_proj.weight', 'decoder.model.layers.9.input_layernorm.weight', 'decoder.model.layers.9.post_attention_layernorm.weight', 'decoder.model.layers.10.self_attn.q_proj.weight', 'decoder.model.layers.10.self_attn.k_proj.weight', 'decoder.model.layers.10.self_attn.v_proj.weight', 'decoder.model.layers.10.self_attn.o_proj.weight', 'decoder.model.layers.10.mlp.gate_proj.weight', 'decoder.model.layers.10.mlp.up_proj.weight', 'decoder.model.layers.10.mlp.down_proj.weight', 'decoder.model.layers.10.input_layernorm.weight', 'decoder.model.layers.10.post_attention_layernorm.weight', 'decoder.model.layers.11.self_attn.q_proj.weight', 'decoder.model.layers.11.self_attn.k_proj.weight', 'decoder.model.layers.11.self_attn.v_proj.weight', 'decoder.model.layers.11.self_attn.o_proj.weight', 'decoder.model.layers.11.mlp.gate_proj.weight', 'decoder.model.layers.11.mlp.up_proj.weight', 'decoder.model.layers.11.mlp.down_proj.weight', 'decoder.model.layers.11.input_layernorm.weight', 'decoder.model.layers.11.post_attention_layernorm.weight', 'decoder.model.layers.12.self_attn.q_proj.weight', 'decoder.model.layers.12.self_attn.k_proj.weight', 'decoder.model.layers.12.self_attn.v_proj.weight', 'decoder.model.layers.12.self_attn.o_proj.weight', 'decoder.model.layers.12.mlp.gate_proj.weight', 'decoder.model.layers.12.mlp.up_proj.weight', 'decoder.model.layers.12.mlp.down_proj.weight', 'decoder.model.layers.12.input_layernorm.weight', 'decoder.model.layers.12.post_attention_layernorm.weight', 'decoder.model.layers.13.self_attn.q_proj.weight', 'decoder.model.layers.13.self_attn.k_proj.weight', 'decoder.model.layers.13.self_attn.v_proj.weight', 'decoder.model.layers.13.self_attn.o_proj.weight', 'decoder.model.layers.13.mlp.gate_proj.weight', 'decoder.model.layers.13.mlp.up_proj.weight', 'decoder.model.layers.13.mlp.down_proj.weight', 'decoder.model.layers.13.input_layernorm.weight', 'decoder.model.layers.13.post_attention_layernorm.weight', 'decoder.model.layers.14.self_attn.q_proj.weight', 'decoder.model.layers.14.self_attn.k_proj.weight', 'decoder.model.layers.14.self_attn.v_proj.weight', 'decoder.model.layers.14.self_attn.o_proj.weight', 'decoder.model.layers.14.mlp.gate_proj.weight', 'decoder.model.layers.14.mlp.up_proj.weight', 'decoder.model.layers.14.mlp.down_proj.weight', 'decoder.model.layers.14.input_layernorm.weight', 'decoder.model.layers.14.post_attention_layernorm.weight', 'decoder.model.layers.15.self_attn.q_proj.weight', 'decoder.model.layers.15.self_attn.k_proj.weight', 'decoder.model.layers.15.self_attn.v_proj.weight', 'decoder.model.layers.15.self_attn.o_proj.weight', 'decoder.model.layers.15.mlp.gate_proj.weight', 'decoder.model.layers.15.mlp.up_proj.weight', 'decoder.model.layers.15.mlp.down_proj.weight', 'decoder.model.layers.15.input_layernorm.weight', 'decoder.model.layers.15.post_attention_layernorm.weight', 'decoder.model.layers.16.self_attn.q_proj.weight', 'decoder.model.layers.16.self_attn.k_proj.weight', 'decoder.model.layers.16.self_attn.v_proj.weight', 'decoder.model.layers.16.self_attn.o_proj.weight', 'decoder.model.layers.16.mlp.gate_proj.weight', 'decoder.model.layers.16.mlp.up_proj.weight', 'decoder.model.layers.16.mlp.down_proj.weight', 'decoder.model.layers.16.input_layernorm.weight', 'decoder.model.layers.16.post_attention_layernorm.weight', 'decoder.model.layers.17.self_attn.q_proj.weight', 'decoder.model.layers.17.self_attn.k_proj.weight', 'decoder.model.layers.17.self_attn.v_proj.weight', 'decoder.model.layers.17.self_attn.o_proj.weight', 'decoder.model.layers.17.mlp.gate_proj.weight', 'decoder.model.layers.17.mlp.up_proj.weight', 'decoder.model.layers.17.mlp.down_proj.weight', 'decoder.model.layers.17.input_layernorm.weight', 'decoder.model.layers.17.post_attention_layernorm.weight', 'decoder.model.layers.18.self_attn.q_proj.weight', 'decoder.model.layers.18.self_attn.k_proj.weight', 'decoder.model.layers.18.self_attn.v_proj.weight', 'decoder.model.layers.18.self_attn.o_proj.weight', 'decoder.model.layers.18.mlp.gate_proj.weight', 'decoder.model.layers.18.mlp.up_proj.weight', 'decoder.model.layers.18.mlp.down_proj.weight', 'decoder.model.layers.18.input_layernorm.weight', 'decoder.model.layers.18.post_attention_layernorm.weight', 'decoder.model.layers.19.self_attn.q_proj.weight', 'decoder.model.layers.19.self_attn.k_proj.weight', 'decoder.model.layers.19.self_attn.v_proj.weight', 'decoder.model.layers.19.self_attn.o_proj.weight', 'decoder.model.layers.19.mlp.gate_proj.weight', 'decoder.model.layers.19.mlp.up_proj.weight', 'decoder.model.layers.19.mlp.down_proj.weight', 'decoder.model.layers.19.input_layernorm.weight', 'decoder.model.layers.19.post_attention_layernorm.weight', 'decoder.model.layers.20.self_attn.q_proj.weight', 'decoder.model.layers.20.self_attn.k_proj.weight', 'decoder.model.layers.20.self_attn.v_proj.weight', 'decoder.model.layers.20.self_attn.o_proj.weight', 'decoder.model.layers.20.mlp.gate_proj.weight', 'decoder.model.layers.20.mlp.up_proj.weight', 'decoder.model.layers.20.mlp.down_proj.weight', 'decoder.model.layers.20.input_layernorm.weight', 'decoder.model.layers.20.post_attention_layernorm.weight', 'decoder.model.layers.21.self_attn.q_proj.weight', 'decoder.model.layers.21.self_attn.k_proj.weight', 'decoder.model.layers.21.self_attn.v_proj.weight', 'decoder.model.layers.21.self_attn.o_proj.weight', 'decoder.model.layers.21.mlp.gate_proj.weight', 'decoder.model.layers.21.mlp.up_proj.weight', 'decoder.model.layers.21.mlp.down_proj.weight', 'decoder.model.layers.21.input_layernorm.weight', 'decoder.model.layers.21.post_attention_layernorm.weight', 'decoder.model.layers.22.self_attn.q_proj.weight', 'decoder.model.layers.22.self_attn.k_proj.weight', 'decoder.model.layers.22.self_attn.v_proj.weight', 'decoder.model.layers.22.self_attn.o_proj.weight', 'decoder.model.layers.22.mlp.gate_proj.weight', 'decoder.model.layers.22.mlp.up_proj.weight', 'decoder.model.layers.22.mlp.down_proj.weight', 'decoder.model.layers.22.input_layernorm.weight', 'decoder.model.layers.22.post_attention_layernorm.weight', 'decoder.model.layers.23.self_attn.q_proj.weight', 'decoder.model.layers.23.self_attn.k_proj.weight', 'decoder.model.layers.23.self_attn.v_proj.weight', 'decoder.model.layers.23.self_attn.o_proj.weight', 'decoder.model.layers.23.mlp.gate_proj.weight', 'decoder.model.layers.23.mlp.up_proj.weight', 'decoder.model.layers.23.mlp.down_proj.weight', 'decoder.model.layers.23.input_layernorm.weight', 'decoder.model.layers.23.post_attention_layernorm.weight', 'decoder.model.layers.24.self_attn.q_proj.weight', 'decoder.model.layers.24.self_attn.k_proj.weight', 'decoder.model.layers.24.self_attn.v_proj.weight', 'decoder.model.layers.24.self_attn.o_proj.weight', 'decoder.model.layers.24.mlp.gate_proj.weight', 'decoder.model.layers.24.mlp.up_proj.weight', 'decoder.model.layers.24.mlp.down_proj.weight', 'decoder.model.layers.24.input_layernorm.weight', 'decoder.model.layers.24.post_attention_layernorm.weight', 'decoder.model.layers.25.self_attn.q_proj.weight', 'decoder.model.layers.25.self_attn.k_proj.weight', 'decoder.model.layers.25.self_attn.v_proj.weight', 'decoder.model.layers.25.self_attn.o_proj.weight', 'decoder.model.layers.25.mlp.gate_proj.weight', 'decoder.model.layers.25.mlp.up_proj.weight', 'decoder.model.layers.25.mlp.down_proj.weight', 'decoder.model.layers.25.input_layernorm.weight', 'decoder.model.layers.25.post_attention_layernorm.weight', 'decoder.model.layers.26.self_attn.q_proj.weight', 'decoder.model.layers.26.self_attn.k_proj.weight', 'decoder.model.layers.26.self_attn.v_proj.weight', 'decoder.model.layers.26.self_attn.o_proj.weight', 'decoder.model.layers.26.mlp.gate_proj.weight', 'decoder.model.layers.26.mlp.up_proj.weight', 'decoder.model.layers.26.mlp.down_proj.weight', 'decoder.model.layers.26.input_layernorm.weight', 'decoder.model.layers.26.post_attention_layernorm.weight', 'decoder.model.layers.27.self_attn.q_proj.weight', 'decoder.model.layers.27.self_attn.k_proj.weight', 'decoder.model.layers.27.self_attn.v_proj.weight', 'decoder.model.layers.27.self_attn.o_proj.weight', 'decoder.model.layers.27.mlp.gate_proj.weight', 'decoder.model.layers.27.mlp.up_proj.weight', 'decoder.model.layers.27.mlp.down_proj.weight', 'decoder.model.layers.27.input_layernorm.weight', 'decoder.model.layers.27.post_attention_layernorm.weight', 'decoder.model.layers.28.self_attn.q_proj.weight', 'decoder.model.layers.28.self_attn.k_proj.weight', 'decoder.model.layers.28.self_attn.v_proj.weight', 'decoder.model.layers.28.self_attn.o_proj.weight', 'decoder.model.layers.28.mlp.gate_proj.weight', 'decoder.model.layers.28.mlp.up_proj.weight', 'decoder.model.layers.28.mlp.down_proj.weight', 'decoder.model.layers.28.input_layernorm.weight', 'decoder.model.layers.28.post_attention_layernorm.weight', 'decoder.model.layers.29.self_attn.q_proj.weight', 'decoder.model.layers.29.self_attn.k_proj.weight', 'decoder.model.layers.29.self_attn.v_proj.weight', 'decoder.model.layers.29.self_attn.o_proj.weight', 'decoder.model.layers.29.mlp.gate_proj.weight', 'decoder.model.layers.29.mlp.up_proj.weight', 'decoder.model.layers.29.mlp.down_proj.weight', 'decoder.model.layers.29.input_layernorm.weight', 'decoder.model.layers.29.post_attention_layernorm.weight', 'decoder.model.layers.30.self_attn.q_proj.weight', 'decoder.model.layers.30.self_attn.k_proj.weight', 'decoder.model.layers.30.self_attn.v_proj.weight', 'decoder.model.layers.30.self_attn.o_proj.weight', 'decoder.model.layers.30.mlp.gate_proj.weight', 'decoder.model.layers.30.mlp.up_proj.weight', 'decoder.model.layers.30.mlp.down_proj.weight', 'decoder.model.layers.30.input_layernorm.weight', 'decoder.model.layers.30.post_attention_layernorm.weight', 'decoder.model.layers.31.self_attn.q_proj.weight', 'decoder.model.layers.31.self_attn.k_proj.weight', 'decoder.model.layers.31.self_attn.v_proj.weight', 'decoder.model.layers.31.self_attn.o_proj.weight', 'decoder.model.layers.31.mlp.gate_proj.weight', 'decoder.model.layers.31.mlp.up_proj.weight', 'decoder.model.layers.31.mlp.down_proj.weight', 'decoder.model.layers.31.input_layernorm.weight', 'decoder.model.layers.31.post_attention_layernorm.weight', 'decoder.model.norm.weight', 'decoder.lm_head.weight'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_dict = {}\n",
    "# Load the original llama3 model\n",
    "model = ScigptModel(args)\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "print(f\"model_dict: {model_dict.keys()}\")\n",
    "\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer0.items():\n",
    "    new_k = \"decoder.model.\" + k\n",
    "    ckpt_dict[new_k] = v\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 1).zfill(2)\n",
    "    layer = torch.load(os.path.join(args.save_dir, f\"layer_{l_index}-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        ckpt_dict[f\"decoder.model.layers.{l}.{k}\"] = layer[k]\n",
    "    del layer\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_33-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_34-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "\n",
    "print(f\"ckpt_dict: {ckpt_dict.keys()}\")\n",
    "model_dict.update(ckpt_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'psize': 4096, 'depth': 2, 'branch_pages': 1, 'leaf_pages': 38, 'overflow_pages': 33698, 'entries': 3745}\n",
      "3744\n"
     ]
    }
   ],
   "source": [
    "import lmdb\n",
    "from sfm.data.prot_data.util import bstr2obj\n",
    "# load data\n",
    "file_path='/home/v-zekunguo/hai1data/nlm/valid_lmdb/valid.patent.v2.txt.lmdb'\n",
    "env = lmdb.open(\n",
    "    file_path, subdir=True, readonly=True, lock=False, readahead=False\n",
    ")\n",
    "txn = env.begin(write=False)\n",
    "\n",
    "print(env.stat())\n",
    "count=0\n",
    "metadata = bstr2obj(txn.get(\"metadata\".encode()))\n",
    "cur_len, cur_keys = metadata[\"size\"], metadata[\"keys\"]\n",
    "print(cur_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Calculate loss\n",
    "print(metadata.keys())\n",
    "loss_list=[]\n",
    "print(metadata['processed_seq_len'])\n",
    "for key in cur_keys:\n",
    "    value = txn.get(str(key).encode())\n",
    "    input_ids = np.frombuffer(value, dtype=np.uint32)\n",
    "    input_tensor = torch.from_numpy(input_ids.astype(np.int64)).unsqueeze(0).to(device)\n",
    "    labels = input_tensor.clone()\n",
    "    out = model.decoder(input_tensor, labels=labels)\n",
    "    input_tensor.to(\"cpu\")\n",
    "    labels.to(\"cpu\")\n",
    "    print(out.loss.cpu().item())\n",
    "    loss_list.append(out.loss.cpu().item())\n",
    "    out = None\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    del out\n",
    "print(sum(loss_list) / len(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Funny you should ask. I was just looking at an article on a \"Frozen\" clear coat. Gives a matte-like breakup of reflections - and ease the eyestrain on the \"Earl Scheib\". Maybe the paint supply store could help?\\\\nGene--Funny you should ask! Just yesterday I got my car back with the brand new repaired & painted front bumper. Same problem. The color matches, but the bumper is way too shiny with the clear coat they had to put on it. I am wondering the same thing--how to dull it a little to match the rest of my old paint job.\\\\nThere are white Scotchbrite pads that might be able to take the gloss down without scratching. I think they are available at HD and Lowes.\\\\nIf that won\\'t work maybe do the whole car w/ 0000 steel wool?\\\\nI just got done rubbing my bumper out. It looks pretty good in that it now doesn\\'t look out of place on my car.\\\\nBe gentle. Practice on a non-visible area like under the nose first. Don\\'t use a heavy hand or rub too long. Just enough to bust the gloss.\\\\nI haven\\'t seen it in direct sunlight yet because I\\'m still re-assembling the grill, but I don\\'t see any swirl marks in indirect sunlight, under the fluorescents or using my drop light reflecting on it.\\\\nIt was pretty funny seeing the reaction I got from body & paint guys when I asked them how to dull my shiny new paint job. Nobody had ever asked them that question before and they just couldn\\'t comprehend it. It was a younger hot rod guy who laughed and understood exactly what I was after. He was the one who, after talking about several options including spray on de-glosser, suggested trying rubbing compound first.\\\\nNext step in making my car reasonably presentable is touching up all the dings. The man who did the bumper gave me some of the paint and I have gone around doing all the chips & dings--well, the worst of them-- but it is a flat paint. I think I\\'ll try clear nail polish to seal and even out the bigger ones. We\\'ll see how that works.....Hey, it\\'s already down to a 20 foot paint job instead of 50 foot!<|end_of_text|><|begin_of_text|>All articles with dead external links, Articles with dead external links from November 2014, Articles incorporating text from Wikipedia,\\\\nNorth Carolina State Senators\\\\nNorth Carolina Democrats\\\\nUnited States Army personnel of the Iraq War\\\\nRecipients of the Bronze Star Medal\\\\nUniversity of North Carolina at Chapel Hill alumni\\\\nAlumni of the London School of Economics\\\\nCal Cunningham\\\\nMember of the North Carolina Senate\\\\nfrom the 23rd district\\\\nJanuary 2001 – January 2003\\\\nEleanor Kinnaird [1]\\\\nElizabeth Cunningham\\\\nhttp://www.ncinjurylitigation.com\\\\nJames Calvin \"Cal\" Cunningham III (born 6 August 1973) is an attorney, captain in the United States Army Reserve, and a former member of the North Carolina Senate. He is a member of the Democratic Party.\\\\nCunningham ran for the United States Senate in 2010,[2][3] losing to Elaine Marshall in a Democratic primary runoff on June 22, 2010.[4][5]\\\\n1.1 Early life and education\\\\n1.2 Public Service\\\\n1.2.1 UNC\\\\n1.2.2 State Senate\\\\n1.2.3 U.S. Army Reserve\\\\n1.3 Other involvement\\\\n1.4 Senate campaign\\\\n2 Legal practice\\\\n3 Awards, honors, community service\\\\nEarly life and education[edit | edit source]\\\\nCunningham was born in Winston-Salem, North Carolina and grew up in Lexington, North Carolina. He attended the Lexington City Schools and Forsyth Country Day School where he graduated in 1991. Cunningham studied at Vanderbilt University before transferring to the University of North Carolina at Chapel Hill. He graduated from UNC in 1996 with a Bachelor\\'s degree (with Honors) in Political Science and Philosophy. Cunningham received a Master\\'s of Science in Public Policy and Public Administration from the London School of Economics. He was awarded a law degree in 1999 from University of North Carolina School of Law.[6]\\\\nCunningham also studied government in Thun, Switzerland, business and finance at the Carolina Business Institute and international law through the Duke University Asian American Transnational Law Institute in Hong Kong.\\\\nDuring the summer of 1993, Cunningham attended American University and interned on Capitol Hill for a subcommittee chaired by Senator Carl Levin.[7]\\\\nPublic Service[edit | edit source]\\\\nUNC[edit | edit source]\\\\nAt UNC-Chapel Hill, Cunningham was elected the Student Body President in 1995. He served ex officio on the Board of Trustees,[8] the General Alumni Association Board of Directors, the Athletic Council, and the Board of Visitors. During his tenure, Cunningham worked to allocate tuition funds for need-based financial aid, faculty pay and the libraries.[9] He supported campus transportation and safety, community service initiatives[10] and an overhaul of campus dining.[11]\\\\nDuring law school, Cunningham served as Chief Justice of the Student Supreme Court. He held the position for two terms and published the first Report of the Court\\'s cases.\\\\nState Senate[edit | edit source]\\\\nIn November 2000, Cunningham was elected to represent the 23rd Senate District in the 144th Session of the North Carolina General Assembly.[12] At the time of his election, he was North Carolina\\'s youngest legislator and represented parts of Davidson, Rowan and Iredell Counties.[13] After the campaign, another candidate challenged Cunningham\\'s residency. The challenge was denied by the local and state Boards of Elections, Superior Court, and the North Carolina Court of Appeals. The North Carolina Supreme Court later refused to grant a stay against the earlier decision of the Court of Appeals.[14]\\\\nIn the Senate, Cunningham served as Vice Chairman of the Senate Judiciary Committee and on the Education Appropriations, Policy and Joint Oversight Committees. Cunningham worked on privacy legislation, campaign reform,[15] the patient\\'s bill of rights,[16] the clean smokestacks bill,[17] class size reductions[18] and preservation of farmland.[19]\\\\nHe did not run for re-election after the 23rd district was split into three Republican-leaning districts by redistricting.[20]\\\\nU.S. Army Reserve[edit | edit source]\\\\nCunningham was commissioned in the Army Reserve, Judge Advocate General\\'s Corps in 2002 and has been mobilized for two active duty tours. In the Reserve, he serves with an airborne unit at Fort Bragg (North Carolina).\\\\nIn 2007, Cunningham was mobilized by XVIII Airborne Corps and served as the senior trial counsel, Multi-National Corps - Iraq.[21] In Iraq, he pioneered an effort with the U.S. Department of Justice to prosecute felony contractor misconduct and worked with the Major Procurement Fraud Task Force. He was lead counsel in the first court-martial of a contractor/civilian under the Uniform Code of Military Justice since 1968.[22] For his service in Iraq, Cunningham was awarded the Bronze Star Medal. Cunningham also received the prestigious General Douglas MacArthur Leadership Award.[23]\\\\nIn 2005, Cunningham also served with XVIII Airborne Corps at Fort Bragg as a Special Assistant United States Attorney.[24] He prosecuted felony and misdemeanor crimes committed on the Fort Bragg military reservation.\\\\nCunningham was assigned to work with a special operations task force in Afghanistan in 2011.[25]\\\\nCunningham is a graduate of the Judge Advocate Officer Advanced Course, Airborne School[26] and the Officer Basic Course. Prior to September 11, 2001, Cunningham served as a Third Class Petty Officer, Naval Reserve with Military Sealift Command, Port of Wilmington.\\\\nOther involvement[edit | edit source]\\\\nCunningham serves on various boards and commissions. Since 2003, he has served as an appointee of the Governor on the Board of Trustees of Davidson County Community College.[27] He also served as an appointee of the Governor on the North Carolina Banking Commission.[28] Cunningham serves on the North Carolina Democratic Party Executive Committee.\\\\nSenate campaign[edit | edit source]\\\\nMain article: United States Senate election in North Carolina, 2010\\\\nIn 2010, Cunningham filed as a candidate for the U.S. Senate seat held by Richard Burr.[29][30] Other candidates in the May 4 Democratic primary included Elaine Marshall and Ken Lewis. Retired NATO Supreme Allied Commander Gen. Wesley Clark endorsed Cunningham, saying that he would be \"the first veteran of the wars in Iraq and Afghanistan to serve in the U.S. Senate.\" [31] Cunningham also received the endorsement of the state\\'s largest organization of teachers, the North Carolina Association of Educators.[32] Cunningham finished in second place in the primary, but since no candidate received 40 percent of the vote, he was entitled to advance to a runoff with the first-place finisher, Marshall. He lost the runoff election on June 22, 2010, with leading Democrats acknowledging that \"the Wes Clark contingent in the (Democratic) party isn\\'t really big\".[33]\\\\nLegal practice[edit | edit source]\\\\nCunningham is a litigation attorney with Carolina Litigation Group, handling \"cases in a variety of subject areas from complex real estate disputes, breaches of contract and business torts to catastrophic personal injury and products liability.\" He is admitted to the Bar in North Carolina and admitted to practice before the Western, Middle and Eastern District federal courts, the U.S. Court of Appeals for the Fourth Circuit, and the United States Supreme Court.[34]\\\\nAwards, honors, community service[edit | edit source]\\\\nCunningham has been recognized for his leadership by his selection as one of the Jaycees\\' Outstanding Young North Carolinians and with the Distinguished Service Award.[35] In 2007, he was selected one of the Triad\\'s Forty Leaders Under Forty.[36]\\\\nFor his military service, Cunningham has been awarded the Bronze Star Medal, the Iraq Campaign Medal, the Army Commendation Medal (3x), the Army Achievement Medal, the Army Reserve Components Achievement Medal (2x), the National Defense Service Medal, the Global War on Terrorism Service Medal, the Army Service Ribbon, the Overseas Service Ribbon, the Army Reserve Components Overseas Training Ribbon, the Armed Forces Reserve Medal with \"M\" Device (2x) and the Parachutist Badge.\\\\nIn 2009, Cunningham was awarded the General Douglas MacArthur Award for Leadership as one of the outstanding company grade officers in the Army, including for his ground-breaking work in Iraq.[23][37]\\\\nDuring college at UNC-Chapel Hill, Cunningham was inducted into the Golden Fleece Honorary Society[38] and the Order of the Grail-Valkyries[39] for his work in positions of student leadership. He was also inducted into the Pi Sigma Alpha Political Science Honor Society for his academic work.\\\\nCunningham appeared as a panelist on the American Public Television program, The Struggle for Moral Leadership, a series of two one-hour programs that explore ethics and morality in political leadership, which was moderated by Harvard University professor Arthur Miller and included political analyst George Stephanopoulos.[40]\\\\nIn 2007, Cunningham was selected for a Marshall Memorial Fellowship[41] and traveled to Belgium, France, Italy, Denmark and Poland to meet with government and civic leaders about Trans-Atlantic security, combating Islamic extremism and terrorism.[42]\\\\nCunningham has received a Pro Bono Impact Award and recognition from Legal Aid of Forsyth County for legal representation of victims of domestic violence and of tenants in disputes with their landlords.[43]\\\\nCunningham served from 2002 to 2005 as a Deacon at the First Presbyterian Church of Lexington and a partial term in 2006-07 as an Elder on the Session. He resigned from the Session for his deployment to Iraq.\\\\nPersonal life[edit | edit source]\\\\nCunningham lives in Lexington with his wife, Elizabeth, and their two children, Caroline (born 2002) and Will (born 2003). In 2007, Cunningham and his wife ran their fifth marathon together in Lake Tahoe. They have a golden retriever named Davidson.\\\\n↑ Kinnaird was elected to represent the same numbered district (23rd); geographically, the district no longer exists.\\\\n↑ \"Cunningham Won\\'t Run for Senate in 2010\". Roll Call. 2009-11-10. http://www.rollcall.com/news/40497-1.html. Retrieved 2011-01-16.\\\\n↑ Charlotte Observer: Does panel not think Marshall can beat Burr?[dead link]\\\\n↑ \"Johnson defeats D\\'Annunzio\". CharlotteObserver.com. 2010-06-22. http://www.charlotteobserver.com/2010/06/22/1518362/johnson-leads-dannunzio-in-8th.html. Retrieved 2011-01-16.\\\\n↑ http://results.enr.clarityelections.com/NC/17242/29119/en/summary.html\\\\n↑ [1][dead link]\\\\n↑ \"Full text of \"S. 885, to modify congressional restrictions on gifts : hearing before the Subcommittee on Oversight of Government Management of the Committee on Governmental Affairs, United States Senate, One Hundred Third Congress, first session, on S. 885... July 19, 1993\"\". Archive.org. http://www.archive.org/stream/s885tomodifycong00unit/s885tomodifycong00unit_djvu.txt. Retrieved 2011-01-16.\\\\n↑ \"Transcript, Faculty Council Meeting, September 8, 1995\". Unc.edu. 1995-09-08. http://www.unc.edu/faculty/faccoun/archives/1995-96/95sep/file.5.html. Retrieved 2011-01-16.\\\\n↑ \"University Gazette, February 7, 1996\". Gazette.unc.edu. 1996-02-07. http://gazette.unc.edu/archives/96feb07/file.3.html. Retrieved 2011-01-16.\\\\n↑ \"Keadle vs. Cunningham — Courts uphold voters\\' choice\". Salisburypost.com. 1999-11-02. http://www.salisburypost.com/2001jan/012501ed.htm. Retrieved 2011-01-16.\\\\n↑ NC General Assembly webmasters. \"North Carolina General Assembly - Senate Bill 8 Information/History (2001-2002 Session)\". Legislature.state.nc.us. http://www.legislature.state.nc.us/gascripts/BillLookUp/BillLookUp.pl?Session=2001&BillID=S8. Retrieved 2011-01-16.\\\\n↑ NC General Assembly webmasters. \"North Carolina General Assembly - Senate Bill 199 Information/History (2001-2002 Session)\". Ncleg.net. http://www.ncleg.net/gascripts/BillLookUp/BillLookUp.pl?BillID=S199&Session=2001. Retrieved 2011-01-16.\\\\n↑ NC General Assembly webmasters. \"North Carolina General Assembly - Senate Bill 1078 Information/History (2001-2002 Session)\". Ncga.state.nc.us. http://www.ncga.state.nc.us/gascripts/BillLookup/BillLookUp.pl?Session=2001&BillID=S1078. Retrieved 2011-01-16.\\\\n↑ \"Keadle calls experience his trump over Cunningham\". Salisburypost.com. 2000-10-01. http://www.salisburypost.com/2000oct/100100c.htm. Retrieved 2011-01-16.\\\\n↑ http://www.ncacc.org/bulletins/2002/bull04.doc\\\\n↑ Ivey, Steve (2008-03-03). \"Cal Cunningham: Kilpatrick Stockton attorney goes from comforts of Triad to dangers of Iraq\". Triad.bizjournals.com. http://triad.bizjournals.com/triad/stories/2008/03/03/story7.html. Retrieved 2011-01-16.\\\\n↑ Ivey, Steve (2008-03-03). \"Cal Cunningham: Kilpatrick Stockton attorney goes from comforts of Triad to dangers of Iraq | The Business Journal\". Triad.bizjournals.com. http://triad.bizjournals.com/triad/stories/2008/03/03/story7.html. Retrieved 2011-01-16.\\\\n↑ http://www.ncbar.org/ncLawyer/10/3703/index.aspx?type=article&print=true\\\\n↑ 23.0 23.1 http://www.armyg1.army.mil/macarthur/\\\\n↑ \"J. Calvin Cunningham, III Lawyer Profile\". martindale.com. http://www.martindale.com/J-Calvin-Cunningham-III/3261398-lawyer.htm. Retrieved 2011-01-16.\\\\n↑ News & Observer: Cal Cunningham off to Afghanistan\\\\n↑ http://www.davidson.cc.nc.us/about/board_of_trustees.htm\\\\n↑ \"Current Banking Commission Members\". Nccob.org. https://www.nccob.org/Online/BRTS/BankCommissionMembers.aspx. Retrieved 2011-01-16.\\\\n↑ \"News & Observer: Cunningham makes it official\". Projects.newsobserver.com. http://projects.newsobserver.com/under_the_dome/cunningham_makes_it_official. Retrieved 2011-01-16.\\\\n↑ \"News & Observer: Cunningham\\'s announcement speech\". Projects.newsobserver.com. http://projects.newsobserver.com/under_the_dome/cunninghams_announcement_speech. Retrieved 2011-01-16.\\\\n↑ \"News & Observer: Cunningham endorsed by retired Gen Wesley Clark\". Projects.newsobserver.com. 2010-03-29. http://projects.newsobserver.com/under_the_dome/cunningham_endorsed_by_retired_gen_wesley_clark. Retrieved 2011-01-16.\\\\n↑ \"News & Observer: Cunningham, Lewis pick up endorsements\". Projects.newsobserver.com. http://projects.newsobserver.com/under_the_dome/cunningham_lewis_pick_up_endorsements. Retrieved 2011-01-16.\\\\n↑ DAVID CATANESE (6/11/2010). \"Marshall wins N.C. Senate nomination - David Catanese\". Politico.Com. http://www.politico.com/news/stories/0610/38894.html. Retrieved 2011-01-16.\\\\n↑ Carolina Litigation Group\\\\n↑ \"Jaycees honor five people for service to community\". The-Dispatch.com. http://www.the-dispatch.com/apps/pbcs.dll/article?AID=/20030507/NEWS/305070310/1005/NEWS01. Retrieved 2011-01-16.\\\\n↑ \"News & Observer: Cunningham wins Army award\". Projects.newsobserver.com. 2009-06-11. http://projects.newsobserver.com/under_the_dome/cunningham_wins_army_award. Retrieved 2011-01-16.\\\\n↑ \"Order of the Golden Fleece of the University of North Carolina at Chapel Hill Records, 1904-2007\". Lib.unc.edu. 1904-04-11. http://www.lib.unc.edu/mss/uars/ead/40160.html. Retrieved 2011-01-16.\\\\n↑ \"Order of the Grail-Valkyries of the University of North Carolina at Chapel Hill Records, 1920-2003\". Lib.unc.edu. http://www.lib.unc.edu/mss/uars/ead/40161.html. Retrieved 2011-01-16.\\\\n↑ \"People in Business\". The-Dispatch.com. http://www.the-dispatch.com/article/20061213/NEWS/612130358. Retrieved 2011-01-16.\\\\n↑ \"May/June 2007\". Carolina Alumni Review. http://www.carolinaalumnireview.com/carolinaalumnireview/20070506/?pg=101. Retrieved 2011-01-16.\\\\nCarolina Litigation Group\\\\nThe Focal Points a blog by Cunningham\\\\nCal Cunningham for U.S. Senate official campaign website\\\\nProfile at Project Vote Smart\\\\nFinancial information (federal office) at the Federal Election Commission\\\\n2010 campaign contributions at OpenSecrets.org\\\\nCategory:People from Davidson County, North Carolina\\\\nRetrieved from \"https://military.wikia.org/wiki/Cal_Cunningham?oldid=5322778\"<|end_of_text|><|begin_of_text|>Discussion in \\'Calibration, Help, and Troubleshooting\\' started by Henry feldman, Apr 7, 2016.\\\\nSo not sure what happened. Printed a perfectly good job. Went out for a while, then returned to print another job. Everything now prints about 3mm above the bed. I do autohome and it seems to do it, but printing one of the calibration cubes I have stored on the card prints up in the air. I leveled the bed, and the \"highest\" value was about 1 with some reaching 0 without putting enough tension on the paper. Any idea what happened and what I should do about it? Note, I did take the glass off to clean it between the jobs, but it went right back on and is clipped in place.\\\\nI\\'ve run into this problem many times. I don\\'t know what causes it, but the symptom seems to be that the glass bed lowers a bit while the print head makes its way back to the dump bucket to prime. When it leaves the dock, most times, the bed raises back up, but way more often than I\\'d like, it stays in the lower position.\\\\nBut it did the autohome thing, seems to be sort of normal, but just stays 4mm off the bed. I remeasured the sensor to the belt thickness. Releveled. Is it possible there is an offset stored in the firmware? If so, how does one fix it?\\\\nDid you install the EEPROM Marlin Editor plugin in Octoprint? I haven\\'t used it yet, but with that plugin it should be possible to read your current Marlin firmware settings.\\\\nYou can set offsets for X, Y and Z using M206 command. Offsets set with M206 can be stored in the EEPROM using M500.\\\\nUnfortunately I found no command to read it other than M503 to print your settings but this prints all settings, you gotta search for the \"\\\\nRecv: echo: M206 X0.00 Y0.00 Z0.00\" in the output.\\\\nBack to your problem: sorry I have no concrete idea.\\\\nDo a complete Z level sequence.\\\\nUnlike what the build manual says I did not use the \"ruler method\" anymore to level the bed when it is way down.\\\\nI auto home as advised, then move the print head to the middle of the bed.\\\\nThen I turn the Z axis by hand so the bed touches the nozzles and level it by moving the head to the left and the right and turn each Z motor that the nozzles touch the bed. That make the \"ruler method\" obsolete I think.\\\\nDo you have clean glass under the z home or did you put on a surface coating?\\\\nYep, totally fixed. I mean sure, I understand why between 2 prints my Z-axis is totally screwed. Anyway a full recommission has returned it to full functionality.\\\\nIt almost sounds like it was reset altogether. Could your finger have slipped and you reset to defaults or something, or maybe shorted something?\\\\nHow is wolfbite? I am using 3d-ezz right now.\\\\nTry to find out the actual trigger-height of the sensor, possibly with G30 command.\\\\nI have all the Wolf products. So far for PLA it is pretty magical, sticks like glue when printing and once it gets cold, it pops off with zero effort (well once getting a frog off was almost impossible, so I had to do the arm water soak and then it came off - then you have to reapply). So far I get about 10-15 prints before having to reapply. Not a huge deal (the bottle holds like 30 applications at least). Just while applying your room smells like vinegar, but after that nothing.\\\\nOK, @Alex9779 it did it again, after a perfect 15 hour print. The only difference from before this started occurring and now is using the FFF from your github. I have to try Koch\\'s postulate here and reintroduce the original FFF to see if the problem goes away (although I can\\'t imagine what is there, unless there is some weird corruption), then reintroduce your FFF to see if it comes back. I\\'ve gone through the generated G-code and don\\'t see anything modifying the Z offset. Any ideas? I have not upgraded my firmware yet (still running stock) so if that\\'s the issue, I can certainly upgrade.\\\\nWhat is weird is then on the last run, it was on for the Z but the Y was totally screwed and back by about the difference between the docked position and the bed (actually printed in space behind the bed and then popped a clip off the glass. I rebooted the printer, re adjusted Z, leveled manually, leveled in Marlin and seems to be fine now.\\\\nI really do anything with Z or other offsets in the profile (that means the startup script).\\\\nAll I do is that I home and set the offset for tool 1 if you print with both.\\\\nIf you print only with right extruder then homing is done with tool 1. Maybe RC3 has a problem with that?\\\\nOK, @Alex9779 you\\'re off the hook, as it is doing it just as frequently with the stock FFF. No idea. This is insanely frustrating. According to my stats I am running a 43% success rate! The last one I had to cancel as it was too tight (and each layer was basically scraping the prior one off the bed). I guess I can go through and re-tighten everything (again) although everything is blue-funked so screws shouldn\\'t be slipping. I\\'d say once my titan gets here it will be better, but at the moment I can\\'t even count on being able to print that set of parts out!<|end_of_text|><|begin_of_text|>facebook_square Created with Sketch.\\\\ntwitter_square Created with Sketch.\\\\ninstagram_square Created with Sketch.\\\\n2021 SPRING EVENTS: A LOOKBACK\\\\nThis Spring we\\'ve been busy at The Rising Majority. In the face of anti-Asian hate and international state violence from Columbia to Palestine to Ohio, we have continued to ask ourselves what does it mean to be in authentic mutli-racial solidarity? What does it mean beyond a statement, beyond a moment, beyond a news cycle? This Spring we have experimented with what this type of solidarity can look like. We\\'ve held film screenings, panel discussions, and arts and healing classes. We have challenged ourselves to lean into community and to trust that we have everything we need to survive.\\\\nIn April we invited you all- our Rising Majority community- to an intimate film screening of American Revolutionary, honoring and educating about the life of Asian American activist Grace Lee Boggs. The screening was a part of our response to the horrific and heartbreaking surge in violence we\\'ve seen this past year against Asian Americans across the US. We held a panel discussion afterward moderated by Eva Cardenas of The Ruckus Society that grappled with what nuanced mutli-racial solidarity can look like when we are in right relationship with each other. The conversation took place between the film\\'s creator Grace Lee, Nikita Mitchell RM\\'s fearless national coordinator, and Holly Yu, a youth organizer with Asian Youth Promoting Advocacy and Leadership.\\\\nIn May we saw struggles against state sanctioned violence increase across the world, from Colombia to Palestine. As a community committed to resistance and freedom for all, we asked you to show up in solidarity to an urgent discussion with poet and Palestinan activist Mohammed El-Kurd, organizer with Arab Resource & Organizing Center Lara Kiswani, Afro-Colombian organizer Yolanda Perea, Black scholar and PhD student Harrinson Cuero, and activist with Special Jurisdiction for Peace (JEP), Esther Olujari. We learned from these organizers what has been happening on the ground, how it intertwines with the struggles we face locally, and what the immediate calls to action are.\\\\nWe know that our struggles in the US are intertwined with struggles globally. We know that the fight towards an end to policing of all forms, a livable wage and the destruction of racial capitalism can not be won in a vacuum. The evidence that our struggles are intertwined is showcased clearly in how the US and Israeli police forces collaborate and train to more effectively surveill, harass and kill our people. The evidence is seen in how the U.S has used our tax dollars to support wars and genocidal efforts in Palestine, Colombia and beyond.\\\\nEven as the media focus on these issues begins to wane, we are committed to amplifying the voices of Palestinian and Columbian organizers. We are committed to this long haul fight to liberation and towards a world free from settler colonialism and state violence.\\\\nAt the end of May we wanted to acknowledge the year it had been. This year we had taken to the streets to protest police murders and in support of #DefundPolice, had held hard conversations with our families and communities about the reality of democracy in the United States, and took action digitally to pressure the federal and local governments to invest in our communities in the midst of a global pandemic.\\\\nWe worked tirelessly. We stretched into uncomfortability, interrogated our values and recommitted to this long protracted struggle for freedom again and again and again. In the midst of on-going attacks and loss, we remained firm in our vision and won many changes over the course of the year.\\\\nTo this end, in late May we invited you to honor and acknowledge this year of resistance with a series of three resiliency classes. The classes we offered were grounded in arts and healing, they provided a moment of reflection and restoration in service to the fight ahead. We also recognize though that resilience is something we cultivate out of a need to survive that which oppresses us. In this series we held that closely and side by side with a desire to care for ourselves and one another. The classes we offered wove together the care that we need to build resilience with a recognition that we must dismantle that which forces us to be resilient in the first place.\\\\nIt has been an honor to work and play in community with you this Spring, stay tuned with us to learn more about what we have coming up this Summer!\\\\nTo learn more text 90975 to Majority.\\\\nSign up today to join The Rising Majority!\\\\nBuilt by The Movement for Black Lives + The Rising Majority<|end_of_text|><|begin_of_text|>You may know Woody Harrelson for his Emmy-winning television work, his Oscar-nominated film roles, or as this year\\'s official sexiest celebrity vegetarian. Which is all fine and good, but we know (and admire and adore) Woody Harrelson for the love he shows the trees.\\\\nLike some Lorax of Hollywood – minus the \"shortish and oldish and brownish and mossy\" – Harrelson speaks for the trees. And not only does he speak for them, he has been actively working on formidable solutions for ways to save the arboreal denizens of this orb we call home.\\\\nNot satisfied to idly stand by as various industries greedily gobble up ancient forests, Harrelson teamed up with sustainably savvy eco-entrepreneur Jeff Golfman (pictured below) in the late 1990s to form Prairie Pulp & Paper.<|end_of_text|><|begin_of_text|>Артем Володимирович Божко (30 жовтня 1985, м. Воскресенськ, СРСР)\\xa0— білоруський хокеїст, правий нападник.\\\\n\\\\nВихованець хокейної школи «Хімік» (Воскресенськ), тренери — М.Жаров, І.Ігнатов. Виступав за  «Мотор» (Барнаул), «Металург» (Сєров), «Автомобіліст» (Єкатеринбург), «Зауралля» (Курган), «Южний Урал» (Орськ), «Капітан» (Ступіно), ХК «Вітебськ», «Німан» (Гродно), «Краковію».\\\\n\\\\nПосилання \\\\n Профіль  на Eliteprospects  \\\\n\\\\nУродженці Воскресенська\\\\nБілоруські хокеїсти\\\\nХокеїсти «Вітебська»\\\\nХокеїсти «Могильова»\\\\nХокеїсти «Автомобіліста»\\\\nХокеїсти «Зауралля»\\\\nХокеїсти «Южного Урала»\\\\nХокеїсти «Мотора» (Барнаул)\\\\nХокеїсти «Капітана»\\\\nХокеїсти «Німана»<|end_of_text|><|begin_of_text|>A curious commentary on CNN today that seems to be part of the recent effort to neuter the Republican party\\'s ideological and substantive opposition to the Obama stimulus package.\\\\nAnd, by the way, if you\\'re a Republican in a state in which the economy is truly hurting, it\\'s risky to oppose a stimulus package.\\\\nI guess that depends on how you define \"wants one.\" 59% of Americans are concerned that President Obama will increase spending too much. So while arguably most of the public wants a stimulus package, that does not mean they want Obama\\'s stimulus package or want as much stimulus.\\\\nFurther, while it may be politically \"risky,\" there\\'s nothing wrong with obstructing bad legislation. In fact, it\\'s high time conservatives take some risks for what they believe in. Someone is either conservative or he isn\\'t. If a Republican politician is conservative then there really shouldn\\'t be any doubt that Obama\\'s current package is a bad idea. It doesn\\'t become a better idea if it\\'s watered down or includes a few bones for Republicans.\\\\nIf someone asks me \"How about I cut off your arm?\" my response is \"How about you don\\'t?\" If they then counter and say, \"In the spirit of compromise and bipartisanship, how about I just cut off your fingers but since I know you like lollipops, I\\'ll give you a lollipop?\" In that situation my response would still be, \"How about you don\\'t?\" There\\'s no compelling reason to compromise with a bad idea that\\'s still bad after the compromise.\\\\nPresident Obama made it pretty clear last Friday that he doesn\\'t need Congressional Republicans. As Obama said, he won. So it would be great if Republicans fought the bad legislation tooth and nail and then voted a very clear \"no\" in both the House and the Senate. Obama doesn\\'t need the Republicans since Democrats control both chambers of Congress. So let the Democrats take complete ownership of this \"solution\" to the problem.\\\\nThe last time legislation was rushed through Congress, it was the financial bailout back in September and October of last year. The public was overwhelmingly against it and Republicans initially stood strong and rejected it, but eventually caved. Within two months the public (and both parties) were complaining about how bad the legislation was.\\\\nThe Republicans were right when they initially rejected it and very well may have won the presidential election in November had both Congressional Republicans and Senator McCain stood up to bad legislation. Rejecting the bad legislation was in line with conservative ideology, it was right for America, and it\\'s what the citizens wanted. Instead, Republicans caved, we got a $700 billion boondoggle filled with pork, and we lost the election.\\\\nNow Congressional Republicans are again being pressured to support bad legislation. Again Republicans and America are being told that it\\'s more important to act quickly rather than act correctly. We\\'re again being told immediate action is required or the economy will collapse.\\\\nRepublicans need to stick to their conservative guns. If it\\'s not consistent with conservative principles, vote NO.\\\\nDemocrats have the votes to pass the stimulus by themselves anyway. They don\\'t need Republicans. If Democrats are really sure their package is a good idea then they\\'ll be more than happy to own the legislation all by themselves and take full credit for it.\\\\nI\\'d rather Republicans be obstructionists doing what\\'s right for the country than bipartisan and doing what\\'s wrong.<|end_of_text|><|begin_of_text|>Mz. Risk® releases helpful tips on insurance issues to improve your information basis and support decision making on risk and insurance issues. Each month throughout the year Mz. Risk® will release tips on an insurance subject. To learn more follow Mz. Risk® on Twitter, Facebook, and LinkedIn. Buy a year\\'s worth of tips in our shop.\\\\nWant to know even more? Send a question to Mz. Risk® to Blog about, or contact D\\'Andre Insurance Group for individual assistance.<|end_of_text|><|begin_of_text|>[6.6.1]  When the disaster was reported to Alexander, who was seriously ill either from despondency or the unfamiliar air, he fell into despair. The rest of the army angrily denounced the emperor because the invading army had been destroyed as a result of his failure to carry out the plans faithfully agreed upon.\\\\n[6.6.2] And now Alexander refused to endure his indisposition and the stifling air any longer. The entire army was sick and the troops from Illyricum especially were seriously ill and dying, being accustomed to moist, cool air and to more food than they were being issued. Eager to set out for Antioch, Alexander ordered the army in Media to proceed to that city.\\\\n[6.6.3] [Winter 232/233] This army, in its advance, was almost totally destroyed in the mountains [of Armenia]; a great many soldiers suffered mutilation in the frigid country, and only a handful of the large number of troops who started the march managed to reach Antioch. The emperor led his own large force to that city, and many of them perished too; so the affair brought the greatest discontent to the army and the greatest dishonor to Alexander, who was betrayed by bad luck and bad judgment. Of the three armies into which he had divided his total force, the greater part was lost by various misfortunes - disease, war, and cold.\\\\n[6.6.4] In Antioch, Alexander was quickly revived by the cool air and good water of that city after the acrid drought in Mesopotamia, and the soldiers too recovered there. The emperor tried to console them for their sufferings by a lavish distribution of money, in the belief that this was the only way he could regain their good will. He assembled an army and prepared to march against the Persians again if they should give trouble and not remain'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Funny you should ask. I was just looking at an article on a \"Frozen\" clear coat. Gives a matte-like breakup of reflections - and ease the eyestrain on the \"Earl Scheib\". Maybe the paint supply store could help?\\\\nGene--Funny you should ask! Just yesterday I got my'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  99886,    347,   3904,   8105,     12,   1187,     13,    578,\n",
       "          24549,    315,    904,    832,    315,  30227,    347,  12843,   8105,\n",
       "             12,     16]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Football is a \", return_tensors=\"pt\")\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1543568/1977080693.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids=torch.tensor(input_tensor).to(device),\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Funny you should ask. I was just looking at an article on a \"Frozen\" clear coat. Gives a matte-like breakup of reflections - and ease the eyestrain on the \"Earl Scheib\". Maybe the paint supply store could help?\\nGene--Funny you should ask! Just yesterday I got my hands on a sample of the new Dupli-Color Satin Clear Coat. It\\'s a 2:1 mix of satin clear and regular clear, and it\\'s designed to be used as a topcoat over Dupli-Color\\'s line of satin colors. I haven\\'t tried it yet, so I can\\'t tell you how well it works, but it looks pretty good. It\\'s available at most auto parts stores.\\nI\\'ve also heard good things about PPG\\'s Ditzler line of flat clear coats, but I don\\'t know if they\\'re available anywhere other than PPG dealerships.\\nHope this helps!\\nMike\n",
      "Funny you should ask. I was just looking at an article on a \"Frozen\" clear coat. Gives a matte-like breakup of reflections - and ease the eyestrain on the \"Earl Scheib\". Maybe the paint supply store could help?\\nGene--Funny you should ask! Just yesterday I got my hands on a sample of the new Dupli-Color Satin Clear Coat. It\\'s a 2:1 mix of satin clear and regular clear, and it\\'s designed to be used as a topcoat over Dupli-Color\\'s line of satin colors. I haven\\'t tried it yet, so I can\\'t tell you how well it works, but it looks pretty good. It\\'s available at most auto parts stores.\\nI\\'ve also heard good things about PPG\\'s Ditzler line of flat clear coats, but I don\\'t know if they\\'re available anywhere other than PPG dealerships.\\nHope this helps!\\nMike\n",
      "Funny you should ask. I was just looking at an article on a \"Frozen\" clear coat. Gives a matte-like breakup of reflections - and ease the eyestrain on the \"Earl Scheib\". Maybe the paint supply store could help?\\nGene--Funny you should ask! Just yesterday I got my hands on a sample of the new Dupli-Color Satin Clear Coat. It\\'s a 2:1 mix of satin clear and regular clear, and it\\'s designed to be used as a topcoat over Dupli-Color\\'s line of satin colors. I haven\\'t tried it yet, so I can\\'t tell you how well it works, but it looks pretty good. It\\'s available at most auto parts stores.\\nI\\'ve also heard good things about PPG\\'\n"
     ]
    }
   ],
   "source": [
    "output = model.decoder.generate(\n",
    "    input_ids=torch.tensor(input_tensor).to(device),\n",
    "    num_beams=5,\n",
    "    max_new_tokens=512,\n",
    "    num_return_sequences=1,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "res = tokenizer.decode(output.sequences[0], skip_special_tokens=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2223703/3208913516.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids=torch.tensor(tokenizer.encode(\"Football is a \", return_tensors=\"pt\")).to(device)\n"
     ]
    }
   ],
   "source": [
    "input_ids=torch.tensor(tokenizer.encode(\"Football is a \", return_tensors=\"pt\")).to(device)\n",
    "labels = input_ids.clone()\n",
    "out = model.decoder(input_ids,labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2223703/1817206588.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids=torch.tensor(tokenizer.encode(\"Football is a \", return_tensors=\"pt\")).to(device),\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Football is a 24/7 sport, and you can't just turn it off,\" he said. \"It's not like baseball or basketball where you can take a few days off and then come back to it. You've got to be ready to go all the time. It's a completely different mindset.\"\\nHe added: \"I feel like I'm in the best shape I've ever been in. I'm stronger, faster, quicker and more explosive than I've ever been. I'm looking forward to getting out there and showing what I can do.\"\\nAs for his future with the Red Sox, Pedroia said: \"I don't know what's going to happen. I don't know what's going to happen. I don't know what's going to happen next year. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what's going to happen the year after that. I don't know what\n"
     ]
    }
   ],
   "source": [
    "output = model.decoder.generate(\n",
    "    input_ids=torch.tensor(tokenizer.encode(\"Football is a \", return_tensors=\"pt\")).to(device),\n",
    "    num_beams=5,\n",
    "    max_new_tokens=512,\n",
    "    num_return_sequences=1,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "res = tokenizer.decode(output.sequences[0], skip_special_tokens=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
