{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ensure moe-infinity is installed by 'pip install git+https://github.com/TorchMoE/MoE-Infinity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not detect pre-installed ops, use JIT mode\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration, TextStreamer\n",
    "from moe_infinity import MoE\n",
    "\n",
    "import safetensors.torch as st\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's much faster to use local file than blobfuse\n",
    "# we also need to convert ckpt format\n",
    "\n",
    "def download_and_convert_ckpt(mixtral_blob_path, nlm_blob_path, local_path):\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "    bar = tqdm(total=35)\n",
    "\n",
    "    tensor_index = {\n",
    "        \"metadata\": {\n",
    "            \"total_size\": 0\n",
    "        },\n",
    "        \"weight_map\": {}\n",
    "    }\n",
    "\n",
    "    # input emb\n",
    "    bar.set_description(\"input emb\")\n",
    "    ckpt_old = torch.load(os.path.join(nlm_blob_path, \"layer_00-model_states.pt\"), map_location='cpu')\n",
    "    ckpt_new_name = \"model_00.safetensors\"\n",
    "    emb_weight = ckpt_old[\"embed_tokens.weight\"]\n",
    "    ckpt_new = {\n",
    "        \"model.embed_tokens.weight\": emb_weight\n",
    "    }\n",
    "\n",
    "    tensor_index[\"metadata\"][\"total_size\"] += emb_weight.numel()\n",
    "    tensor_index[\"weight_map\"][\"model.embed_tokens.weight\"] = ckpt_new_name\n",
    "    st.save_file(ckpt_new, os.path.join(local_path, ckpt_new_name))\n",
    "    bar.update(1)\n",
    "\n",
    "    # layer 1 to 32\n",
    "    for i in range(0, 32):\n",
    "        bar.set_description(f\"layer {i+1}\")\n",
    "        ckpt_old = torch.load(os.path.join(nlm_blob_path, f\"layer_{i+1:02d}-model_states.pt\"), map_location='cpu')\n",
    "        ckpt_new_name = f\"model_{i+1:02d}.safetensors\"\n",
    "        ckpt_new = {}\n",
    "\n",
    "        # Attn QKVO proj\n",
    "        ckpt_new[f\"model.layers.{i}.self_attn.q_proj.weight\"] = ckpt_old[\"self_attn.q_proj.weight\"]\n",
    "        ckpt_new[f\"model.layers.{i}.self_attn.k_proj.weight\"] = ckpt_old[\"self_attn.k_proj.weight\"]\n",
    "        ckpt_new[f\"model.layers.{i}.self_attn.v_proj.weight\"] = ckpt_old[\"self_attn.v_proj.weight\"]\n",
    "        ckpt_new[f\"model.layers.{i}.self_attn.o_proj.weight\"] = ckpt_old[\"self_attn.o_proj.weight\"]\n",
    "\n",
    "        # MoE\n",
    "        for j in range(8):\n",
    "            ckpt_new[f\"model.layers.{i}.block_sparse_moe.experts.{j}.w1.weight\"] = ckpt_old[f\"block_sparse_moe.experts.{j}.w1.weight\"]\n",
    "            ckpt_new[f\"model.layers.{i}.block_sparse_moe.experts.{j}.w2.weight\"] = ckpt_old[f\"block_sparse_moe.experts.{j}.w2.weight\"]\n",
    "            ckpt_new[f\"model.layers.{i}.block_sparse_moe.experts.{j}.w3.weight\"] = ckpt_old[f\"block_sparse_moe.experts.{j}.w3.weight\"]\n",
    "        ckpt_new[f\"model.layers.{i}.block_sparse_moe.gate.weight\"] = ckpt_old[\"block_sparse_moe.gate.weight\"]\n",
    "\n",
    "        # LN\n",
    "        ckpt_new[f\"model.layers.{i}.input_layernorm.weight\"] = ckpt_old[\"input_layernorm.weight\"]\n",
    "        ckpt_new[f\"model.layers.{i}.post_attention_layernorm.weight\"] = ckpt_old[\"post_attention_layernorm.weight\"]\n",
    "\n",
    "        for k, v in ckpt_new.items():\n",
    "            tensor_index[\"metadata\"][\"total_size\"] += v.numel()\n",
    "            tensor_index[\"weight_map\"][k] = ckpt_new_name\n",
    "\n",
    "        st.save_file(ckpt_new, os.path.join(local_path, ckpt_new_name))\n",
    "        bar.update(1)\n",
    "\n",
    "    # Final norm\n",
    "    bar.set_description(\"final norm\")\n",
    "    ckpt_old = torch.load(os.path.join(nlm_blob_path, \"layer_33-model_states.pt\"), map_location='cpu')\n",
    "    ckpt_new_name = \"model_33.safetensors\"\n",
    "    emb_weight = ckpt_old[\"norm.weight\"]\n",
    "    ckpt_new = {\n",
    "        \"model.norm.weight\": emb_weight\n",
    "    }\n",
    "\n",
    "    tensor_index[\"metadata\"][\"total_size\"] += emb_weight.numel()\n",
    "    tensor_index[\"weight_map\"][\"model.norm.weight\"] = ckpt_new_name\n",
    "    st.save_file(ckpt_new, os.path.join(local_path, ckpt_new_name))\n",
    "    bar.update(1)\n",
    "\n",
    "    # LM head\n",
    "    bar.set_description(\"LM head\")\n",
    "    ckpt_old = torch.load(os.path.join(nlm_blob_path, \"layer_34-model_states.pt\"), map_location='cpu')\n",
    "    ckpt_new_name = \"model_34.safetensors\"\n",
    "    emb_weight = ckpt_old[\"lm_head.weight\"]\n",
    "    ckpt_new = {\n",
    "        \"lm_head.weight\": emb_weight\n",
    "    }\n",
    "\n",
    "    tensor_index[\"metadata\"][\"total_size\"] += emb_weight.numel()\n",
    "    tensor_index[\"weight_map\"][\"lm_head.weight\"] = ckpt_new_name\n",
    "    st.save_file(ckpt_new, os.path.join(local_path, ckpt_new_name))\n",
    "    bar.update(1)\n",
    "\n",
    "    with open(os.path.join(local_path, \"model.safetensors.index.json\"), \"w\") as f:\n",
    "        json.dump(tensor_index, f, indent=2)\n",
    "\n",
    "    print(f\"Maped {tensor_index['metadata']['total_size']} tensors\")\n",
    "\n",
    "    # Other config files\n",
    "    config = json.load(open(os.path.join(mixtral_blob_path, \"config.json\")))\n",
    "    config[\"vocab_size\"] = 33982\n",
    "    with open(os.path.join(local_path, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "    for file in [\"generation_config.json\", \"special_tokens_map.json\", \"tokenizer.json\", \"tokenizer.model\", \"tokenizer_config.json\"]:\n",
    "        shutil.copyfile(os.path.join(mixtral_blob_path, file), os.path.join(local_path, file))\n",
    "\n",
    "    # show file list in local_path\n",
    "    print(\"Files in local_path:\")\n",
    "    for root, dirs, files in os.walk(local_path):\n",
    "        for file in files:\n",
    "            print(os.path.relpath(os.path.join(root, file), local_path))\n",
    "    print(\"Done\")\n",
    "    bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_convert_ckpt(\n",
    "    \"/hai1/shufxi/Mixtral-8x7B-v0.1\",\n",
    "    \"/nlm/shufxi/nlm/8x7b/stageB/global_step54999\",\n",
    "    \"/tmp/nlm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/shufxi/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/shufxi/.cache/torch_extensions/py311_cu121/prefetch/build.ninja...\n",
      "Building extension module prefetch...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load prefetch op: 2.3728654384613037 seconds\n",
      "SPDLOG_LEVEL : (null)\n",
      "2024-05-10 08:06:09.635 INFO Create ArcherAioThread for thread: , 0\n",
      "2024-05-10 08:06:09.635 INFO Loading index file from , /tmp/moe-infinity/archer_index\n",
      "2024-05-10 08:06:09.636 INFO Index file size , 995\n",
      "2024-05-10 08:06:09.636 INFO Device count , 1\n",
      "2024-05-10 08:06:09.636 INFO Enabled peer access for all devices\n",
      "Loading model from offload_path ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module prefetch...\n",
      "Model create:   0%|          | 0/994 [00:00<?, ?it/s]MixtralBLockSparseTop2MLP is deprecated by MixtralBlockSparseTop2MLP and will be removed in v4.40.\n",
      "Model create:  91%|█████████ | 905/994 [00:00<00:00, 2330.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixtralConfig {\n",
      "  \"_name_or_path\": \"/tmp/nlm\",\n",
      "  \"architectures\": [\n",
      "    \"MixtralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mixtral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_experts_per_tok\": 2,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 8,\n",
      "  \"output_router_logits\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.02,\n",
      "  \"router_jitter_noise\": 0.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.40.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 33982\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint = '/tmp/nlm'\n",
    "config = {\n",
    "    \"offload_path\": \"/tmp/moe-infinity\",\n",
    "    \"device_memory_ratio\": 0.75,\n",
    "}\n",
    "\n",
    "model = MoE(checkpoint, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4204, 8192)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load('/nlm/shufxi/data/SFM.Mixtral.v0/valid.npy')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.from_numpy(data.astype('int64')).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac3d9689eff4cf6a4e92839952eb138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_sum = 0\n",
    "for i in tqdm(range(0, data.shape[0])):\n",
    "    with torch.no_grad():\n",
    "        input_ids = data[i].unsqueeze(0)\n",
    "        labels = input_ids.clone()\n",
    "        outputs = model(input_ids, labels=labels, return_dict=True)\n",
    "        loss = outputs.loss\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "print(loss_sum / data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4662233471870423"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0034876863634325456 * data.shape[0] / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sfm.data.sci_data.NlmTokenizer import NlmTokenizer\n",
    "tokenizer = NlmTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"An apple a day\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"<mol>C1=CC=CC=C1</mol> <mol>\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"<protein>MKQHKAMIVALIVICITAVVAALVTRKDLCEVHIRTGQTEVAVF</protein> <protein>\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfm_moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
