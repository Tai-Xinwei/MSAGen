{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ensure moe-infinity is installed by 'pip install git+https://github.com/TorchMoE/MoE-Infinity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not detect pre-installed ops, use JIT mode\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, SwitchTransformersForConditionalGeneration, TextStreamer\n",
    "from moe_infinity import MoE\n",
    "\n",
    "import safetensors.torch as st\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's much faster to use local file than blobfuse\n",
    "# we also need to convert ckpt format\n",
    "\n",
    "def download_and_convert_ckpt(mixtral_blob_path, nlm_blob_path, local_path):\n",
    "    os.makedirs(local_path, exist_ok=True)\n",
    "    bar = tqdm(total=35)\n",
    "\n",
    "    tensor_index = {\n",
    "        \"metadata\": {\n",
    "            \"total_size\": 0\n",
    "        },\n",
    "        \"weight_map\": {}\n",
    "    }\n",
    "\n",
    "    # input emb\n",
    "    bar.set_description(\"input emb\")\n",
    "    ckpt_old = torch.load(os.path.join(nlm_blob_path, \"layer_00-model_states.pt\"), map_location='cpu')\n",
    "    ckpt_new_name = \"model_00.safetensors\"\n",
    "    emb_weight = ckpt_old[\"embed_tokens.weight\"]\n",
    "    ckpt_new = {\n",
    "        \"model.embed_tokens.weight\": emb_weight\n",
    "    }\n",
    "\n",
    "    tensor_index[\"metadata\"][\"total_size\"] += emb_weight.numel()\n",
    "    tensor_index[\"weight_map\"][\"model.embed_tokens.weight\"] = ckpt_new_name\n",
    "    st.save_file(ckpt_new, os.path.join(local_path, ckpt_new_name))\n",
    "    bar.update(1)\n",
    "\n",
    "    # layer 1 to 32\n",
    "    for i in range(0, 32):\n",
    "        bar.set_description(f\"layer {i+1}\")\n",
    "        ckpt_old = torch.load(os.path.join(nlm_blob_path, f\"layer_{i+1:02d}-model_states.pt\"), map_location='cpu')\n",
    "        ckpt_new_name = f\"model_{i+1:02d}.safetensors\"\n",
    "        ckpt_new = {}\n",
    "\n",
    "        # Attn QKVO proj\n",
    "        ckpt_new[f\"model.layers.{i}.self_attn.q_proj.weight\"] = ckpt_old[\"self_attn.q_proj.weight\"]\n",
    "        ckpt_new[f\"model.layers.{i}.self_attn.k_proj.weight\"] = ckpt_old[\"self_attn.k_proj.weight\"]\n",
    "        ckpt_new[f\"model.layers.{i}.self_attn.v_proj.weight\"] = ckpt_old[\"self_attn.v_proj.weight\"]\n",
    "        ckpt_new[f\"model.layers.{i}.self_attn.o_proj.weight\"] = ckpt_old[\"self_attn.o_proj.weight\"]\n",
    "\n",
    "        # MoE\n",
    "        for j in range(8):\n",
    "            ckpt_new[f\"model.layers.{i}.block_sparse_moe.experts.{j}.w1.weight\"] = ckpt_old[f\"block_sparse_moe.experts.{j}.w1.weight\"]\n",
    "            ckpt_new[f\"model.layers.{i}.block_sparse_moe.experts.{j}.w2.weight\"] = ckpt_old[f\"block_sparse_moe.experts.{j}.w2.weight\"]\n",
    "            ckpt_new[f\"model.layers.{i}.block_sparse_moe.experts.{j}.w3.weight\"] = ckpt_old[f\"block_sparse_moe.experts.{j}.w3.weight\"]\n",
    "        ckpt_new[f\"model.layers.{i}.block_sparse_moe.gate.weight\"] = ckpt_old[\"block_sparse_moe.gate.weight\"]\n",
    "\n",
    "        # LN\n",
    "        ckpt_new[f\"model.layers.{i}.input_layernorm.weight\"] = ckpt_old[\"input_layernorm.weight\"]\n",
    "        ckpt_new[f\"model.layers.{i}.post_attention_layernorm.weight\"] = ckpt_old[\"post_attention_layernorm.weight\"]\n",
    "\n",
    "        for k, v in ckpt_new.items():\n",
    "            tensor_index[\"metadata\"][\"total_size\"] += v.numel()\n",
    "            tensor_index[\"weight_map\"][k] = ckpt_new_name\n",
    "\n",
    "        st.save_file(ckpt_new, os.path.join(local_path, ckpt_new_name))\n",
    "        bar.update(1)\n",
    "\n",
    "    # Final norm\n",
    "    bar.set_description(\"final norm\")\n",
    "    ckpt_old = torch.load(os.path.join(nlm_blob_path, \"layer_33-model_states.pt\"), map_location='cpu')\n",
    "    ckpt_new_name = \"model_33.safetensors\"\n",
    "    emb_weight = ckpt_old[\"norm.weight\"]\n",
    "    ckpt_new = {\n",
    "        \"model.norm.weight\": emb_weight\n",
    "    }\n",
    "\n",
    "    tensor_index[\"metadata\"][\"total_size\"] += emb_weight.numel()\n",
    "    tensor_index[\"weight_map\"][\"model.norm.weight\"] = ckpt_new_name\n",
    "    st.save_file(ckpt_new, os.path.join(local_path, ckpt_new_name))\n",
    "    bar.update(1)\n",
    "\n",
    "    # LM head\n",
    "    bar.set_description(\"LM head\")\n",
    "    ckpt_old = torch.load(os.path.join(nlm_blob_path, \"layer_34-model_states.pt\"), map_location='cpu')\n",
    "    ckpt_new_name = \"model_34.safetensors\"\n",
    "    emb_weight = ckpt_old[\"lm_head.weight\"]\n",
    "    ckpt_new = {\n",
    "        \"lm_head.weight\": emb_weight\n",
    "    }\n",
    "\n",
    "    tensor_index[\"metadata\"][\"total_size\"] += emb_weight.numel()\n",
    "    tensor_index[\"weight_map\"][\"lm_head.weight\"] = ckpt_new_name\n",
    "    st.save_file(ckpt_new, os.path.join(local_path, ckpt_new_name))\n",
    "    bar.update(1)\n",
    "\n",
    "    with open(os.path.join(local_path, \"model.safetensors.index.json\"), \"w\") as f:\n",
    "        json.dump(tensor_index, f, indent=2)\n",
    "\n",
    "    print(f\"Maped {tensor_index['metadata']['total_size']} tensors\")\n",
    "\n",
    "    # Other config files\n",
    "    config = json.load(open(os.path.join(mixtral_blob_path, \"config.json\")))\n",
    "    config[\"vocab_size\"] = 33982\n",
    "    with open(os.path.join(local_path, \"config.json\"), \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "    for file in [\"generation_config.json\", \"special_tokens_map.json\", \"tokenizer.json\", \"tokenizer.model\", \"tokenizer_config.json\"]:\n",
    "        shutil.copyfile(os.path.join(mixtral_blob_path, file), os.path.join(local_path, file))\n",
    "\n",
    "    # show file list in local_path\n",
    "    print(\"Files in local_path:\")\n",
    "    for root, dirs, files in os.walk(local_path):\n",
    "        for file in files:\n",
    "            print(os.path.relpath(os.path.join(root, file), local_path))\n",
    "    print(\"Done\")\n",
    "    bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3276cbb3fcb642b79e4eb85ae84e3e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maped 46719029248 tensors\n",
      "Files in local_path:\n",
      "model_32.safetensors\n",
      "model_20.safetensors\n",
      "model.safetensors.index.json\n",
      "model_34.safetensors\n",
      "model_29.safetensors\n",
      "model_00.safetensors\n",
      "model_11.safetensors\n",
      "model_06.safetensors\n",
      "generation_config.json\n",
      "model_13.safetensors\n",
      "model_09.safetensors\n",
      "model_14.safetensors\n",
      "tokenizer.json\n",
      "special_tokens_map.json\n",
      "model_07.safetensors\n",
      "model_10.safetensors\n",
      "tokenizer_config.json\n",
      "config.json\n",
      "model_16.safetensors\n",
      "model_03.safetensors\n",
      "model_30.safetensors\n",
      "model_22.safetensors\n",
      "model_08.safetensors\n",
      "model_23.safetensors\n",
      "model_12.safetensors\n",
      "model_24.safetensors\n",
      "model_01.safetensors\n",
      "model_21.safetensors\n",
      "model_17.safetensors\n",
      "model_19.safetensors\n",
      "model_18.safetensors\n",
      "model_04.safetensors\n",
      "model_15.safetensors\n",
      "tokenizer.model\n",
      "model_33.safetensors\n",
      "model_05.safetensors\n",
      "model_28.safetensors\n",
      "model_02.safetensors\n",
      "model_31.safetensors\n",
      "model_26.safetensors\n",
      "model_27.safetensors\n",
      "model_25.safetensors\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "download_and_convert_ckpt(\n",
    "    \"/hai1/shufxi/Mixtral-8x7B-v0.1\",\n",
    "    \"/nlm/shufxi/nlm/8x7b/stageA/global_step3999\",\n",
    "    \"/tmp/nlm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/shufxi/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/shufxi/.cache/torch_extensions/py310_cu121/prefetch/build.ninja...\n",
      "Building extension module prefetch...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load prefetch op: 2.3624579906463623 seconds\n",
      "SPDLOG_LEVEL : (null)\n",
      "2024-04-26 09:10:43.151 INFO Create ArcherAioThread for thread: , 0\n",
      "2024-04-26 09:10:43.151 INFO Loading index file from , /tmp/moe-infinity/archer_index\n",
      "2024-04-26 09:10:43.152 INFO Index file size , 995\n",
      "2024-04-26 09:10:43.152 INFO Device count , 1\n",
      "2024-04-26 09:10:43.152 INFO Enabled peer access for all devices\n",
      "Loading model from offload_path ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module prefetch...\n",
      "Model create:   1%|          | 6/994 [00:00<00:20, 47.61it/s]MixtralBLockSparseTop2MLP is deprecated by MixtralBlockSparseTop2MLP and will be removed in v4.40.\n",
      "Model create:  91%|█████████ | 905/994 [00:03<00:00, 251.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixtralConfig {\n",
      "  \"_name_or_path\": \"/tmp/nlm\",\n",
      "  \"architectures\": [\n",
      "    \"MixtralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mixtral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_experts_per_tok\": 2,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 8,\n",
      "  \"output_router_logits\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.02,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.39.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 33982\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model create:  94%|█████████▎| 930/994 [00:19<00:00, 251.00it/s]"
     ]
    }
   ],
   "source": [
    "checkpoint = '/tmp/nlm'\n",
    "config = {\n",
    "    \"offload_path\": \"/tmp/moe-infinity\",\n",
    "    \"device_memory_ratio\": 0.75,\n",
    "}\n",
    "\n",
    "model = MoE(checkpoint, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-26 09:20:43,565] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'NlmTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[32m2024-04-26 09:20:48.061\u001b[0m][\u001b[36mINFO\u001b[0m]: Tokenizer has 33982 tokens\n"
     ]
    }
   ],
   "source": [
    "from sfm.data.sci_data.NlmTokenizer import NlmTokenizer\n",
    "tokenizer = NlmTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shufxi/anaconda3/envs/sfm_moe/lib/python3.10/site-packages/transformers/generation/utils.py:1460: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>An apple a day keeps the doctor away.\n",
      "\n",
      "We’ve all heard this saying, but is it true?\n"
     ]
    }
   ],
   "source": [
    "input_text = \"An apple a day\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <mol> <m>C <m>1 <m>= <m>C <m>C <m>= <m>C <m>C <m>= <m>C <m>1 </mol> <mol> <m>C <m>C <m>C <m>C <m>C <m>C a great place to visit.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"<mol>C1=CC=CC=C1</mol> <mol>\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <protein> <a>M <a>K <a>Q <a>H <a>K <a>A <a>M <a>I <a>V <a>A <a>L <a>I <a>V <a>I <a>C <a>I <a>T <a>A <a>V <a>V <a>A <a>A <a>L <a>V <a>T <a>R <a>K <a>D <a>L <a>C <a>E <a>V <a>H <a>I <a>R <a>T <a>G <a>Q <a>T <a>E <a>V <a>A <a>V <a>F </protein> <protein> <a>M <a>L <a>L <a>L <a>L <a>L <a>L <a>V <a>V <a>I <a>L <a>A <a>L <a>A <a>L <a>A <a>L <a>A <a>L <a>A\n"
     ]
    }
   ],
   "source": [
    "input_text = \"<protein>MKQHKAMIVALIVICITAVVAALVTRKDLCEVHIRTGQTEVAVF</protein> <protein>\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfm_moe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
