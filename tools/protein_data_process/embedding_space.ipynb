{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import numpy as np\n",
    "sys.path.append('/home/yaosenmin/SFM_framework/')\n",
    "\n",
    "from pathlib import Path\n",
    "from finetune_pfm_v2 import init_model, load_batched_dataset, multi_label_transform\n",
    "from sfm.utils.move_to_device import move_to_device\n",
    "from sfm.data.prot_data.dataset import FoundationModelDataset\n",
    "from sfm.data.prot_data.collater import pad_1d_unsqueeze\n",
    "from sfm.data.prot_data.vocalubary import Alphabet\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = \"svg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "args = Namespace()\n",
    "args.task_name = \"solubility\"\n",
    "args.max_length = 2048\n",
    "args.base_model = 'pfm'\n",
    "\n",
    "# model\n",
    "args.encoder_layers=33\n",
    "args.encoder_embed_dim=1280\n",
    "args.encoder_ffn_embed_dim=5120\n",
    "args.encoder_attention_heads=20\n",
    "args.num_pred_attn_layer=2\n",
    "args.atom_loss_coeff=1.0\n",
    "args.pos_loss_coeff=1.0\n",
    "args.sandwich_ln=True\n",
    "args.ft=True\n",
    "args.dropout=0\n",
    "args.fp16=True\n",
    "args.attn_dropout=0\n",
    "args.act_dropout=0\n",
    "args.weight_decay=0.0\n",
    "args.droppath_prob=0.0\n",
    "args.max_num_aa=2048\n",
    "args.noise_mode='diff'\n",
    "args.noise_scale=0.2\n",
    "args.mask_ratio=0.2\n",
    "args.mode_prob=1.0,0.0,0.0\n",
    "args.d_tilde=1.0\n",
    "args.max_lr=\"1e-5\"\n",
    "args.strategy=\"DDP\"\n",
    "args.pipeline_model_parallel_size=0\n",
    "args.train_batch_size=64\n",
    "args.val_batch_size=61\n",
    "args.max_tokens=6400\n",
    "args.max_length=2048\n",
    "args.train_data_path=\"None\"\n",
    "args.valid_data_path=\"None\"\n",
    "args.data_basepath=\"/blob/data/bfm_benchmark\"\n",
    "args.task_name=\"solubility\"\n",
    "# args.loadcheck_path=\"/home/yaosen/blob/pfmexp/output/bfm650m_maskspan1_ddp4e5d8mask020drop1L1536_pairv4_bert2_128V100_adam2/checkpoints/checkpoint_E45.pt\"\n",
    "# args.loadcheck_path=\"/blob/pfmexp/output/bfm650m_maskspan3_ddp4e5d16mask020drop1L1536B2k_bpepairv4_bert2_128A100_adam2/checkpoints/checkpoint_E63.pt\"\n",
    "args.loadcheck_path = \"/blob/pfmexp/output/finetune/finetune-solubility_seed21_lr2e-5_E50_bs32_bfm650m_maskspan1_ddp4e5d8mask020drop1L1536_pairv4_bert2_128V100_adam2/checkpoint_E3.pt\"\n",
    "# emm\n",
    "args.add_3d=False\n",
    "args.num_3d_bias_kernel=0\n",
    "args.no_2d=False\n",
    "args.rank=0\n",
    "args.num_residues=32\n",
    "args.t_timesteps=0\n",
    "args.ddpm_schedule='cosine'\n",
    "args.ddpm_beta_start=0.0\n",
    "args.ddpm_beta_end=0.0\n",
    "args.head_dropout=0.0\n",
    "args.early_stopping_patience=0\n",
    "args.early_stopping_metric='loss'\n",
    "args.early_stopping_mode='min'\n",
    "args.grad_scaler_init = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastaDataset(FoundationModelDataset):\n",
    "    def __init__(self, args) -> None:\n",
    "        super().__init__()\n",
    "        self.args = self.set_default_args(args)\n",
    "        self.vocab = Alphabet()\n",
    "        self.seqs = []\n",
    "        self.names = []\n",
    "        for record in SeqIO.parse(args.fasta_file, \"fasta\"):\n",
    "            self.names.append(record.description)\n",
    "            self.seqs.append(str(record.seq).upper())\n",
    "\n",
    "    def set_default_args(self, args):\n",
    "        if not hasattr(args, \"max_length\"):\n",
    "            args.max_length = 2048\n",
    "        if not hasattr(args, \"fasta_file\"):\n",
    "            raise ValueError(\"Please specify fasta_file\")\n",
    "        \n",
    "    def __getitem__(self, index: int) -> dict:\n",
    "        item = {\"id\": index, 'name': self.names[index], \"aa\": self.seqs[index]}\n",
    "        tokens = [self.vocab.tok_to_idx[tok] for tok in item[\"aa\"]]\n",
    "        if self.vocab.prepend_bos:\n",
    "            tokens.insert(0, self.vocab.cls_idx)\n",
    "        if self.vocab.append_eos:\n",
    "            tokens.append(self.vocab.eos_idx)\n",
    "        item[\"aa\"] = np.array(tokens, dtype=np.int64)\n",
    "        return item\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def size(self, index: int) -> int:\n",
    "        return len(self.seqs[index])\n",
    "\n",
    "    def num_tokens(self, index: int) -> int:\n",
    "        return len(self.seqs[index]) + 2\n",
    "\n",
    "    def num_tokens_vec(self, indices):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def collate(self, samples: list) -> dict:\n",
    "        max_tokens = max(len(s[\"aa\"]) for s in samples)\n",
    "        batch = dict()\n",
    "\n",
    "        batch[\"id\"] = torch.tensor([s[\"id\"] for s in samples], dtype=torch.long)\n",
    "        batch[\"naa\"] = torch.tensor([len(s[\"aa\"]) for s in samples], dtype=torch.long)\n",
    "        batch[\"name\"] = [s[\"name\"] for s in samples]\n",
    "        # (Nres+2,) -> (B, Nres+2)\n",
    "        batch[\"x\"] = torch.cat(\n",
    "            [\n",
    "                pad_1d_unsqueeze(\n",
    "                    torch.from_numpy(s[\"aa\"]), max_tokens, 0, self.vocab.padding_idx\n",
    "                )\n",
    "                for s in samples\n",
    "            ]\n",
    "        )\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_fn(rank, world_size, args, load_ckpt, batches):\n",
    "    device = f\"cuda:{rank % world_size}\"\n",
    "    model = init_model(args, load_ckpt=load_ckpt)\n",
    "\n",
    "    # load downstream ckpt\n",
    "    checkpoints_state = torch.load(args.loadcheck_path, map_location=\"cpu\")\n",
    "    if \"model\" in checkpoints_state:\n",
    "        checkpoints_state = checkpoints_state[\"model\"]\n",
    "    elif \"module\" in checkpoints_state:\n",
    "        checkpoints_state = checkpoints_state[\"module\"]\n",
    "\n",
    "    IncompatibleKeys = model.load_state_dict(checkpoints_state, strict=False)\n",
    "    IncompatibleKeys = IncompatibleKeys._asdict()\n",
    "    print(f\"checkpoint: {args.loadcheck_path} is loaded\")\n",
    "    print(f\"Following keys are incompatible: {IncompatibleKeys.keys()}\")\n",
    "    # end\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    embeds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(batches, ncols=80, desc=f\"Rank {rank}\"):\n",
    "            batch = move_to_device(batch, device)\n",
    "            # B, L, D\n",
    "            # embed = model.model.ft_forward(batch)\n",
    "            embed = model(batch)\n",
    "            # seq_mask = ~(batch[\"x\"].eq(0) | batch[\"x\"].eq(1) | batch[\"x\"].eq(2))\n",
    "            # seq_length = seq_mask.sum(dim=1)\n",
    "            # embed[seq_mask] = 0.0\n",
    "            # embed = embed.sum(dim=1) / seq_length.unsqueeze(dim=1)\n",
    "            embeds.append(embed.cpu().numpy())\n",
    "            targets.append(batch[\"name\"])\n",
    "            # targets.append(batch['target'].squeeze())\n",
    "    return np.concatenate(embeds, axis=0), np.concatenate(targets, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCOPe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.fasta_file = '/blob/data/astral-scopedom-seqres-sel-gs-bib-40-2.08.fa'\n",
    "scope = FastaDataset(args)\n",
    "scope_loader = torch.utils.data.DataLoader(\n",
    "    scope,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=scope.collate,\n",
    "    pin_memory=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "world_size = 1\n",
    "# all_batches = [batch for batch in tqdm(scope_loader, ncols=80, total=len(scope_loader))]\n",
    "\n",
    "# chunks = lambda l, n: [l[i : i + n] for i in range(0, len(l), n)]\n",
    "# batch_chunks = list(chunks(all_batches, len(all_batches) // world_size + 1))\n",
    "# print(f\"{len(all_batches)}, {sum([len(c) for c in batch_chunks])}\")\n",
    "\n",
    "# result = Parallel(n_jobs=world_size)(\n",
    "#     delayed(embed_fn)(rank, world_size, args, True, batch)\n",
    "#     for rank, batch in enumerate(scope_loader)\n",
    "# )\n",
    "\n",
    "embeddings, descriptions = embed_fn(0, 1, args, True, scope_loader)\n",
    "\n",
    "# embeddings = np.concatenate([r[0] for r in result], axis=0)\n",
    "# descriptions = np.concatenate([r[1] for r in result], axis=0)\n",
    "# np.savez('scope_embed.npz', embeddings=embeddings, descriptions=descriptions)\n",
    "\n",
    "\n",
    "classes = []\n",
    "classes_dict = {\n",
    "    \"a\": \"a: All alpha\",\n",
    "    \"b\": \"b: All beta\",\n",
    "    \"c\": \"c: Alpha & beta (a/b)\",\n",
    "    \"d\": \"d: Alpha & beta (a+b)\",\n",
    "    \"e\": \"e: Multi-domain\",\n",
    "    \"f\": \"f: Membrane, cell surface\",\n",
    "    \"g\": \"g: Small proteins\",\n",
    "}\n",
    "for d in descriptions:\n",
    "    item = d.split(\" \")[1]\n",
    "    classes.append(classes_dict[item[0]])\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(classes))\n",
    "print(embeddings.shape)\n",
    "\n",
    "# mask = [c[0] in ['a', 'b', 'c', 'g'] for c in classes]\n",
    "# sub_embed = embeddings[mask]\n",
    "# sub_classes = [c for idx, c in enumerate(classes) if mask[idx]]\n",
    "\n",
    "import umap\n",
    "import umap.plot\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, verbose=1, metric='nan_euclidean', )#perplexity=10, early_exaggeration=100,)#perplexity=50)\n",
    "tsne_embed = tsne.fit_transform(embeddings)\n",
    "reducer = umap.UMAP(n_jobs=64, n_neighbors=15, min_dist=0.0, repulsion_strength=5.0,)\n",
    "mapper = reducer.fit(embeddings)\n",
    "umap_embed = reducer.transform(embeddings)\n",
    "\n",
    "ax = umap.plot.points(mapper, points=tsne_embed, labels=np.array(classes), color_key_cmap='tab10',)\n",
    "\n",
    "\n",
    "ax.set_title(f'TSNE clustering SCOPe', fontsize=12)\n",
    "ax.texts[0].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# solubility embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if finetune: True\n",
      "checkpoint: /blob/pfmexp/output/finetune/finetune-solubility_seed21_lr2e-5_E50_bs32_bfm650m_maskspan1_ddp4e5d8mask020drop1L1536_pairv4_bert2_128V100_adam2/checkpoint_E3.pt is loaded\n",
      "Following keys are incompatible: dict_keys(['missing_keys', 'unexpected_keys'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rank 0: 100%|█████████████████████████████████████| 7/7 [00:13<00:00,  1.95s/it]\n"
     ]
    }
   ],
   "source": [
    "args.fasta_file = '/home/yaosenmin/high.fa'\n",
    "solu = FastaDataset(args)\n",
    "solu_loader = torch.utils.data.DataLoader(\n",
    "    solu,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=solu.collate,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "pred, label = embed_fn(0, 1, args, True, solu_loader)\n",
    "pred, label = pred.squeeze(), label.squeeze()\n",
    "\n",
    "\n",
    "# train_dataset, valid_dataset, test_dataset_dict = load_batched_dataset(args)\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     test_dataset_dict['test'],\n",
    "#     batch_size=16,\n",
    "#     shuffle=False,\n",
    "#     num_workers=0,\n",
    "#     collate_fn=test_dataset_dict['test'].collate,\n",
    "#     pin_memory=True,\n",
    "# )\n",
    "\n",
    "# world_size = 16\n",
    "# all_batches = [batch for batch in tqdm(test_loader, ncols=80, total=len(test_loader))]\n",
    "\n",
    "# chunks = lambda l, n: [l[i : i + n] for i in range(0, len(l), n)]\n",
    "# batch_chunks = list(chunks(all_batches, len(all_batches) // world_size + 1))\n",
    "# print(f\"{len(all_batches)}, {sum([len(c) for c in batch_chunks])}\")\n",
    "\n",
    "# pred = np.concatenate([r[0] for r in result], axis=0).squeeze()\n",
    "# label = np.concatenate([r[1] for r in result], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset, test_dataset_dict = load_batched_dataset(args)\n",
    "\n",
    "idx2tok = {v: k for k, v in train_dataset.vocab.tok_to_idx.items()}\n",
    "with open('/home/yaosenmin/solubility_train.csv', 'w') as f:\n",
    "    print('label,seq', file=f)\n",
    "    for item in train_dataset:\n",
    "        target = item['target'].item()\n",
    "        seq = ''.join([idx2tok[idx] for idx in item['aa'] if idx not in [0, 1, 2]])\n",
    "        print(f'{target},{seq}', file=f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "sigmoid = lambda z: 1/(1 + np.exp(-z))\n",
    "prob = sigmoid(pred)\n",
    "df = pd.DataFrame({\"pred\": prob, \"label\": label})\n",
    "sns.boxplot(x=\"label\", y=\"pred\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.457914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.142577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.061834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.362893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.455072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.558226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.763785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred\n",
       "count  100.000000\n",
       "mean     0.457914\n",
       "std      0.142577\n",
       "min      0.061834\n",
       "25%      0.362893\n",
       "50%      0.455072\n",
       "75%      0.558226\n",
       "max      0.763785"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label'] == 'high'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.457914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.142577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.061834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.362893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.455072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.558226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.763785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred\n",
       "count  100.000000\n",
       "mean     0.457914\n",
       "std      0.142577\n",
       "min      0.061834\n",
       "25%      0.362893\n",
       "50%      0.455072\n",
       "75%      0.558226\n",
       "max      0.763785"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['label'] == 'low'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AA embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.fasta_file = '/home/yaosenmin/aa.fa'\n",
    "aa_data = FastaDataset(args)\n",
    "aa_loader = torch.utils.data.DataLoader(\n",
    "    aa_data,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=aa_data.collate,\n",
    "    pin_memory=True,\n",
    ")\n",
    "world_size = 1\n",
    "all_batches = [batch for batch in tqdm(aa_loader, ncols=80, total=len(aa_loader))]\n",
    "\n",
    "chunks = lambda l, n: [l[i : i + n] for i in range(0, len(l), n)]\n",
    "batch_chunks = list(chunks(all_batches, len(all_batches) // world_size + 1))\n",
    "print(f\"{len(all_batches)}, {sum([len(c) for c in batch_chunks])}\")\n",
    "\n",
    "result = Parallel(n_jobs=world_size)(\n",
    "    delayed(embed_fn)(rank, world_size, args, True, batch)\n",
    "    for rank, batch in enumerate(batch_chunks)\n",
    ")\n",
    "\n",
    "aa_dict = {a: arr for a, arr in zip(\"ARNDBCEQZGHILKMFPSTWYVXOU\", result[0][0])}\n",
    "np.savez(\"aa.npz\", **aa_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Jul 14 22:59:05 2019\n",
    "\n",
    "@author: mheinzinger\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set(); sns.set_style(\"whitegrid\")\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "def plot_tsne( data, fig_path, SEQ, MARKER, CLASSES, SIZES ):\n",
    "\n",
    "    colors  = sns.color_palette(\"Paired\", len(SEQ))\n",
    "    colors = [ colors[i] for i in range(1,len(SEQ), 2)]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.grid(False)\n",
    "        \n",
    "    for idx, AA in enumerate( SEQ ):\n",
    "        # x,y coords for t-sne and marker\n",
    "        x  = data[idx, 0]\n",
    "        y  = data[idx, 1]\n",
    "        mark = MARKER[idx]\n",
    "        size = SIZES[AA]\n",
    "        \n",
    "        if size < 130:\n",
    "            size=60\n",
    "        elif size > 130 and size < 150:\n",
    "            size=150\n",
    "        else:\n",
    "            size=300\n",
    "        \n",
    "        # color\n",
    "        AA_class = CLASSES[ mark ][0]\n",
    "        label    = CLASSES[ mark ][1]\n",
    "        color = np.expand_dims( np.asarray( colors[AA_class]), axis=0 )\n",
    "        if AA == 'C':\n",
    "            mark = 'v'\n",
    "            AA_class = CLASSES[ mark ][0]\n",
    "            label    = CLASSES[ mark ][1]\n",
    "            color = np.expand_dims( np.asarray( colors[AA_class]), axis=0 )\n",
    "            sns.scatterplot( x=[x], y=[y-1], marker=mark, label=label, s=size/2, color=color, ax=ax, linewidth=0 ) # Needs to be adjusted as well if labels are off. Originally 13\n",
    "            mark = '^'\n",
    "            AA_class = CLASSES[ mark ][0]\n",
    "            label    = CLASSES[mark ][1]\n",
    "            color = np.expand_dims( np.asarray( colors[AA_class]), axis=0 )\n",
    "            sns.scatterplot( x=[x], y=[y+1], marker=mark, label=label, s=size/2, color=color, ax=ax, linewidth=0 )\n",
    "        else:\n",
    "            sns.scatterplot( x=[x], y=[y], marker=mark, label=label, s=size, color=color, ax=ax )\n",
    "        plt.text(x+1, y+1, AA, fontsize=14) # TODO: needs to be adjusted if labels are off. Originally, 15\n",
    "        \n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    \n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    \n",
    "    # manually define a new patch \n",
    "    legend_elements = [ Line2D([0], [0], marker='o', color='black', label='Small',  markersize= 60**(1/2), linestyle='None'),\n",
    "                        Line2D([0], [0], marker='o', color='black', label='Medium', markersize=150**(1/2), linestyle='None'),\n",
    "                        Line2D([0], [0], marker='o', color='black', label='Big',    markersize=300**(1/2), linestyle='None') ]\n",
    "    \n",
    "    #handles is a list, so append manual patch\n",
    "    handles += legend_elements\n",
    "    labels  += ['Small (<130 Dalton)','Medium','Big (>150 Dalton)']\n",
    "    \n",
    "\n",
    "    by_label = dict( zip(labels, handles) )\n",
    "    #by_label = sorted(by_label.items())\n",
    "    label_sorting = [ 'Hydrophobic (aromatic)',\n",
    "                      'Hydrophobic (aliphatic)',\n",
    "                      'Positive',               \n",
    "                      'Negative',                \n",
    "                      'Polar neutral',           \n",
    "                      'Special cases',                    \n",
    "                      'Small (<130 Dalton)',                   \n",
    "                      'Medium',                  \n",
    "                      'Big (>150 Dalton)',                     \n",
    "                      ]\n",
    "    \n",
    "    labels, handles = list(), list()\n",
    "    for label in label_sorting:\n",
    "        labels.append(label)\n",
    "        handles.append( by_label[label] )\n",
    "    \n",
    "    #by_label = list( zip(*by_label) )\n",
    "    \n",
    "    lgd = ax.legend( handles, labels,\n",
    "                    loc='upper left', bbox_to_anchor=(0., -0.03 ), ncol=2, frameon=False, \n",
    "                         borderaxespad=0., markerscale=1 )\n",
    "    for lh in lgd.legendHandles:  # removes opacity from legend\n",
    "        try:\n",
    "            lh.set_sizes([100.0])\n",
    "        except AttributeError:\n",
    "            continue\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # write figures as PDF to disk\n",
    "    fig.savefig( fig_path, format='pdf', bbox_inches='tight' ) \n",
    "    plt.close(fig) # close figure handle\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_tsne_rep( data, perp, n_iter ):\n",
    "    from sklearn.metrics import pairwise_distances\n",
    "    distance_matrix = pairwise_distances( data, data, metric='cosine', n_jobs=-1)\n",
    "\n",
    "    from sklearn.manifold import TSNE\n",
    "    trafo_data = TSNE(    n_components= 2, \n",
    "                          perplexity  = perp,\n",
    "                          init        = 'random', \n",
    "                          random_state= 42, \n",
    "                          n_iter      = n_iter,\n",
    "                          verbose     = 1,\n",
    "                          metric      = 'precomputed'\n",
    "                          ).fit_transform(distance_matrix)\n",
    "    \n",
    "    return trafo_data\n",
    "\n",
    "\n",
    "def remove_ambigious(aa_embd, SEQ, MARKER):\n",
    "    \n",
    "    def _remove_idx( aa_embd, string_1, string_2, to_be_removed):\n",
    "        idx = string_1.index(to_be_removed)\n",
    "        string_1 = string_1[:idx] + string_1[idx+1:]\n",
    "        string_2 = string_2[:idx] + string_2[idx+1:]\n",
    "        aa_embd= np.vstack( (aa_embd[:idx], aa_embd[idx+1:]))\n",
    "        return aa_embd, string_1, string_2\n",
    "    \n",
    "    for non_std_aa in \"ZBOU\":\n",
    "        aa_embd, SEQ, MARKER = _remove_idx( aa_embd, SEQ, MARKER, non_std_aa )\n",
    "\n",
    "    return aa_embd, SEQ, MARKER\n",
    "    \n",
    "    \n",
    "def aa_plot( root_dir, fig_path, perp, n_iter, use_standard_aas=True ):\n",
    "    SEQ     = \"ARNDBCEQZGHILKMFPSTWYVXOU\" # 20 standard AAs + rare + ambigious (B,Z)\n",
    "    MARKER  = \"vPoXodXoodPvvPv^doo^^vooo\"\n",
    "    CLASSES = { 'v': ( 0, 'Hydrophobic (aliphatic)' ),\n",
    "                '^': ( 1, 'Hydrophobic (aromatic)'),\n",
    "                'P': ( 2, 'Positive'),\n",
    "                'X': ( 3, 'Negative'),\n",
    "                'o': ( 4, 'Polar neutral'),\n",
    "                'd': ( 5, 'Special cases') \n",
    "                }\n",
    "\n",
    "    SIZES = { 'A' : 89, \n",
    "              'R' : 174,\n",
    "              'N' : 132,\n",
    "              'D' : 133,\n",
    "              'B' : 133,\n",
    "              'C' : 121,\n",
    "              'E' : 147,\n",
    "              'Q' : 146,\n",
    "              'Z' : 133,\n",
    "              'G' : 75,\n",
    "              'H' : 155,\n",
    "              'I' : 131,\n",
    "              'L' : 131,\n",
    "              'K' : 146,\n",
    "              'M' : 149,\n",
    "              'F' : 165,\n",
    "              'P' : 115,\n",
    "              'S' : 105,\n",
    "              'T' : 119,\n",
    "              'W' : 204,\n",
    "              'Y' : 181,\n",
    "              'V' : 117,\n",
    "              'X' : 133,\n",
    "              'O' : 133,\n",
    "              'U' : 133\n",
    "              }\n",
    "    \n",
    "    #npz_path = root_dir / 'single_aas.npz'\n",
    "    npz_path = Path('aa.npz') # TODO: insert path to npz with single amino acids as keys and embeddings as values here\n",
    "    aa_embd  = dict(np.load( npz_path, mmap_mode='r'))\n",
    "    \n",
    "    if len(aa_embd.keys()) > 1:\n",
    "        tmp_embd = list()\n",
    "        for aa in SEQ:\n",
    "            tmp_embd.append( aa_embd[aa] )\n",
    "        aa_embd = dict()\n",
    "        aa_embd['>single_aas'] = np.vstack(tmp_embd)\n",
    "    \n",
    "    aa_embd  = next(iter(aa_embd.values()))[:len(SEQ),:]\n",
    "    if use_standard_aas:\n",
    "        aa_embd, SEQ, MARKER = remove_ambigious(aa_embd, SEQ, MARKER)\n",
    "    \n",
    "    print(aa_embd.shape)\n",
    "    aa_tsne  = get_tsne_rep( aa_embd, perp, n_iter )\n",
    "    print(aa_tsne.shape)\n",
    "    plot_tsne( aa_tsne, fig_path, SEQ, MARKER, CLASSES, SIZES )\n",
    "    return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    root_dir = Path.cwd()\n",
    "    fig_root = root_dir # / 'electra_discriminator' / 'fig_single_aas'\n",
    "\n",
    "    # perplexity for t-SNE plots\n",
    "    perp = 3\n",
    "    n_iter =15000\n",
    "    \n",
    "    fig_path = fig_root / 'tsne_aa_perp{}_niter{}_withX_single_aas_bert_BFD.pdf'.format(perp, n_iter)\n",
    "    aa_plot( root_dir, fig_path, perp, n_iter, use_standard_aas=True )\n",
    "    return None\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
