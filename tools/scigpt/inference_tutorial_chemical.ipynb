{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/SFM_shufxi_7bv2_ft/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-12 06:11:11,641] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from sfm.data.sci_data.SFMDecTokenizer import SFMDecTokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'SFMDecTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'sfm.data.sci_data.SFMDecTokenizer.SFMDecTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[32m2024-04-12 06:11:15.502\u001b[0m][\u001b[36mINFO\u001b[0m]: Loading protein sentencepiece model from /home/yinxia/blob1.v2/shufxi/data/scigpt/ur50bpe/bpe.model and /home/yinxia/blob1.v2/shufxi/data/scigpt/ur50bpe/bpe.vocab\n",
      "[\u001b[32m2024-04-12 06:11:15.561\u001b[0m][\u001b[36mINFO\u001b[0m]: Loading DNA sentencepiece model from /home/yinxia/blob1.v2/shufxi/data/scigpt/dnabpe/bpe.model and /home/yinxia/blob1.v2/shufxi/data/scigpt/dnabpe/bpe.vocab\n",
      "[\u001b[32m2024-04-12 06:11:15.632\u001b[0m][\u001b[36mINFO\u001b[0m]: Loading RNA sentencepiece model from /home/yinxia/blob1.v2/shufxi/data/scigpt/rnabpe/bpe.model and /home/yinxia/blob1.v2/shufxi/data/scigpt/rnabpe/bpe.vocab\n",
      "[\u001b[32m2024-04-12 06:11:16.102\u001b[0m][\u001b[36mINFO\u001b[0m]: Tokenizer has 40014 tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer_home = '/home/yinxia/hai1/ds_dataset/llama2/llama-2-7b'\n",
    "tokenizer = SFMDecTokenizer.from_pretrained(\n",
    "    tokenizer_home,\n",
    "    prot_spm_path='/home/yinxia/blob1.v2/shufxi/data/scigpt/ur50bpe/bpe',\n",
    "    dna_spm_path='/home/yinxia/blob1.v2/shufxi/data/scigpt/dnabpe/bpe',\n",
    "    rna_spm_path='/home/yinxia/blob1.v2/shufxi/data/scigpt/rnabpe/bpe',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<mol>', '<m>c', '<m>1', '<m>c', '<m>c', '<m>c', '<m>c', '<m>c', '<m>1', '<m>N', '</mol>']\n",
      "['<protein>', '<a>E', '<a>M', '<a>M', '<a>FEQ', '<a>TFK', '<a>NID', '</protein>']\n",
      "['<dna>', '<d>A', '<d>U', '<d>C', '<d>GAAA', '</dna>']\n",
      "['▁The', '<protein>', '<a>E', '<a>M', '<a>M', '<a>FEQ', '<a>TFK', '<a>NID', '</protein>', '▁and', '▁the', '▁comp', 'ound', '<mol>', '<m>C', '<m>C', '<m>C', '</mol>', '▁can', '▁interact', '▁with', '▁each', '▁other', '.']\n"
     ]
    }
   ],
   "source": [
    "tok = tokenizer.tokenize('<mol>c1ccccc1N</mol>')\n",
    "print(tok)\n",
    "tok = tokenizer.tokenize('<protein>EMMFEQTFKNID</protein>')\n",
    "print(tok)\n",
    "tok = tokenizer.tokenize('<dna>AUCGAAA</dna>')\n",
    "print(tok)\n",
    "tok = tokenizer.tokenize('The <protein>EMMFEQTFKNID</protein> and the compound <mol>CCC</mol> can interact with each other.')\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently, we have three models\n",
    "\n",
    "# 1. general pre-trained model, no instruction tuning\n",
    "ckpt_home = r'/home/yinxia/hai1/shufxi/scigpt/7bv3/stageB/global_step32999'\n",
    "\n",
    "# 2. dedicate protein model, which is continued pre-trained from the `1. general model`\n",
    "# ckpt_home = r'/home/yinxia/blob1.v2/shufxi/scigpt/7bv3/prot/20240227113241/global_step3905'\n",
    "\n",
    "# 3. simply run instruction tuning on our collected dataset\n",
    "# ckpt_home = r'/home/yinxia/blob1.v2/shufxi/scigpt/7bv3/inst/20240301131100/global_step4753'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:28<00:00, 74.18s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 embed_tokens.weight torch.Size([40014, 4096])\n",
      "01 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "01 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "01 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "01 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "01 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "01 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "01 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "01 input_layernorm.weight torch.Size([4096])\n",
      "01 post_attention_layernorm.weight torch.Size([4096])\n",
      "02 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "02 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "02 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "02 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "02 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "02 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "02 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "02 input_layernorm.weight torch.Size([4096])\n",
      "02 post_attention_layernorm.weight torch.Size([4096])\n",
      "03 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "03 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "03 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "03 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "03 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "03 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "03 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "03 input_layernorm.weight torch.Size([4096])\n",
      "03 post_attention_layernorm.weight torch.Size([4096])\n",
      "04 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "04 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "04 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "04 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "04 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "04 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "04 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "04 input_layernorm.weight torch.Size([4096])\n",
      "04 post_attention_layernorm.weight torch.Size([4096])\n",
      "05 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "05 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "05 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "05 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "05 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "05 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "05 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "05 input_layernorm.weight torch.Size([4096])\n",
      "05 post_attention_layernorm.weight torch.Size([4096])\n",
      "06 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "06 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "06 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "06 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "06 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "06 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "06 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "06 input_layernorm.weight torch.Size([4096])\n",
      "06 post_attention_layernorm.weight torch.Size([4096])\n",
      "07 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "07 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "07 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "07 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "07 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "07 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "07 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "07 input_layernorm.weight torch.Size([4096])\n",
      "07 post_attention_layernorm.weight torch.Size([4096])\n",
      "08 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "08 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "08 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "08 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "08 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "08 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "08 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "08 input_layernorm.weight torch.Size([4096])\n",
      "08 post_attention_layernorm.weight torch.Size([4096])\n",
      "09 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "09 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "09 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "09 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "09 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "09 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "09 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "09 input_layernorm.weight torch.Size([4096])\n",
      "09 post_attention_layernorm.weight torch.Size([4096])\n",
      "10 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "10 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "10 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "10 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "10 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "10 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "10 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "10 input_layernorm.weight torch.Size([4096])\n",
      "10 post_attention_layernorm.weight torch.Size([4096])\n",
      "11 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "11 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "11 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "11 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "11 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "11 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "11 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "11 input_layernorm.weight torch.Size([4096])\n",
      "11 post_attention_layernorm.weight torch.Size([4096])\n",
      "12 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "12 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "12 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "12 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "12 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "12 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "12 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "12 input_layernorm.weight torch.Size([4096])\n",
      "12 post_attention_layernorm.weight torch.Size([4096])\n",
      "13 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "13 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "13 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "13 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "13 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "13 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "13 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "13 input_layernorm.weight torch.Size([4096])\n",
      "13 post_attention_layernorm.weight torch.Size([4096])\n",
      "14 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "14 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "14 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "14 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "14 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "14 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "14 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "14 input_layernorm.weight torch.Size([4096])\n",
      "14 post_attention_layernorm.weight torch.Size([4096])\n",
      "15 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "15 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "15 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "15 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "15 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "15 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "15 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "15 input_layernorm.weight torch.Size([4096])\n",
      "15 post_attention_layernorm.weight torch.Size([4096])\n",
      "16 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "16 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "16 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "16 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "16 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "16 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "16 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "16 input_layernorm.weight torch.Size([4096])\n",
      "16 post_attention_layernorm.weight torch.Size([4096])\n",
      "17 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "17 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "17 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "17 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "17 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "17 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "17 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "17 input_layernorm.weight torch.Size([4096])\n",
      "17 post_attention_layernorm.weight torch.Size([4096])\n",
      "18 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "18 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "18 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "18 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "18 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "18 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "18 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "18 input_layernorm.weight torch.Size([4096])\n",
      "18 post_attention_layernorm.weight torch.Size([4096])\n",
      "19 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "19 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "19 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "19 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "19 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "19 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "19 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "19 input_layernorm.weight torch.Size([4096])\n",
      "19 post_attention_layernorm.weight torch.Size([4096])\n",
      "20 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "20 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "20 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "20 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "20 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "20 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "20 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "20 input_layernorm.weight torch.Size([4096])\n",
      "20 post_attention_layernorm.weight torch.Size([4096])\n",
      "21 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "21 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "21 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "21 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "21 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "21 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "21 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "21 input_layernorm.weight torch.Size([4096])\n",
      "21 post_attention_layernorm.weight torch.Size([4096])\n",
      "22 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "22 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "22 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "22 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "22 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "22 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "22 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "22 input_layernorm.weight torch.Size([4096])\n",
      "22 post_attention_layernorm.weight torch.Size([4096])\n",
      "23 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "23 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "23 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "23 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "23 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "23 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "23 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "23 input_layernorm.weight torch.Size([4096])\n",
      "23 post_attention_layernorm.weight torch.Size([4096])\n",
      "24 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "24 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "24 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "24 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "24 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "24 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "24 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "24 input_layernorm.weight torch.Size([4096])\n",
      "24 post_attention_layernorm.weight torch.Size([4096])\n",
      "25 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "25 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "25 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "25 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "25 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "25 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "25 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "25 input_layernorm.weight torch.Size([4096])\n",
      "25 post_attention_layernorm.weight torch.Size([4096])\n",
      "26 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "26 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "26 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "26 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "26 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "26 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "26 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "26 input_layernorm.weight torch.Size([4096])\n",
      "26 post_attention_layernorm.weight torch.Size([4096])\n",
      "27 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "27 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "27 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "27 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "27 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "27 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "27 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "27 input_layernorm.weight torch.Size([4096])\n",
      "27 post_attention_layernorm.weight torch.Size([4096])\n",
      "28 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "28 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "28 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "28 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "28 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "28 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "28 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "28 input_layernorm.weight torch.Size([4096])\n",
      "28 post_attention_layernorm.weight torch.Size([4096])\n",
      "29 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "29 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "29 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "29 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "29 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "29 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "29 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "29 input_layernorm.weight torch.Size([4096])\n",
      "29 post_attention_layernorm.weight torch.Size([4096])\n",
      "30 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "30 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "30 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "30 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "30 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "30 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "30 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "30 input_layernorm.weight torch.Size([4096])\n",
      "30 post_attention_layernorm.weight torch.Size([4096])\n",
      "31 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "31 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "31 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "31 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "31 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "31 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "31 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "31 input_layernorm.weight torch.Size([4096])\n",
      "31 post_attention_layernorm.weight torch.Size([4096])\n",
      "32 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "32 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "32 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "32 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "32 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "32 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "32 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "32 input_layernorm.weight torch.Size([4096])\n",
      "32 post_attention_layernorm.weight torch.Size([4096])\n",
      "33 norm.weight torch.Size([4096])\n",
      "33 lm_head.weight torch.Size([40014, 4096])\n",
      "33 num_head.fc1.weight torch.Size([16384, 4096])\n",
      "33 num_head.fc1.bias torch.Size([16384])\n",
      "33 num_head.fc2.weight torch.Size([1, 16384])\n",
      "33 num_head.fc2.bias torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def show_ckpt(name, ckpt):\n",
    "    for k, v in ckpt.items():\n",
    "        if 'dummy' not in k:\n",
    "            print(name, k, v.shape)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(tokenizer_home)\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "ckpt_dict = {}\n",
    "layer0 = torch.load(os.path.join(ckpt_home, \"layer_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict['model.embed_tokens.weight'] = layer0['embed_tokens.weight']\n",
    "show_ckpt('layer0', layer0)\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 1).zfill(2)\n",
    "    layer = torch.load(os.path.join(ckpt_home, f\"layer_{l_index}-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    show_ckpt(l_index, layer)\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        ckpt_dict[f\"model.layers.{l}.{k}\"] = layer[k]\n",
    "layer = torch.load(os.path.join(ckpt_home, \"layer_33-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "show_ckpt(33, layer)\n",
    "ckpt_dict[\"model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(ckpt_home, \"layer_34-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "show_ckpt(33, layer)\n",
    "ckpt_dict[\"lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "model_dict.update(ckpt_dict)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the usage of fragment-based generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem, RDLogger\n",
    "RDLogger.DisableLog('rdApp.error')\n",
    "\n",
    "def reconstruct_molecule_from_fragments(fragments):  \n",
    "    # Initialize an empty molecule to store the reconstructed molecule  \n",
    "    reconstructed_mol = Chem.RWMol()  \n",
    "    # Combine the fragments into a single molecule  \n",
    "    combined_mol = fragments[0]  \n",
    "    for fragment in fragments[1:]:  \n",
    "        combined_mol = Chem.CombineMols(combined_mol, fragment)  \n",
    "    \n",
    "    # Convert the combined molecule to an RWMol  \n",
    "    reconstructed_mol = Chem.RWMol(combined_mol)  \n",
    "    # Find the dummy atoms and their corresponding neighbors  \n",
    "    \n",
    "    dummy_atoms = []  \n",
    "    for atom_idx, atom in enumerate(reconstructed_mol.GetAtoms()):  \n",
    "        if atom.GetAtomicNum() == 0:  \n",
    "            neighbors = [x.GetIdx() for x in atom.GetNeighbors()]  \n",
    "            dummy_atoms.append((atom_idx, atom.GetAtomMapNum(), neighbors[0] ))  \n",
    "  \n",
    "    # Sort dummy atoms by atom map number  \n",
    "    dummy_atoms.sort(key=lambda x: x[1])  \n",
    "  \n",
    "    # Connect the fragments using the dummy atoms and remove them  \n",
    "    for i in range(0, len(dummy_atoms), 2):  \n",
    "        atom_idx1, map_num1, neighbor1 = dummy_atoms[i]  \n",
    "        atom_idx2, map_num2, neighbor2 = dummy_atoms[i + 1]  \n",
    "  \n",
    "        if map_num1 == map_num2:  \n",
    "            # Add a bond between the atoms connected to the dummy atoms  \n",
    "            reconstructed_mol.AddBond(neighbor1, neighbor2, Chem.rdchem.BondType.SINGLE)  \n",
    "  \n",
    "    dummy_atoms = [e[0] for e in dummy_atoms]\n",
    "    dummy_atoms = sorted(dummy_atoms, reverse=True)\n",
    "    for atomidx in dummy_atoms: \n",
    "        reconstructed_mol.RemoveAtom(atomidx)  \n",
    "\n",
    "    for atom in reconstructed_mol.GetAtoms():  \n",
    "        atom.SetAtomMapNum(0)  \n",
    "  \n",
    "    return reconstructed_mol\n",
    "\n",
    "\n",
    "def extract_fragment_output(output):\n",
    "    try:\n",
    "        partA = re.search('<fragA>(.*?)</fragA>', output)\n",
    "        partB = re.search('<fragB>(.*?)</fragB>', output)\n",
    "        smi_A = partA.group(1).replace('<m>', '').replace(' ', '').strip()\n",
    "        smi_B = partB.group(1).replace('<m>', '').replace(' ', '').strip()\n",
    "\n",
    "        segs = []\n",
    "        for s in smi_A.split('.') + smi_B.split('.'):\n",
    "            s = s.strip()\n",
    "            if s == '':\n",
    "                continue\n",
    "            segs.append(s)\n",
    "\n",
    "        mol_frag = [Chem.MolFromSmiles(s) for s in segs]\n",
    "        m = reconstruct_molecule_from_fragments(mol_frag)\n",
    "        reconstruct_smi = Chem.MolToSmiles(m)\n",
    "        return reconstruct_smi\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O/N=C(\\Cc1ccc(-c2cccnc2)cc1)c1cc(Cl)sc1Cl\n",
      "O/N=C(\\Cc1nc(-c2cccnc2)cs1)c1cc(Cl)sc1Cl\n",
      "O/N=C(\\Cc1ccc(-c2cccnc2)cc1)c1cc(Cl)sc1Cl\n",
      "O/N=C(\\Cc1ccc(-c2cccnc2)cc1)c1cc(Cl)sc1Cl\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem, RDLogger\n",
    "import re \n",
    "\n",
    "input_ids = tokenizer('<fragA>O/N=C(\\C[*:100000])c1cc(Cl)sc1Cl.c1cncc([*:100001])c1</fragA>', return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "use_sample = True\n",
    "if use_sample:\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=300,\n",
    "        num_return_sequences=4,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "else:\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        num_beams=4,\n",
    "        max_new_tokens=300,\n",
    "        num_return_sequences=4,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "decoded_seq = set()\n",
    "for o in output:\n",
    "    s = tokenizer.decode(o)\n",
    "    r = extract_fragment_output(s)\n",
    "    if r is not None:\n",
    "        decoded_seq.add(r)\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <m>C <m>C <m>( <m>= <m>O <m>) <m>N <m>1 <m>C <m>C <m>2 <m>C <m>C <m>2 <m>( <m>c <m>2 <m>c <m>c <m>c <m>( <m>N <m>) <m>c <m>c <m>2 <m>) <m>C <m>1 \n",
      " <m>C <m>C <m>( <m>= <m>O <m>) <m>N <m>1 <m>C <m>[C@@H] <m>2 <m>C <m>[C@] <m>2 <m>( <m>c <m>2 <m>c <m>c <m>c <m>( <m>N <m>) <m>c <m>c <m>2 <m>) <m>C <m>1 \n",
      " <m>C <m>C <m>( <m>= <m>O <m>) <m>N <m>1 <m>C <m>[C@H] <m>2 <m>C <m>[C@] <m>2 <m>( <m>c <m>2 <m>c <m>c <m>c <m>( <m>N <m>) <m>c <m>c <m>2 <m>) <m>C <m>1 \n",
      " <m>C <m>C <m>( <m>= <m>O <m>) <m>N <m>1 <m>C <m>[C@H] <m>2 <m>C <m>[C@@] <m>2 <m>( <m>c <m>2 <m>c <m>c <m>c <m>( <m>N <m>) <m>c <m>c <m>2 <m>) <m>C <m>1 \n"
     ]
    }
   ],
   "source": [
    "reactants = r\"CC(=O)N1CC2CC2(c2ccc([N+]([O-])=O)cc2)C1\"\n",
    "\n",
    "input_str = f'<reactants>{reactants}</reactants><product>'\n",
    "input_ids = tokenizer(input_str, return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=300,\n",
    "    num_return_sequences=4,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "for o in output:\n",
    "    s = tokenizer.decode(o)\n",
    "    s = re.search('<product>(.*?)</product>', s).group(1)\n",
    "    s = s.replace('<m>', '').replace(' ', '').strip()\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
