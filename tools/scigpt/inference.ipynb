{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/sfm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-16 07:39:39,651] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'SFMDecTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'sfm.data.sci_data.SFMDecTokenizer.SFMDecTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from sfm.data.sci_data.SFMDecTokenizer import SFMDecTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "tokenizer_home = '/hai1/ds_dataset/llama2/llama-2-7b'\n",
    "\n",
    "tokenizer = SFMDecTokenizer.from_pretrained(tokenizer_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34177\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 32007, 32008]\n",
      "[1, 263, 15721, 363, 379, 5667, 338, 32001]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"<protein></protein>\").input_ids)\n",
    "print(tokenizer(\"a drug for HIV is <mol>\").input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32007, 32008, 32000, 32000, 32000, 32000, 32000],\n",
       "        [    1,   263, 15721,   363,   379,  5667,   338, 32001]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus([\"<protein></protein>\", \"a drug for HIV is <mol>\"], padding=True, return_tensors='pt').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dummy.weight', 'dummy.bias', 'norm.weight']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "ckpt_home = '/hai1/shufxi/scigpt/7b/stageB/global_step16599/'\n",
    "ckpt_path = os.path.join(ckpt_home, 'layer_33-model_states.pt')\n",
    "model_states = torch.load(ckpt_path, map_location='cpu')\n",
    "all_keys = list(model_states.keys())\n",
    "print(all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 embed_tokens.weight torch.Size([34177, 4096])\n",
      "01 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "01 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "01 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "01 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "01 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "01 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "01 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "01 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "01 input_layernorm.weight torch.Size([4096])\n",
      "01 post_attention_layernorm.weight torch.Size([4096])\n",
      "02 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "02 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "02 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "02 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "02 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "02 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "02 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "02 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "02 input_layernorm.weight torch.Size([4096])\n",
      "02 post_attention_layernorm.weight torch.Size([4096])\n",
      "03 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "03 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "03 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "03 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "03 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "03 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "03 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "03 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "03 input_layernorm.weight torch.Size([4096])\n",
      "03 post_attention_layernorm.weight torch.Size([4096])\n",
      "04 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "04 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "04 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "04 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "04 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "04 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "04 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "04 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "04 input_layernorm.weight torch.Size([4096])\n",
      "04 post_attention_layernorm.weight torch.Size([4096])\n",
      "05 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "05 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "05 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "05 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "05 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "05 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "05 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "05 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "05 input_layernorm.weight torch.Size([4096])\n",
      "05 post_attention_layernorm.weight torch.Size([4096])\n",
      "06 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "06 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "06 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "06 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "06 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "06 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "06 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "06 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "06 input_layernorm.weight torch.Size([4096])\n",
      "06 post_attention_layernorm.weight torch.Size([4096])\n",
      "07 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "07 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "07 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "07 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "07 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "07 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "07 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "07 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "07 input_layernorm.weight torch.Size([4096])\n",
      "07 post_attention_layernorm.weight torch.Size([4096])\n",
      "08 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "08 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "08 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "08 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "08 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "08 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "08 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "08 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "08 input_layernorm.weight torch.Size([4096])\n",
      "08 post_attention_layernorm.weight torch.Size([4096])\n",
      "09 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "09 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "09 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "09 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "09 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "09 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "09 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "09 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "09 input_layernorm.weight torch.Size([4096])\n",
      "09 post_attention_layernorm.weight torch.Size([4096])\n",
      "10 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "10 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "10 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "10 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "10 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "10 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "10 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "10 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "10 input_layernorm.weight torch.Size([4096])\n",
      "10 post_attention_layernorm.weight torch.Size([4096])\n",
      "11 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "11 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "11 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "11 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "11 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "11 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "11 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "11 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "11 input_layernorm.weight torch.Size([4096])\n",
      "11 post_attention_layernorm.weight torch.Size([4096])\n",
      "12 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "12 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "12 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "12 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "12 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "12 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "12 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "12 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "12 input_layernorm.weight torch.Size([4096])\n",
      "12 post_attention_layernorm.weight torch.Size([4096])\n",
      "13 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "13 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "13 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "13 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "13 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "13 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "13 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "13 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "13 input_layernorm.weight torch.Size([4096])\n",
      "13 post_attention_layernorm.weight torch.Size([4096])\n",
      "14 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "14 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "14 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "14 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "14 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "14 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "14 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "14 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "14 input_layernorm.weight torch.Size([4096])\n",
      "14 post_attention_layernorm.weight torch.Size([4096])\n",
      "15 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "15 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "15 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "15 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "15 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "15 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "15 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "15 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "15 input_layernorm.weight torch.Size([4096])\n",
      "15 post_attention_layernorm.weight torch.Size([4096])\n",
      "16 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "16 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "16 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "16 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "16 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "16 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "16 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "16 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "16 input_layernorm.weight torch.Size([4096])\n",
      "16 post_attention_layernorm.weight torch.Size([4096])\n",
      "17 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "17 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "17 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "17 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "17 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "17 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "17 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "17 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "17 input_layernorm.weight torch.Size([4096])\n",
      "17 post_attention_layernorm.weight torch.Size([4096])\n",
      "18 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "18 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "18 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "18 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "18 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "18 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "18 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "18 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "18 input_layernorm.weight torch.Size([4096])\n",
      "18 post_attention_layernorm.weight torch.Size([4096])\n",
      "19 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "19 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "19 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "19 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "19 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "19 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "19 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "19 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "19 input_layernorm.weight torch.Size([4096])\n",
      "19 post_attention_layernorm.weight torch.Size([4096])\n",
      "20 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "20 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "20 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "20 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "20 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "20 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "20 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "20 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "20 input_layernorm.weight torch.Size([4096])\n",
      "20 post_attention_layernorm.weight torch.Size([4096])\n",
      "21 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "21 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "21 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "21 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "21 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "21 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "21 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "21 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "21 input_layernorm.weight torch.Size([4096])\n",
      "21 post_attention_layernorm.weight torch.Size([4096])\n",
      "22 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "22 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "22 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "22 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "22 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "22 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "22 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "22 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "22 input_layernorm.weight torch.Size([4096])\n",
      "22 post_attention_layernorm.weight torch.Size([4096])\n",
      "23 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "23 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "23 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "23 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "23 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "23 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "23 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "23 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "23 input_layernorm.weight torch.Size([4096])\n",
      "23 post_attention_layernorm.weight torch.Size([4096])\n",
      "24 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "24 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "24 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "24 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "24 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "24 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "24 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "24 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "24 input_layernorm.weight torch.Size([4096])\n",
      "24 post_attention_layernorm.weight torch.Size([4096])\n",
      "25 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "25 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "25 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "25 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "25 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "25 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "25 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "25 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "25 input_layernorm.weight torch.Size([4096])\n",
      "25 post_attention_layernorm.weight torch.Size([4096])\n",
      "26 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "26 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "26 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "26 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "26 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "26 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "26 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "26 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "26 input_layernorm.weight torch.Size([4096])\n",
      "26 post_attention_layernorm.weight torch.Size([4096])\n",
      "27 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "27 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "27 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "27 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "27 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "27 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "27 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "27 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "27 input_layernorm.weight torch.Size([4096])\n",
      "27 post_attention_layernorm.weight torch.Size([4096])\n",
      "28 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "28 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "28 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "28 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "28 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "28 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "28 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "28 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "28 input_layernorm.weight torch.Size([4096])\n",
      "28 post_attention_layernorm.weight torch.Size([4096])\n",
      "29 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "29 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "29 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "29 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "29 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "29 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "29 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "29 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "29 input_layernorm.weight torch.Size([4096])\n",
      "29 post_attention_layernorm.weight torch.Size([4096])\n",
      "30 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "30 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "30 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "30 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "30 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "30 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "30 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "30 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "30 input_layernorm.weight torch.Size([4096])\n",
      "30 post_attention_layernorm.weight torch.Size([4096])\n",
      "31 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "31 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "31 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "31 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "31 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "31 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "31 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "31 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "31 input_layernorm.weight torch.Size([4096])\n",
      "31 post_attention_layernorm.weight torch.Size([4096])\n",
      "32 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "32 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "32 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "32 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "32 self_attn.rotary_emb.inv_freq torch.Size([64])\n",
      "32 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "32 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "32 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "32 input_layernorm.weight torch.Size([4096])\n",
      "32 post_attention_layernorm.weight torch.Size([4096])\n",
      "33 norm.weight torch.Size([4096])\n",
      "33 lm_head.weight torch.Size([34177, 4096])\n",
      "33 num_head.fc1.weight torch.Size([16384, 4096])\n",
      "33 num_head.fc1.bias torch.Size([16384])\n",
      "33 num_head.fc2.weight torch.Size([1, 16384])\n",
      "33 num_head.fc2.bias torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_ckpt(name, ckpt):\n",
    "    for k, v in ckpt.items():\n",
    "        if 'dummy' not in k:\n",
    "            print(name, k, v.shape)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(tokenizer_home)\n",
    "#model.resize_token_embeddings(len(tokenizer) + 1)\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "ckpt_dict = {}\n",
    "layer0 = torch.load(os.path.join(ckpt_home, \"layer_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict['model.embed_tokens.weight'] = layer0['embed_tokens.weight']\n",
    "show_ckpt('layer0', layer0)\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 1).zfill(2)\n",
    "    #print(f\"../blob2/checkpoints/MetaLLM-7B-D_NODE4-D_PROC16-instruction-mt/global_step97077/layer_{l_index}-model_states.pt\")\n",
    "    layer = torch.load(os.path.join(ckpt_home, f\"layer_{l_index}-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    show_ckpt(l_index, layer)\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        ckpt_dict[f\"model.layers.{l}.{k}\"] = layer[k]\n",
    "layer = torch.load(os.path.join(ckpt_home, \"layer_33-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "show_ckpt(33, layer)\n",
    "ckpt_dict[\"model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(ckpt_home, \"layer_34-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "show_ckpt(33, layer)\n",
    "ckpt_dict[\"lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "model_dict.update(ckpt_dict)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.load_state_dict(model_dict)\n",
    "\n",
    "# tokenizer.save_pretrained(\"/home/v-zequnliu/blob/v-zequnliu/llama/scigpt_7b_199\")\n",
    "# model.save_pretrained(\"/home/v-zequnliu/blob/v-zequnliu/llama/scigpt_7b_199\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(l_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁a', '▁drug', '▁for', '▁H', 'IV', '▁is', '<mol>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"a drug for HIV is <mol>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   263, 15721,   363,   379,  5667,   338, 32001]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   263, 15721,   363,   379,  5667,   338, 32001, 32124, 32124,\n",
      "         32125, 32124, 32126, 32125, 32124, 32126, 32131, 32124, 32125, 32129,\n",
      "         32128, 32126, 32140, 32127, 32124, 32131, 32125, 32124, 32123, 32130,\n",
      "         32123, 32123, 32123, 32132, 32123, 32130, 32126, 32124, 32124, 32131,\n",
      "         32127, 32124, 32140, 32125, 32128, 32126, 32124, 32140, 32125, 32124,\n",
      "         32123, 32127, 32123, 32123, 32123, 32123, 32123, 32127, 32126, 32124,\n",
      "         32125, 32129, 32128, 32126, 32131, 32139, 32127, 32123, 32130, 32123,\n",
      "         32123, 32123, 32123, 32123, 32130, 32124, 32139, 32127, 32128, 32002,\n",
      "             2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "text = \"a drug for HIV is <mol>\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n",
    "print(input_ids)\n",
    "#bad_words = [[i] for i in range(3, 32001)]\n",
    "#bad_words.append([0])\n",
    "output = model.generate(input_ids, do_sample=True, top_p=0.9, max_new_tokens=100, num_beams=5, num_return_sequences=1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a drug for HIV is <mol> <m>C <m>C <m>( <m>C <m>) <m>( <m>C <m>) <m>N <m>C <m>( <m>= <m>O <m>) <m>[C@@H] <m>1 <m>C <m>N <m>( <m>C <m>c <m>2 <m>c <m>c <m>c <m>n <m>c <m>2 <m>) <m>C <m>C <m>N <m>1 <m>C <m>[C@@H] <m>( <m>O <m>) <m>C <m>[C@@H] <m>( <m>C <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>) <m>C <m>( <m>= <m>O <m>) <m>N <m>[C@H] <m>1 <m>c <m>2 <m>c <m>c <m>c <m>c <m>c <m>2 <m>C <m>[C@H] <m>1 <m>O </mol>\n"
     ]
    }
   ],
   "source": [
    "for t in output:\n",
    "    print(tokenizer.decode(t, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>The aspirin is represented as <mol> <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>[Ca+2] </mol> in the SMILES sequence: <mol> <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>[Ca+2] </mol> . Acetylsalicylic acid is a monocarboxylic acid that is the acetyl derivative of salicylic acid. It has a role as a non-steroidal anti-inflammatory drug, a non-narcotic analgesic, an antipyretic, an EC 1.14.99.1 (prostaglandin-endoperoxide synthase) inhibitor, an EC 1\n",
      "<s>The aspirin is represented as <mol> <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>[Ca+2] </mol> in the SMILES sequence: <mol> <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>[Ca+2] </mol> . Acetylsalicylic acid is a monocarboxylic acid that is the acetyl derivative of salicylic acid. It has a role as a non-steroidal anti-inflammatory drug, a non-narcotic analgesic, an antipyretic, an EC 1.14.99.1 (prostaglandin-endoperoxide synthase) inhibitor, an EC 1\n",
      "<s>The aspirin is represented as <mol> <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>[Ca+2] </mol> in the SMILES sequence: <mol> <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>[Ca+2] </mol> . Acetylsalicylic acid is a monocarboxylic acid that is the acetyl derivative of salicylic acid. It has a role as a non-steroidal anti-inflammatory drug, a non-narcotic analgesic, an antipyretic, an EC 1.14.99.1 (prostaglandin-endoperoxide synthase) inhibitor, an EC 3\n",
      "<s>The aspirin is represented as <mol> <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>[Ca+2] </mol> in the SMILES sequence: <mol> <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>[O-] <m>. <m>[Ca+2] </mol> . Acetylsalicylic acid is a monocarboxylic acid that is the acetyl derivative of salicylic acid. It has a role as a non-steroidal anti-inflammatory drug, a non-narcotic analgesic, an antipyretic, an EC 1.14.99.1 (prostaglandin-endoperoxide synthase) inhibitor, an EC 3\n"
     ]
    }
   ],
   "source": [
    "text = \"The aspirin is represented as <mol>\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.generate(input_ids, do_sample=True, top_p=0.9, max_new_tokens=200, num_beams=4, num_return_sequences=4)\n",
    "\n",
    "for s in output:\n",
    "    print(tokenizer.decode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>The SMILES of aspirin is <mol> <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>O </mol>  .</s>\n",
      "<s>The SMILES of aspirin is <mol> <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>O </mol>  .</s>\n",
      "<s>The SMILES of aspirin is <mol> <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>O </mol>  .</s>\n",
      "<s>The SMILES of aspirin is <mol> <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>O </mol>  .</s>\n"
     ]
    }
   ],
   "source": [
    "text = \"The SMILES of aspirin is <mol>\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.generate(input_ids, do_sample=True, top_p=0.9, max_new_tokens=200, num_beams=4, num_return_sequences=4)\n",
    "\n",
    "for s in output:\n",
    "    print(tokenizer.decode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"a protein with 4 helixes is <protein>\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n",
    "#bad_words = [[i] for i in range(3, 32001)]\n",
    "#bad_words.append([0])\n",
    "output = model.generate(input_ids, do_sample=True, top_p=0.9, max_new_tokens=100, num_beams=5, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a protein with 4 helixes is <protein> <a>M <a>K <a>K <a>I <a>E <a>A <a>I <a>I <a>R <a>P <a>F <a>K <a>L <a>D <a>E <a>V <a>K <a>I <a>A <a>L <a>V <a>N <a>A <a>G <a>I <a>V <a>G <a>M <a>T <a>V <a>S <a>E <a>V <a>R <a>G <a>F <a>G <a>R <a>Q <a>K <a>G <a>Q <a>T <a>E <a>R <a>Y <a>R <a>G <a>S <a>E <a>Y <a>T <a>V <a>E <a>F <a>L <a>Q <a>K <a>L <a>K <a>L <a>E <a>I <a>V <a>V <a>E <a>D <a>A <a>Q <a>V <a>D <a>T <a>V <a>I <a>D <a>K <a>I <a>V <a>A <a>A <a>A <a>R <a>T <a>G <a>E <a>I <a>G <a>D <a>G <a>K <a>I <a>F <a>V <a>S <a>P <a>V <a>D <a>Q <a>T <a>I\n"
     ]
    }
   ],
   "source": [
    "for t in output:\n",
    "    print(tokenizer.decode(t, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/shufxi/repo/SFM_framework/tools/scigpt/inference.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/shufxi/repo/SFM_framework/tools/scigpt/inference.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/shufxi/repo/SFM_framework/tools/scigpt/inference.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#bad_words = [[i] for i in range(3, 32001)]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/shufxi/repo/SFM_framework/tools/scigpt/inference.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#bad_words.append([0])\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/shufxi/repo/SFM_framework/tools/scigpt/inference.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids, do_sample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, num_beams\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.8/site-packages/transformers/generation/utils.py:1685\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1678\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1679\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1680\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[1;32m   1681\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1682\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1683\u001b[0m     )\n\u001b[1;32m   1684\u001b[0m     \u001b[39m# 13. run beam search\u001b[39;00m\n\u001b[0;32m-> 1685\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_search(\n\u001b[1;32m   1686\u001b[0m         input_ids,\n\u001b[1;32m   1687\u001b[0m         beam_scorer,\n\u001b[1;32m   1688\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1689\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1690\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1691\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1692\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1693\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1694\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1695\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1696\u001b[0m     )\n\u001b[1;32m   1698\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SAMPLE:\n\u001b[1;32m   1699\u001b[0m     \u001b[39m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(generation_config)\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.8/site-packages/transformers/generation/utils.py:3024\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   3022\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3024\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   3025\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   3026\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3027\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   3028\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   3029\u001b[0m )\n\u001b[1;32m   3031\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3032\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:1038\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1035\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1037\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1038\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1039\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1040\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1041\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1042\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1043\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1044\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1045\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1046\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1047\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1048\u001b[0m )\n\u001b[1;32m   1050\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1051\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:925\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    921\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    922\u001b[0m         create_custom_forward(decoder_layer), hidden_states, attention_mask, position_ids\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    924\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    926\u001b[0m         hidden_states,\n\u001b[1;32m    927\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    928\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    929\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    930\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    931\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    932\u001b[0m         padding_mask\u001b[39m=\u001b[39;49mpadding_mask,\n\u001b[1;32m    933\u001b[0m     )\n\u001b[1;32m    935\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    937\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:632\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39m    past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    630\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m--> 632\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_layernorm(hidden_states)\n\u001b[1;32m    634\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[1;32m    635\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn(\n\u001b[1;32m    636\u001b[0m     hidden_states\u001b[39m=\u001b[39mhidden_states,\n\u001b[1;32m    637\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     padding_mask\u001b[39m=\u001b[39mpadding_mask,\n\u001b[1;32m    643\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:113\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    111\u001b[0m variance \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    112\u001b[0m hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrsqrt(variance \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariance_epsilon)\n\u001b[0;32m--> 113\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m*\u001b[39m hidden_states\u001b[39m.\u001b[39;49mto(input_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = \"The treatment of SARS-CoV-2\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n",
    "#bad_words = [[i] for i in range(3, 32001)]\n",
    "#bad_words.append([0])\n",
    "output = model.generate(input_ids, do_sample=False, max_new_tokens=200, num_beams=5, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The treatment of SARS-CoV-2 infection is mainly dependent on the use of antivirals that target the viral RNA-dependent RNA polymerase (RdRp) non-structural protein 13 (nsp13) and the main protease (M <sup>pro </sup>) to inhibit viral replication and transcription, respectively <sup>1,2 </sup>. However, viral variants that harbour mutations in these proteins can lead to drug resistance <sup>3-5 </sup>. An alternative viral target is the nsp16-nsp10 helicase-protease, which is essential for viral replication <sup>6,7 </sup>. Here we report the discovery and characterization of GC376, a potent inhibitor of SARS-CoV-2 nsp16-nsp10. The crystal structure of nsp16-\n"
     ]
    }
   ],
   "source": [
    "for t in output:\n",
    "    print(tokenizer.decode(t, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>The <mol> <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>O </mol> is a potent inhibitor of the human immunodeficiency virus type 1 reverse transcriptase. The <mol> <m>C <m>C <m>( <m>= <m>O <m>) <m>O <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>C <m>( <m>= <m>O <m>) <m>O </mol> (PA) is a potent inhibitor of the human immunodeficiency virus type 1 (HIV-1) reverse transcriptase (RT) in vitro. The inhibition is noncompetitive with respect to the template-primer and noncompetitive or uncompetitive with respect to the deoxynucleoside triphosphate. The inhibition is not reversed by dilution or by increasing the concentration of the template-primer. The inhibition is not reversed by increasing the concentration of the deoxynucleoside triphosphate. The inhibition is not reversed by increasing the concentration of the enzyme. The inhibition is\n"
     ]
    }
   ],
   "source": [
    "text = \"The <mol> CC(=O)Oc1ccccc1C(=O)O </mol> is a \"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.generate(input_ids, do_sample=False, max_new_tokens=200, num_beams=1, num_return_sequences=1)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>A material with two iron atoms and three oxygen atoms is <material> undrum. A material with three iron atoms and two oxygen atoms is ferric oxide. A material with four iron atoms and one oxygen atom is ferrous oxide. A material with four iron atoms and two oxygen atoms is ferrous oxide. A material with four iron atoms and three oxygen atoms is ferrous oxide. A material with four iron atoms and four oxygen atoms is ferrous oxide. A material with four iron atoms and five oxygen atoms is ferrous oxide. A material with four iron atoms and six oxygen atoms is ferrous oxide. A material with four iron atoms and seven oxygen atoms is ferrous oxide. A material with four iron atoms and eight oxygen atoms is ferrous oxide. A material with four iron atoms and nine oxygen atoms is ferrous oxide. A material with four iron atoms and ten oxygen atoms is ferrous oxide.\n",
      "<s>A material with two iron atoms and three oxygen atoms is <material> undrum. A material with three iron atoms and two oxygen atoms is ferric oxide. A material with four iron atoms and one oxygen atom is ferrous oxide. A material with four iron atoms and two oxygen atoms is ferric oxide. A material with four iron atoms and three oxygen atoms is ferric oxide. A material with four iron atoms and four oxygen atoms is ferric oxide. A material with four iron atoms and five oxygen atoms is ferric oxide. A material with four iron atoms and six oxygen atoms is ferric oxide. A material with four iron atoms and seven oxygen atoms is ferric oxide. A material with four iron atoms and eight oxygen atoms is ferric oxide. A material with four iron atoms and nine oxygen atoms is ferric oxide. A material with four iron atoms and ten oxygen atoms is ferric oxide. A material with four iron atoms and eleven o\n",
      "<s>A material with two iron atoms and three oxygen atoms is <material> undrum. A material with three iron atoms and two oxygen atoms is ferric oxide. A material with four iron atoms and one oxygen atom is ferrous oxide. A material with four iron atoms and two oxygen atoms is ferrous oxide. A material with four iron atoms and three oxygen atoms is ferrous oxide. A material with four iron atoms and four oxygen atoms is ferrous oxide. A material with four iron atoms and five oxygen atoms is ferrous oxide. A material with four iron atoms and six oxygen atoms is ferrous oxide. A material with five iron atoms and one oxygen atom is ferrous oxide. A material with five iron atoms and two oxygen atoms is ferrous oxide. A material with five iron atoms and three oxygen atoms is ferrous oxide. A material with five iron atoms and four oxygen atoms is ferrous oxide.\n",
      "<s>A material with two iron atoms and three oxygen atoms is <material> undrum. A material with three iron atoms and two oxygen atoms is ferric oxide. A material with four iron atoms and one oxygen atom is ferrous oxide. A material with four iron atoms and two oxygen atoms is ferric oxide. A material with four iron atoms and three oxygen atoms is ferric oxide. A material with four iron atoms and four oxygen atoms is ferric oxide. A material with four iron atoms and five oxygen atoms is ferric oxide. A material with four iron atoms and six oxygen atoms is ferric oxide. A material with four iron atoms and seven oxygen atoms is ferric oxide. A material with four iron atoms and eight oxygen atoms is ferric oxide. A material with four iron atoms and nine oxygen atoms is ferric oxide. A material with four iron atoms and ten oxygen atoms is ferric oxide. A material with four iron atoms and twelve o\n"
     ]
    }
   ],
   "source": [
    "text = \"A material with two iron atoms and three oxygen atoms is <material>\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.generate(input_ids, do_sample=True, max_new_tokens=200, num_beams=4, num_return_sequences=4)\n",
    "for s in output:\n",
    "    print(tokenizer.decode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>The molecule is a steroid ester that is methyl (17E)-pregna-4,17-dien-21-oate substituted by oxo groups at positions 3 and 11. It is a 3-oxo-Delta(4) steroid, an 11-oxo steroid, a steroid ester and a methyl ester. It derives from a hydride of a pregnane. The SMILES is <mol> <m>C <m>C <m>[C@] <m>1 <m>( <m>O <m>) <m>C <m>C <m>[C@H] <m>2 <m>[C@@H] <m>3 <m>C <m>C <m>C <m>4 <m>= <m>C <m>C <m>( <m>= <m>O <m>) <m>C <m>C <m>[C@@H] <m>4 <m>[C@H] <m>3 <m>C <m>C <m>[C@@] <m>2 <m>1 <m>C </mol>  .</s>\n"
     ]
    }
   ],
   "source": [
    "text = \"The molecule is a steroid ester that is methyl (17E)-pregna-4,17-dien-21-oate substituted by oxo groups at positions 3 and 11. It is a 3-oxo-Delta(4) steroid, an 11-oxo steroid, a steroid ester and a methyl ester. It derives from a hydride of a pregnane. The SMILES is <mol>\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.generate(input_ids, do_sample=True, top_p=0.9, max_new_tokens=200, num_beams=4, num_return_sequences=1)\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 112])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(torch.cat([input_ids, input_ids], dim=0), do_sample=True, top_p=0.9, max_new_tokens=200, num_beams=4, num_return_sequences=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>The molecule is a steroid ester that is methyl (17E)-pregna-4,17-dien-21-oate substituted by oxo groups at positions 3 and 11. It is a 3-oxo-Delta(4) steroid, an 11-oxo steroid, a steroid ester and a methyl ester. It derives from a hydride of a pregnane. The SMILES is <mol> <m>C <m>C <m>[C@] <m>1 <m>( <m>O <m>) <m>C <m>C <m>[C@H] <m>2 <m>[C@@H] <m>3 <m>C <m>C <m>C <m>4 <m>= <m>C <m>C <m>( <m>= <m>O <m>) <m>C <m>C <m>[C@@H] <m>4 <m>[C@H] <m>3 <m>C <m>C <m>[C@@] <m>2 <m>1 <m>C </mol>  .</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a protein that folds fast\n",
      "a transmembrane protein\n",
      "a protein that transport protons across membranes\n",
      "a protein that cut polypeptides\n",
      "a protein that binds to Zinc ions\n",
      "a protein that has 7 transmembrane helices\n",
      "a protein that aborb photon energies\n",
      "a protein emits fluorescence light\n",
      "a protein that is an enzyme involves drug metabolism\n",
      "a protein that transport protons across membranes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xfac\\x0fn\\xb9\\x8b[\\x80\\xbb1\\xfe\\xd7Y\\xc0E\\xe3\\xd1\\x98 \\x017T\\x9bw\\xf3\\xbb\\xe7\\xe3\\x1b\\xb3\\xcb\\x86\\x9dJ\\xc4Y\\x9c\\xbdX5]C\\xb6I\\xa7\\x87\\xd1\\x0c\\xb9\\xd8\\xe2\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03', b'\\x08\\x07\\x08\\x08\\x08', b'\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08']\n",
      "Bad pipe message: %s [b'\\x01\\x05\\x01\\x06\\x01']\n",
      "Bad pipe message: %s [b'4\\xde\\xd0w\\xc7\\x04\\x9dy']\n",
      "Bad pipe message: %s [b\"4\\xf3\\x96\\xa0h\\xb6L\\xfd\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\"]\n",
      "Bad pipe message: %s [b'\\x15:\\xe6\\xb7#\\xeb)\\x81\\xb7%\\xeab\\x8f\\xa9#\\xf4\\x05\\x8c\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0', b\"a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\"]\n",
      "Bad pipe message: %s [b'\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00']\n",
      "Bad pipe message: %s [b'\\x00\\x11\\xe6\\xae\\xbb\\xf7LO\\x02\\x12\\x1bSJ\\xa3\\xb7q\\xb4\\xba\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01']\n",
      "Bad pipe message: %s [b'\\n']\n",
      "Bad pipe message: %s [b'\\xf8\\x85\\x1d\\x97\\xd3 $?\\x9b\\xf7S\\x7f\\x85<\\x1b\\x8a\\xe0\\xbb\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00', b'\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00']\n",
      "Bad pipe message: %s [b'\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x03\\xc0\\x10']\n",
      "Bad pipe message: %s [b'\\x7f}\\xb5\\x9f\\xe8a\\x163\\x9d0\\xfaT\\x8bh\\xf5\\xd1Q%\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002']\n",
      "Bad pipe message: %s [b'DV\\xc4Ks\\x19#L\\xcf<\\xb7\\x82\\xa6\\xf7\\xf4\\xf0\\x0cF\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x00', b'\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004']\n",
      "Bad pipe message: %s [b\"\\x9c\\x0f\\xc3\\x1a\\x97/S\\xc9\\xbd\\xdb\\x061\\xde\\t\\x87\\xaf\\x9e0\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\"]\n",
      "Bad pipe message: %s [b\"wm\\x1b\\xe8\\x80\\xceK\\xb4\\xe4\\x92j\\xd7\\x18\\xf1\\x97\\xe8\\xab\\xaf\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\"]\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "\"a protein that folds fast\",\n",
    "\"a transmembrane protein\",\n",
    "\"a protein that transport protons across membranes\",\n",
    "\"a protein that cut polypeptides\",\n",
    "\"a protein that binds to Zinc ions\",\n",
    "\"a protein that has 7 transmembrane helices\",\n",
    "\"a protein that aborb photon energies\",\n",
    "\"a protein emits fluorescence light\",\n",
    "\"a protein that is an enzyme involves drug metabolism\",\n",
    "\"a protein that transport protons across membranes\",\n",
    "]\n",
    "\n",
    "with open('/home/shufxi/protein.txt', 'w') as f:\n",
    "    for text in text_list:\n",
    "        print(text)\n",
    "        input_ids = tokenizer(text + ' <protein>', return_tensors=\"pt\").input_ids.cuda()\n",
    "        output = model.generate(input_ids, do_sample=True, top_p=0.95, max_new_tokens=1000, num_return_sequences=20)\n",
    "        f.write(text + '\\n')\n",
    "        for s in output:\n",
    "            pred = tokenizer.decode(s)\n",
    "            if '</protein>' not in pred:\n",
    "                continue\n",
    "            pred_protein = pred[pred.find('<protein>') + len('<protein>'):pred.find('</protein>')]\n",
    "            pred_protein = pred_protein.replace('<a>', '').replace(' ', '')\n",
    "            f.write(pred_protein + '\\n')\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>A short protein with only beta helix is <protein> <a>M <a>S <a>L <a>Q <a>E <a>M <a>F <a>R <a>F <a>P <a>M <a>G <a>L <a>L <a>L <a>G <a>S <a>V <a>L <a>L <a>V <a>A <a>S <a>A <a>P <a>A <a>T <a>L <a>E <a>P <a>P <a>G <a>C <a>S <a>N <a>K <a>E <a>Q <a>Q <a>V <a>T <a>V <a>S <a>H <a>N <a>L <a>S <a>N <a>S <a>P <a>A <a>S <a>N <a>L <a>S <a>L <a>G <a>H <a>L <a>E <a>S <a>L <a>K <a>T <a>G <a>Q <a>D <a>R <a>M <a>L <a>V <a>S <a>S <a>D <a>G <a>K <a>S <a>V <a>Q <a>V <a>T <a>E <a>G <a>G <a>W <a>S <a>G <a>L <a>G <a>C <a>P </protein> inhibitory to HIV-1 integrase. The HIV-1 virus is the causative agent of the acquired immunodeficiency syndrome (AIDS). Integration of the viral DNA into the host chromosome is mediated by the viral proteins, the viral enzyme integrase and the viral DNA-terminal protein, termed 3' processing endonuclease. We studied the effect of a 16-mer peptide, a beta helix of the HIV-1 capsid protein p24, on both enzymes. In this peptide (p24(36)(25-40)), Ser42 was modified with a <mol> <m>O <m>= <m>C <m>( <m>[O-] <m>) <m>C <m>C <m>C <m>C <m>[C@@H] <m>1 <m>S <m>C <m>[C@@H] <m>2 <m>N <m>C <m>( <m>= <m>O <m>) <m>N <m>[C@@H] <m>2 <m>1 </mol> moiety. As shown in an in vitro assay, the p24(36)(25-40)\n",
      "MSLQEMFRFPMGLLLGSVLLVASAPATLEPPGCSNKEQQVTVSHNLSNSPASNLSLGHLESLKTGQDRMLVSSDGKSVQVTEGGWSGLGCP\n",
      "\n",
      "<s>A short protein with only beta helix is <protein> <a>M <a>P <a>Y <a>V <a>T <a>P <a>T <a>D <a>Q <a>L <a>L <a>Y <a>L <a>V <a>D <a>N <a>K <a>M <a>H <a>A <a>I <a>I <a>V <a>E <a>T <a>E <a>F <a>G <a>P <a>L <a>K <a>P <a>G <a>Y <a>G <a>Y <a>V <a>W <a>W <a>C <a>G <a>N <a>G <a>P <a>G <a>G <a>S <a>L <a>E <a>F <a>G <a>N <a>F <a>K <a>S <a>R <a>I <a>I <a>P <a>G <a>S <a>P <a>F <a>T <a>T <a>I <a>Y <a>P <a>N <a>H <a>Y <a>V <a>E <a>H <a>I <a>T <a>T <a>P <a>D <a>Y <a>P <a>S <a>K <a>P <a>G <a>K <a>H <a>R <a>N <a>Q <a>G <a>H <a>I <a>E <a>I <a>V <a>L <a>V <a>Q <a>T <a>D <a>A <a>S <a>T <a>N <a>W <a>K <a>D <a>V <a>P <a>A <a>V <a>T <a>Q <a>A <a>M <a>L <a>K <a>H <a>P <a>A <a>S <a>N <a>H <a>L <a>R <a>I <a>G <a>S <a>F <a>L <a>T <a>T <a>P <a>D <a>G <a>K <a>P <a>V <a>P <a>N <a>D <a>T <a>G <a>S <a>E <a>V <a>E <a>V <a>F <a>Q <a>E <a>G <a>T <a>D <a>L <a>N <a>S <a>A <a>I <a>P <a>G <a>T <a>W <a>E <a>E <a>H <a>L <a>E <a>V <a>L <a>R <a>E <a>T <a>V <a>I <a>K <a>E <a>L <a>D <a>R </protein> in the E. coli periplasm. The folding of a protein can require more than half of its sequence to occur. It has been shown that a small domain, derived from an N-terminal fragment of human serum albumin, can functionally fold to a beta helix upon disulfide-linking to form a covalently-linked dimer. To investigate this, we have expressed as a C-terminal fusion of maltose-binding protein a 27-residue polypeptide derived from the sequence of\n",
      "MPYVTPTDQLLYLVDNKMHAIIVETEFGPLKPGYGYVWWCGNGPGGSLEFGNFKSRIIPGSPFTTIYPNHYVEHITTPDYPSKPGKHRNQGHIEIVLVQTDASTNWKDVPAVTQAMLKHPASNHLRIGSFLTTPDGKPVPNDTGSEVEVFQEGTDLNSAIPGTWEEHLEVLRETVIKELDR\n",
      "\n",
      "<s>A short protein with only beta helix is <protein> <a>M <a>G <a>L <a>S <a>D <a>G <a>E <a>W <a>Q <a>L <a>V <a>L <a>N <a>V <a>W <a>G <a>K <a>V <a>E <a>A <a>D <a>I <a>P <a>S <a>H <a>G <a>Q <a>E <a>V <a>L <a>I <a>R <a>L <a>F <a>K <a>G <a>H <a>P <a>E <a>T <a>L <a>E <a>K <a>F <a>D <a>K <a>F <a>K <a>H <a>L <a>K <a>S <a>E <a>D <a>E <a>M <a>K <a>A <a>S <a>E <a>D <a>L <a>K <a>K <a>H <a>G <a>A <a>T <a>V <a>L <a>T <a>A <a>L <a>G <a>G <a>I <a>L <a>K <a>K <a>K <a>G <a>H <a>H <a>E <a>A <a>E <a>I <a>K <a>P <a>L <a>A <a>Q <a>S <a>H <a>A <a>T <a>K <a>H <a>K <a>I <a>P <a>V <a>K <a>Y <a>L <a>E <a>F <a>I <a>S <a>E <a>C <a>I <a>I <a>Q <a>V <a>L <a>Q <a>S <a>K <a>H <a>P <a>G <a>D <a>F <a>G <a>A <a>D <a>A <a>Q <a>G <a>A <a>M <a>N <a>K <a>A <a>L <a>E <a>L <a>F <a>R <a>K <a>D <a>M <a>A <a>S <a>N <a>Y <a>K <a>E <a>L <a>G <a>F <a>Q <a>G </protein> (Mb). Here we describe its design. We start with an analysis of the sequence of the protein, and the sequence requirements for formation of the alpha helix and the beta turn. We find that in the protein Mb the hydrophobic residues participating in the formation of the secondary structure are conserved. We suggest that the sequence requirements for the formation of the alpha helix and the beta turn in the protein Mb can be explained by the hydrophobic effect, and that these secondary structures could be stabilized by the hydrophobic effect in the absence of additional interactions. The hydrophobic interactions can then also provide a molecular explanation for the correlation between the length of the\n",
      "MGLSDGEWQLVLNVWGKVEADIPSHGQEVLIRLFKGHPETLEKFDKFKHLKSEDEMKASEDLKKHGATVLTALGGILKKKGHHEAEIKPLAQSHATKHKIPVKYLEFISECIIQVLQSKHPGDFGADAQGAMNKALELFRKDMASNYKELGFQG\n",
      "\n",
      "<s>A short protein with only beta helix is <protein> <a>M <a>V <a>L <a>G <a>G <a>C <a>P <a>V <a>S <a>Y <a>L <a>L <a>L <a>C <a>G <a>Q <a>A <a>A <a>L <a>L <a>L <a>G <a>N <a>L <a>L <a>L <a>L <a>H <a>C <a>V <a>S <a>R <a>S <a>H <a>S <a>Q <a>N <a>A <a>T <a>A <a>E <a>P <a>E <a>L <a>T <a>S <a>A <a>G <a>A <a>A <a>Q <a>P <a>E <a>G <a>P <a>G <a>G <a>A <a>A <a>S <a>W <a>E <a>Y <a>G <a>D <a>P <a>H <a>S <a>P <a>V <a>I <a>L <a>C <a>S <a>Y <a>L <a>P <a>D <a>E <a>F <a>I <a>E <a>C <a>E <a>D <a>P <a>V <a>D <a>H <a>V <a>G <a>N <a>A <a>T <a>A <a>S <a>Q <a>E <a>L <a>G <a>Y <a>G <a>C <a>L <a>K <a>F <a>G <a>G <a>Q <a>A <a>Y <a>S <a>D <a>V <a>E <a>H <a>T <a>S <a>V <a>Q <a>C <a>H <a>A <a>L <a>D <a>G <a>I <a>E <a>C <a>A <a>S <a>P <a>R <a>T <a>F <a>L <a>R <a>E <a>N <a>K <a>P <a>C <a>I <a>K <a>Y <a>T <a>G <a>H <a>Y <a>F <a>I <a>T <a>T <a>L <a>L <a>Y <a>S <a>F <a>F <a>L <a>G <a>C <a>F <a>G <a>V <a>D <a>R <a>F <a>C <a>L <a>G <a>H <a>T <a>G <a>T <a>A <a>V <a>G <a>K <a>L <a>L <a>T <a>L <a>G <a>G <a>L <a>G <a>I <a>W <a>W <a>F <a>V <a>D <a>L <a>I <a>L <a>L <a>I <a>T <a>G <a>G <a>L <a>M <a>P <a>S <a>D <a>G <a>S <a>N <a>W <a>C <a>T <a>V <a>Y </protein> (beta) nerve growth factor-1. Neutralizing antibodies against nerve growth factor (NGF) have been shown to attenuate the progression of neurodegenerative diseases and pain, making NGF a potentially promising therapeutic. NGF is comprised of a short N-terminal beta helix and an extended epidermal\n",
      "MVLGGCPVSYLLLCGQAALLLGNLLLLHCVSRSHSQNATAEPELTSAGAAQPEGPGGAASWEYGDPHSPVILCSYLPDEFIECEDPVDHVGNATASQELGYGCLKFGGQAYSDVEHTSVQCHALDGIECASPRTFLRENKPCIKYTGHYFITTLLYSFFLGCFGVDRFCLGHTGTAVGKLLTLGGLGIWWFVDLILLITGGLMPSDGSNWCTVY\n",
      "\n",
      "<s>A short protein with only beta helix is <protein> <a>M <a>V <a>L <a>A <a>G <a>L <a>I <a>W <a>V <a>G <a>S <a>A <a>Y <a>A <a>A <a>D <a>I <a>S <a>L <a>L <a>N <a>V <a>S <a>Y <a>D <a>P <a>T <a>R <a>E <a>L <a>Y <a>V <a>D <a>F <a>N <a>K <a>A <a>F <a>A <a>A <a>H <a>Y <a>Q <a>K <a>E <a>T <a>G <a>K <a>S <a>V <a>E <a>I <a>K <a>Q <a>S <a>H <a>G <a>G <a>S <a>G <a>K <a>Q <a>A <a>R <a>S <a>V <a>I <a>D <a>G <a>L <a>R <a>A <a>D <a>V <a>V <a>T <a>L <a>A <a>L <a>A <a>Y <a>D <a>I <a>D <a>E <a>V <a>A <a>E <a>R <a>G <a>L <a>L <a>A <a>K <a>D <a>W <a>Q <a>K <a>R <a>L <a>P <a>E <a>N <a>S <a>S <a>P <a>Y <a>T <a>S <a>T <a>I <a>V <a>F <a>L <a>V <a>R <a>T <a>G <a>N <a>P <a>K <a>G <a>I <a>K <a>D <a>W <a>D <a>D <a>L <a>I <a>K <a>P <a>G <a>V <a>N <a>V <a>I <a>T <a>P <a>N <a>P <a>K <a>T <a>S <a>G <a>G <a>A <a>R <a>W <a>N <a>Y <a>L <a>A <a>A <a>W <a>G <a>F <a>A <a>L <a>K <a>K <a>L <a>G <a>G <a>E <a>D <a>K <a>A <a>R <a>Q <a>F <a>V <a>S <a>D <a>I <a>Y <a>K <a>H <a>V <a>P <a>V <a>L <a>D <a>T <a>G <a>A <a>R <a>G <a>S <a>T <a>V <a>T <a>F <a>V <a>E <a>R <a>G <a>E <a>G <a>D <a>V <a>L <a>L <a>A <a>W <a>E <a>N <a>E <a>A <a>F <a>L <a>A <a>L <a>K <a>Q <a>F <a>G <a>K <a>D <a>K <a>M <a>E <a>I <a>N <a>Y <a>N <a>E <a>D <a>S <a>Q <a>S <a>A <a>D <a>L <a>G <a>D <a>K <a>G <a>I <a>T <a>V <a>F <a>A <a>P <a>T <a>D <a>K <a>V <a>P <a>V <a>P <a>L <a>D <a>A <a>G <a>L <a>Q <a>R <a>S <a>V <a>V <a>F <a>V <a>Q <a>L <a>H <a>V <a>K <a>G <a>G </protein> (RNase H1) from the model organism Bacillus subtilis has been developed for the first time. The secondary structure\n",
      "MVLAGLIWVGSAYAADISLLNVSYDPTRELYVDFNKAFAAHYQKETGKSVEIKQSHGGSGKQARSVIDGLRADVVTLALAYDIDEVAERGLLAKDWQKRLPENSSPYTSTIVFLVRTGNPKGIKDWDDLIKPGVNVITPNPKTSGGARWNYLAAWGFALKKLGGEDKARQFVSDIYKHVPVLDTGARGSTVTFVERGEGDVLLAWENEAFLALKQFGKDKMEINYNEDSQSADLGDKGITVFAPTDKVPVPLDAGLQRSVVFVQLHVKGG\n",
      "\n",
      "<s>A short protein with only beta helix is <protein> <a>M <a>K <a>V <a>T <a>G <a>I <a>F <a>L <a>L <a>S <a>A <a>L <a>A <a>L <a>L <a>S <a>L <a>S <a>G <a>N <a>T <a>G <a>A <a>D <a>S <a>L <a>G <a>R <a>E <a>A <a>K <a>C <a>Y <a>N <a>E <a>L <a>N <a>G <a>C <a>T <a>K <a>I <a>Y <a>D <a>P <a>V <a>C <a>G <a>T <a>D <a>G <a>N <a>T <a>Y <a>P <a>N <a>E <a>C <a>V <a>L <a>C <a>F <a>E <a>N <a>R <a>K <a>R <a>Q <a>T <a>S <a>I <a>L <a>I <a>Q <a>K <a>S <a>G <a>P <a>C </protein>  inhibitor. Charybdotoxin, a scorpion venom, blocks the large conductance, Ca2+ activated potassium channel (KCa1.1) that is present in smooth muscle and other tissues. In common with other short peptides from scorpions and spiders, charybdotoxin contains only beta helix structure, rather than the three disulfide bridges, and the single alpha helix of most other proteins. We report that charybdotoxin can be expressed as a soluble protein with no additional amino acid residues, and that its biological activity is not significantly altered. Beta helix appears to be a robust folding motif, in common with the parallel beta helix of collagen-like proteins.</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "MKVTGIFLLSALALLSLSGNTGADSLGREAKCYNELNGCTKIYDPVCGTDGNTYPNECVLCFENRKRQTSILIQKSGPC\n",
      "\n",
      "<s>A short protein with only beta helix is <protein> <a>M <a>E <a>T <a>W <a>S <a>T <a>Y <a>H <a>H <a>Q <a>K <a>D <a>L <a>A <a>A <a>R <a>L <a>L <a>V <a>K <a>V <a>K <a>N <a>L <a>A <a>A <a>A <a>K <a>F <a>I <a>E <a>Q <a>K <a>I <a>Y <a>P <a>H <a>L <a>R <a>K <a>G <a>L <a>V <a>C <a>A <a>K <a>E <a>T <a>S <a>H <a>P <a>I <a>I <a>P <a>H <a>D <a>R <a>N <a>V <a>D <a>W <a>W <a>Y <a>F <a>C <a>V <a>R <a>D <a>H <a>L <a>P <a>H <a>S <a>T <a>L <a>K <a>E <a>T <a>Q <a>K <a>A <a>A <a>R <a>P <a>W <a>Q <a>E <a>A <a>S <a>I <a>F <a>N <a>N <a>E <a>H <a>F <a>T <a>A <a>F <a>S <a>H <a>H <a>V <a>D <a>S <a>L <a>C <a>S <a>L <a>K <a>N <a>L <a>T <a>L <a>S <a>H <a>N <a>C <a>L <a>Q <a>D <a>R <a>H <a>L <a>Y <a>H <a>C <a>E <a>N <a>F <a>R <a>S <a>A <a>V <a>S <a>W <a>M <a>V <a>V <a>R <a>T <a>D <a>F <a>Q <a>L <a>L <a>E <a>G <a>V <a>R <a>T <a>Q <a>D <a>S <a>E <a>E </protein> based on secondary structures. Our previous study on the primary structures of the M-superfamily of conopeptides revealed a possible correlation between its structure and function. However, no M-conopeptide was experimentally proved to have only a beta-helix as a structural backbone in living organisms. In this report, using circular dichroism (CD), we found an M-superfamily of conopeptide, Aam015, only with a single-beta-helix structure. CD analysis also showed that both the N-terminal and C-terminal fragments of Aam015 still have beta-helix structures. This is the first example\n",
      "METWSTYHHQKDLAARLLVKVKNLAAAKFIEQKIYPHLRKGLVCAKETSHPIIPHDRNVDWWYFCVRDHLPHSTLKETQKAARPWQEASIFNNEHFTAFSHHVDSLCSLKNLTLSHNCLQDRHLYHCENFRSAVSWMVVRTDFQLLEGVRTQDSEE\n",
      "\n",
      "<s>A short protein with only beta helix is <protein> <a>M <a>A <a>A <a>A <a>A <a>V <a>A <a>A <a>A <a>A <a>A <a>A <a>A <a>A <a>A <a>A <a>S <a>L <a>Q <a>V <a>L <a>E <a>M <a>E <a>S <a>M <a>E <a>T <a>A <a>A <a>A <a>G <a>S <a>A <a>G <a>L <a>A <a>A <a>E <a>V <a>R <a>G <a>S <a>G <a>T <a>V <a>D <a>F <a>G <a>P <a>G <a>P <a>G <a>I <a>S <a>A <a>M <a>E <a>A <a>S <a>G <a>G <a>D <a>P <a>G <a>P <a>E <a>A <a>E <a>D <a>F <a>E <a>C <a>S <a>S <a>H <a>C <a>S <a>E <a>L <a>S <a>W <a>R <a>Q <a>N <a>E <a>Q <a>R <a>R <a>Q <a>G <a>L <a>F <a>C <a>D <a>I <a>T <a>L <a>C <a>F <a>G <a>G <a>A <a>G <a>G <a>R <a>E <a>F <a>R <a>A <a>H <a>R <a>S <a>V <a>L <a>A <a>A <a>A <a>T <a>E <a>Y <a>F <a>T <a>P <a>L <a>L <a>S <a>G <a>Q <a>F <a>S <a>E <a>S <a>R <a>S <a>G <a>R <a>V <a>E <a>M <a>R <a>K <a>W <a>S <a>S <a>E <a>P <a>G <a>P <a>E <a>P <a>D <a>T <a>V <a>E <a>A <a>V <a>I <a>E <a>Y <a>M <a>Y <a>T <a>G <a>R <a>I <a>R <a>V <a>S <a>T <a>G <a>S <a>V <a>H <a>E <a>V <a>L <a>E <a>L <a>A <a>D <a>R <a>F <a>L <a>L <a>I <a>R <a>L <a>K <a>E <a>F <a>C <a>G <a>E <a>F <a>L <a>K <a>K <a>K <a>L <a>H <a>L <a>S <a>N <a>C <a>V <a>A <a>I <a>H <a>S <a>L <a>A <a>H <a>M <a>Y <a>T <a>L <a>S <a>Q <a>L <a>A <a>L <a>K <a>A <a>A <a>D <a>M <a>I <a>R <a>R <a>N <a>A <a>L <a>R <a>T <a>Q <a>L <a>Y <a>L <a>E <a>M <a>L <a>M <a>H <a>L <a>K <a>T <a>N <a>H <a>I <a>E <a>E <a>A <a>L <a>F <a>Q <a>L <a>G <a>L <a>A </protein> The four-helix bundle is the fundamental protein fold, the beta-sandwich conformation of which is the only three-dimensional motif that has been found to possess all the\n",
      "MAAAAVAAAAAAAAAASLQVLEMESMETAAAGSAGLAAEVRGSGTVDFGPGPGISAMEASGGDPGPEAEDFECSSHCSELSWRQNEQRRQGLFCDITLCFGGAGGREFRAHRSVLAAATEYFTPLLSGQFSESRSGRVEMRKWSSEPGPEPDTVEAVIEYMYTGRIRVSTGSVHEVLELADRFLLIRLKEFCGEFLKKKLHLSNCVAIHSLAHMYTLSQLALKAADMIRRNALRTQLYLEMLMHLKTNHIEEALFQLGLA\n",
      "\n",
      "<s>A short protein with only beta helix is <protein> <a>M <a>V <a>K <a>Q <a>I <a>E <a>S <a>K <a>T <a>A <a>F <a>Q <a>E <a>A <a>L <a>D <a>A <a>A <a>G <a>D <a>K <a>L <a>V <a>V <a>V <a>D <a>F <a>S <a>A <a>T <a>W <a>C <a>G <a>P <a>C <a>K <a>M <a>I <a>K <a>P <a>F <a>F <a>H <a>S <a>L <a>S <a>E <a>K <a>Y <a>S <a>N <a>V <a>I <a>F <a>L <a>E <a>V <a>D <a>V <a>D <a>D <a>C <a>Q <a>D <a>V <a>A <a>S <a>E <a>C <a>E <a>V <a>K <a>C <a>M <a>P <a>T <a>F <a>Q <a>F <a>F <a>K <a>K <a>G <a>Q <a>K <a>V <a>G <a>E <a>F <a>S <a>G <a>A <a>N <a>K <a>E <a>K <a>L <a>E <a>A <a>T <a>I <a>N <a>E <a>L <a>V </protein> and has been found only in extremophiles. Heterooligomerization of TtRos with another homologous protein, Tt-ROC1, enhances the thermal stability of TtRos. This study found that the heterooligomers inhibited the fibrillation of A(1-42) and the fibrillar species were eliminated by the heterooligomers. In addition, the heterooligomers improved the membrane protection and reduced the cytotoxicity caused by the oligomers. The enhancing mechanism has been explored in detail. The results showed that TtRos monomer had a tendency to change its structure to a beta-rich state in aqueous solution, which was enhanced by Tt-ROC1 to generate a heterooligomer. The heterooligomers formed the beta-rich core and\n",
      "MVKQIESKTAFQEALDAAGDKLVVVDFSATWCGPCKMIKPFFHSLSEKYSNVIFLEVDVDDCQDVASECEVKCMPTFQFFKKGQKVGEFSGANKEKLEATINELV\n",
      "\n",
      "<s>A short protein with only beta helix is <protein> <a>M <a>D <a>L <a>D <a>V <a>V <a>N <a>M <a>F <a>V <a>I <a>A <a>G <a>G <a>T <a>L <a>A <a>I <a>P <a>I <a>L <a>A <a>F <a>V <a>A <a>S <a>F <a>L <a>L <a>W <a>P <a>S <a>A <a>L <a>I <a>R <a>I <a>Y <a>Y <a>W <a>Y <a>W <a>R <a>R <a>T <a>L <a>G <a>M <a>Q <a>V <a>R <a>Y <a>V <a>H <a>H <a>E <a>D <a>Y <a>Q <a>F <a>C <a>Y <a>S <a>F <a>R <a>G <a>R <a>P <a>G <a>H <a>K <a>P <a>S <a>I <a>L <a>M <a>L <a>H <a>G <a>F <a>S <a>A <a>H <a>K <a>D <a>M <a>W <a>L <a>S <a>V <a>V <a>K <a>F <a>L <a>P <a>K <a>N <a>L <a>H <a>L <a>V <a>C <a>V <a>D <a>M <a>P <a>G <a>H <a>E <a>G <a>T <a>T <a>R <a>S <a>S <a>L <a>D <a>D <a>L <a>S <a>I <a>D <a>G <a>Q <a>V <a>K <a>R <a>I <a>H <a>Q <a>F <a>V <a>E <a>C <a>L <a>K <a>L <a>N <a>K <a>K <a>P <a>F <a>H <a>L <a>V <a>G <a>T <a>S <a>M <a>G <a>G <a>Q <a>V <a>A <a>G <a>V <a>Y <a>A <a>A <a>Y <a>Y <a>P <a>S <a>D <a>V <a>S <a>S <a>L <a>C <a>L <a>V <a>C <a>P <a>A <a>G <a>L <a>Q <a>Y <a>S <a>T <a>D <a>N <a>Q <a>F <a>V <a>Q <a>R <a>L <a>K <a>E <a>L <a>Q <a>G <a>S <a>A <a>A <a>V <a>E <a>K <a>I <a>P <a>L <a>I <a>P <a>S <a>T <a>P <a>E <a>E <a>M <a>S <a>E <a>M <a>L <a>Q <a>L <a>C <a>S <a>Y <a>V <a>R <a>F <a>K <a>V <a>P <a>Q <a>Q <a>I <a>L <a>Q <a>G <a>L <a>V <a>D <a>V <a>R <a>I <a>P <a>H <a>N <a>H <a>T <a>S <a>S <a>C <a>L <a>F <a>Q <a>G <a>S <a>P <a>G <a>S <a>L <a>H </protein> homology model. The three-dimensional fold of the BH2 region of the 82-residue p68 protein, a mitogenic transcriptional activator, has been predicted using\n",
      "MDLDVVNMFVIAGGTLAIPILAFVASFLLWPSALIRIYYWYWRRTLGMQVRYVHHEDYQFCYSFRGRPGHKPSILMLHGFSAHKDMWLSVVKFLPKNLHLVCVDMPGHEGTTRSSLDDLSIDGQVKRIHQFVECLKLNKKPFHLVGTSMGGQVAGVYAAYYPSDVSSLCLVCPAGLQYSTDNQFVQRLKELQGSAAVEKIPLIPSTPEEMSEMLQLCSYVRFKVPQQILQGLVDVRIPHNHTSSCLFQGSPGSLH\n",
      "\n",
      "<s>A short protein with only beta helix is <protein> <a>M <a>S <a>E <a>H <a>E <a>T <a>S <a>Q <a>E <a>T <a>N <a>I <a>N <a>E <a>L <a>K <a>K <a>V <a>R <a>L <a>E <a>K <a>L <a>E <a>E <a>L <a>R <a>E <a>M <a>G <a>I <a>N <a>P <a>F <a>G <a>S <a>R <a>Y <a>I <a>R <a>D <a>T <a>D <a>T <a>A <a>A <a>I <a>L <a>Q <a>N <a>Y <a>T <a>A <a>L <a>E <a>G <a>Q <a>T <a>C <a>T <a>D <a>W <a>S <a>D <a>D <a>E <a>W <a>D <a>K <a>A <a>I <a>K <a>L <a>V <a>H <a>L <a>E <a>A <a>E <a>K <a>V <a>H <a>G <a>K <a>K <a>I <a>G <a>V <a>G <a>I <a>D <a>A <a>T <a>G <a>S <a>E <a>A <a>A <a>E <a>A <a>G <a>A <a>Y <a>V <a>F <a>Q <a>K <a>R <a>D <a>K <a>N <a>A <a>C <a>V <a>L <a>T <a>W <a>T <a>D <a>R <a>D <a>I <a>L <a>A <a>D <a>N <a>I <a>R <a>C <a>D <a>C <a>E <a>A <a>Q <a>N <a>G <a>R <a>L <a>H <a>T <a>A <a>C <a>K <a>H <a>R <a>A <a>A <a>A <a>A <a>L <a>E <a>H <a>H <a>H <a>H <a>H <a>H </protein> </s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "MSEHETSQETNINELKKVRLEKLEELREMGINPFGSRYIRDTDTAAILQNYTALEGQTCTDWSDDEWDKAIKLVHLEAEKVHGKKIGVGIDATGSEAAEAGAYVFQKRDKNACVLTWTDRDILADNIRCDCEAQNGRLHTACKHRAAAALEHHHHHH\n",
      "\n",
      "<s>A short protein with only beta helix is <protein> <a>M <a>K <a>F <a>S <a>R <a>F <a>A <a>E <a>T <a>I <a>Q <a>L <a>K <a>N <a>N <a>N <a>H <a>V <a>V <a>S <a>V <a>T <a>V <a>T <a>L <a>K <a>I <a>S <a>D <a>C <a>N <a>G <a>I <a>I <a>Y <a>F <a>T <a>D <a>L <a>Q <a>L <a>Q <a>D <a>G <a>D <a>Q <a>L <a>T <a>R <a>A <a>A <a>I <a>S <a>Q <a>A <a>Y <a>L <a>K <a>M <a>K <a>E <a>L <a>F <a>N <a>Y <a>S <a>S <a>I <a>I <a>F <a>I <a>F <a>E <a>V <a>I <a>E <a>V <a>F <a>R <a>A <a>N <a>K <a>V <a>S <a>S <a>H <a>H <a>V <a>R <a>T <a>I <a>F <a>E <a>V <a>S <a>S <a>K <a>A <a>E <a>T <a>F <a>L <a>T <a>L <a>F </protein> toxin. beta-Hairpins (beta-HPs) are important structural elements for building a number of biologically active peptides and proteins. We propose that a short 36-residue beta-HP might have a biological activity. A beta-HP with amino acid sequence of NH(2)-TKTVFGGGGTFGGGFGGGGFDGFR-COOH (beta-HP36) was synthesized and studied by CD, fluorescence emission, and fluorescence anisotropy. It was found that the beta-HP36 was folded into the beta-solenoid structure that was formed by the stacked beta-HPs. The beta-HP36 was also studied by atomic force microscopy (AFM) in buffer solutions with various ionic strengths. By CD it was found that, in\n",
      "MKFSRFAETIQLKNNNHVVSVTVTLKISDCNGIIYFTDLQLQDGDQLTRAAISQAYLKMKELFNYSSIIFIFEVIEVFRANKVSSHHVRTIFEVSSKAETFLTLF\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer('A short protein with only beta helix is <protein>', return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.generate(input_ids, do_sample=True, top_p=0.95, max_new_tokens=300, num_return_sequences=20)\n",
    "for s in output:\n",
    "    s = tokenizer.decode(s)\n",
    "    if '</protein>' not in s:\n",
    "        continue\n",
    "    print(s)\n",
    "    s = s[s.find('<protein>') + len('<protein>'):s.find('</protein>')]\n",
    "    s = s.replace('<a>', '').replace(' ', '')\n",
    "    print(s)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>I will tell you something about SARS-COV-2 main protase. I will not show you its 3-D structure, because it is known. But the thing is that all the COVID-19 vaccines currently used in the world, are based on this enzyme structure. I want to tell you something very interesting about this enzyme. It is a single-chain 3C-like cysteine protease with a molecular weight of 294 kDa. It is an exogenous protein. It is coded in the RNA virus, and it is involved in viral gene processing, structural protein disassembly, and viral maturation. It cleaves the viral polyprotein into smaller proteins. It is used to disassemble the coronavirus and virus. This protein is a very useful molecule for SARS-CoV-2. It is very dangerous because it is a single-chain enzyme and is a part of the RNA virus. The antibody that you find in these vaccines will only affect the viral enzyme. I do not like the vaccines, but I will tell you the problem. I am trying to explain to you that it is possible to create another virus that can not be identified by these vaccines. There will be another mutation virus that has never been discovered. Because you can not destroy any protein by vaccine. This protein is so useful for SAR\n",
      "<s>I will tell you something about SARS-COV-2 main protase. We will discuss the structure, dynamics and mutations. SARS-CoV-2 is a coronavirus infectious disease (COVID-19) which spreads rapidly among human beings and other animals. The infection rate is increasing day by day. There is no effective vaccine and drug available for treating the COVID-19. The COVID-19 virus is a novel coronavirus. As a result, we cannot predict any outcomes or consequences of this pandemic in the future. We are taking some steps, but it is not enough to control or prevent COVID-19. Many scientific studies are investigating COVID-19 infection and its consequences. Among them, the scientific studies on COVID-19 main protease are increasing every day. As a result, these proteases are the potential target for developing vaccines and drugs. Therefore, we are trying to predict the structure, dynamics and mutations of SARS-CoV-2 main protease. We analyzed a large amount of SARS-CoV-2 main protease sequences to characterize its structures and dynamics. As a result, we observed the dynamic structure of SARS-CoV-2 main protease, and its variants have the potential to enhance the dynamics of main protease in the SARS-CoV-2 virus. We also observed the SARS-CoV-2 main\n",
      "<s> I will tell you something about SARS-COV-2 main protase. The novel coronavirus, SARS-COV-2, is responsible for causing COVID-19, which has caused more than 2 million cases of coronavirus illness worldwide. The main protease enzyme (M</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "<s>I will tell you something about SARS-COV-2 main protase. Corona virus SARS-CoV-2 is a positive strand RNA virus. It utilizes an unknown endogenous RNA-Dependent RNA Polymerase (RdRp), also named 3CLpro, for the maturation of viral proteins encoded by the 3 'and 5' extremity of the viral genome. This transcription activity is essential for the virus replication and can be a target for drugs able to inhibit the virus replication. The 3CLpro belongs to the M46 family of protease that share the S3 subsite and are active in the hydrolysis of N-terminal peptide bonds. It is well known that the M46 protease, like the majority of serine proteases, undergoes an activation-transition state stabilization of the acyl-enzyme complex with a histidine side chain protonated and the catalytic serine deprotonated. The protonation of the active site histidine (catalytic histidine or His (57) in 3CLpro) could play an important role in the enzyme activity and regulation of the catalysis process. We have examined the effects of the histidine protonation on the free enzyme and on the SARS-CoV-2 enzyme complexed\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer('I will tell you something about SARS-COV-2 main protase.', return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.generate(input_ids, do_sample=True, top_p=0.95, max_new_tokens=300, num_return_sequences=4)\n",
    "for s in output:\n",
    "    s = tokenizer.decode(s)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>A drug for SARS-CoV-2 is <mol> <m>C <m>c <m>1 <m>c <m>c <m>c <m>c <m>( <m>C <m>) <m>c <m>1 <m>O <m>C <m>C <m>( <m>= <m>O <m>) <m>N <m>[C@@H] <m>( <m>C <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>) <m>[C@@H] <m>( <m>O <m>) <m>C <m>[C@H] <m>( <m>C <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>) <m>N <m>C <m>( <m>= <m>O <m>) <m>[C@H] <m>( <m>C <m>( <m>C <m>) <m>C <m>) <m>N <m>1 <m>C <m>C <m>C <m>N <m>C <m>1 <m>= <m>O </mol> that inhibits the SARS-CoV-2 main protease. The SARS-CoV-2 main protease (Mpro) is a promising drug target for the treatment of COVID-19. In this study, we performed a virtual screening of Food and Drug Administration (FDA)-approved drugs against SARS-CoV-2 Mpro and identified lopinavir as a potent inhibitor of SARS-CoV-2 Mpro. Lopinavir inhibited SARS-CoV-2 Mpro with an IC50 value of 0.001 M, which is 1000-fold more potent than that of remdesivir, the only FDA-approved drug for the treatment of COVID-19. Lopinavir inhibited SARS-CoV-2 replication in Vero E6 cells with an EC50 value of 0.001 M, which is 1000-\n",
      "Cc1cccc(C)c1OCC(=O)N[C@@H](Cc1ccccc1)[C@@H](O)C[C@H](Cc1ccccc1)NC(=O)[C@H](C(C)C)N1CCCNC1=O\n",
      "<s>A drug for SARS-CoV-2 is <mol> <m>C <m>c <m>1 <m>c <m>c <m>c <m>c <m>( <m>C <m>) <m>c <m>1 <m>O <m>C <m>C <m>( <m>= <m>O <m>) <m>N <m>[C@@H] <m>( <m>C <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>) <m>[C@@H] <m>( <m>O <m>) <m>C <m>[C@H] <m>( <m>C <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>) <m>N <m>C <m>( <m>= <m>O <m>) <m>[C@H] <m>( <m>C <m>( <m>C <m>) <m>C <m>) <m>N <m>1 <m>C <m>C <m>C <m>N <m>C <m>1 <m>= <m>O </mol> that inhibits the SARS-CoV-2 main protease. The SARS-CoV-2 main protease (Mpro) is a promising drug target for the treatment of COVID-19. In this study, we performed a virtual screening of Food and Drug Administration (FDA)-approved drugs against SARS-CoV-2 Mpro and identified lopinavir as a potent inhibitor of SARS-CoV-2 Mpro. Lopinavir inhibited SARS-CoV-2 Mpro with an IC50 value of 0.001 M, which is 1000-fold more potent than that of remdesivir, the only FDA-approved drug for the treatment of COVID-19. Lopinavir inhibited SARS-CoV-2 replication in Vero E6 cells with an EC50 value of 0.002 M, which is 100-fold\n",
      "Cc1cccc(C)c1OCC(=O)N[C@@H](Cc1ccccc1)[C@@H](O)C[C@H](Cc1ccccc1)NC(=O)[C@H](C(C)C)N1CCCNC1=O\n",
      "<s>A drug for SARS-CoV-2 is <mol> <m>C <m>c <m>1 <m>c <m>c <m>c <m>c <m>( <m>C <m>) <m>c <m>1 <m>O <m>C <m>C <m>( <m>= <m>O <m>) <m>N <m>[C@@H] <m>( <m>C <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>) <m>[C@@H] <m>( <m>O <m>) <m>C <m>[C@H] <m>( <m>C <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>) <m>N <m>C <m>( <m>= <m>O <m>) <m>[C@H] <m>( <m>C <m>( <m>C <m>) <m>C <m>) <m>N <m>1 <m>C <m>C <m>C <m>N <m>C <m>1 <m>= <m>O </mol> that inhibits the SARS-CoV-2 main protease. The SARS-CoV-2 main protease (Mpro) is a promising drug target for the treatment of COVID-19. In this study, we performed a virtual screening of Food and Drug Administration (FDA)-approved drugs against SARS-CoV-2 Mpro and identified lopinavir as a potent inhibitor of SARS-CoV-2 Mpro. Lopinavir inhibited SARS-CoV-2 Mpro with an IC50 value of 0.001 M, which is 1000-fold more potent than that of remdesivir, the only FDA-approved drug for the treatment of COVID-19. Lopinavir inhibited SARS-CoV-2 replication in Vero E6 cells with an EC50 value of 0.002 M, which is 1000-\n",
      "Cc1cccc(C)c1OCC(=O)N[C@@H](Cc1ccccc1)[C@@H](O)C[C@H](Cc1ccccc1)NC(=O)[C@H](C(C)C)N1CCCNC1=O\n",
      "<s>A drug for SARS-CoV-2 is <mol> <m>C <m>c <m>1 <m>c <m>c <m>c <m>c <m>( <m>C <m>) <m>c <m>1 <m>O <m>C <m>C <m>( <m>= <m>O <m>) <m>N <m>[C@@H] <m>( <m>C <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>) <m>[C@@H] <m>( <m>O <m>) <m>C <m>[C@H] <m>( <m>C <m>c <m>1 <m>c <m>c <m>c <m>c <m>c <m>1 <m>) <m>N <m>C <m>( <m>= <m>O <m>) <m>[C@H] <m>( <m>C <m>( <m>C <m>) <m>C <m>) <m>N <m>1 <m>C <m>C <m>C <m>N <m>C <m>1 <m>= <m>O </mol> that inhibits the SARS-CoV-2 main protease. The SARS-CoV-2 main protease (Mpro) is a promising drug target for the treatment of COVID-19. In this study, we performed a virtual screening of Food and Drug Administration (FDA)-approved drugs against SARS-CoV-2 Mpro and identified lopinavir as a potent inhibitor of SARS-CoV-2 Mpro. Lopinavir inhibited SARS-CoV-2 Mpro with an IC50 value of 0.001 M, which is 1000-fold more potent than that of remdesivir, the only FDA-approved drug for the treatment of COVID-19. Lopinavir inhibited SARS-CoV-2 replication in Vero E6 cells with an EC50 value of 0.001 M, which is 100-fold\n",
      "Cc1cccc(C)c1OCC(=O)N[C@@H](Cc1ccccc1)[C@@H](O)C[C@H](Cc1ccccc1)NC(=O)[C@H](C(C)C)N1CCCNC1=O\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer('A drug for SARS-CoV-2 is <mol>', return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.generate(input_ids, num_beams=4, max_new_tokens=300, num_return_sequences=4)\n",
    "for s in output:\n",
    "    s = tokenizer.decode(s)\n",
    "    print(s)\n",
    "    mol = s[s.find('<mol>') + len('<mol>'):s.find('</mol>')]\n",
    "    mol = mol.replace('<m>', '').replace(' ', '')\n",
    "    print(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
