{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'SFMDecTokenizer'.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:27<00:00, 13.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 embed_tokens.weight torch.Size([40014, 4096])\n",
      "01 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "01 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "01 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "01 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "01 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "01 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "01 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "01 input_layernorm.weight torch.Size([4096])\n",
      "01 post_attention_layernorm.weight torch.Size([4096])\n",
      "02 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "02 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "02 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "02 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "02 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "02 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "02 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "02 input_layernorm.weight torch.Size([4096])\n",
      "02 post_attention_layernorm.weight torch.Size([4096])\n",
      "03 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "03 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "03 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "03 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "03 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "03 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "03 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "03 input_layernorm.weight torch.Size([4096])\n",
      "03 post_attention_layernorm.weight torch.Size([4096])\n",
      "04 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "04 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "04 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "04 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "04 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "04 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "04 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "04 input_layernorm.weight torch.Size([4096])\n",
      "04 post_attention_layernorm.weight torch.Size([4096])\n",
      "05 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "05 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "05 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "05 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "05 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "05 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "05 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "05 input_layernorm.weight torch.Size([4096])\n",
      "05 post_attention_layernorm.weight torch.Size([4096])\n",
      "06 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "06 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "06 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "06 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "06 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "06 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "06 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "06 input_layernorm.weight torch.Size([4096])\n",
      "06 post_attention_layernorm.weight torch.Size([4096])\n",
      "07 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "07 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "07 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "07 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "07 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "07 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "07 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "07 input_layernorm.weight torch.Size([4096])\n",
      "07 post_attention_layernorm.weight torch.Size([4096])\n",
      "08 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "08 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "08 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "08 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "08 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "08 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "08 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "08 input_layernorm.weight torch.Size([4096])\n",
      "08 post_attention_layernorm.weight torch.Size([4096])\n",
      "09 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "09 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "09 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "09 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "09 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "09 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "09 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "09 input_layernorm.weight torch.Size([4096])\n",
      "09 post_attention_layernorm.weight torch.Size([4096])\n",
      "10 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "10 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "10 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "10 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "10 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "10 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "10 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "10 input_layernorm.weight torch.Size([4096])\n",
      "10 post_attention_layernorm.weight torch.Size([4096])\n",
      "11 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "11 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "11 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "11 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "11 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "11 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "11 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "11 input_layernorm.weight torch.Size([4096])\n",
      "11 post_attention_layernorm.weight torch.Size([4096])\n",
      "12 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "12 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "12 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "12 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "12 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "12 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "12 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "12 input_layernorm.weight torch.Size([4096])\n",
      "12 post_attention_layernorm.weight torch.Size([4096])\n",
      "13 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "13 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "13 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "13 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "13 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "13 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "13 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "13 input_layernorm.weight torch.Size([4096])\n",
      "13 post_attention_layernorm.weight torch.Size([4096])\n",
      "14 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "14 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "14 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "14 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "14 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "14 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "14 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "14 input_layernorm.weight torch.Size([4096])\n",
      "14 post_attention_layernorm.weight torch.Size([4096])\n",
      "15 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "15 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "15 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "15 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "15 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "15 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "15 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "15 input_layernorm.weight torch.Size([4096])\n",
      "15 post_attention_layernorm.weight torch.Size([4096])\n",
      "16 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "16 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "16 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "16 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "16 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "16 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "16 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "16 input_layernorm.weight torch.Size([4096])\n",
      "16 post_attention_layernorm.weight torch.Size([4096])\n",
      "17 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "17 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "17 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "17 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "17 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "17 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "17 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "17 input_layernorm.weight torch.Size([4096])\n",
      "17 post_attention_layernorm.weight torch.Size([4096])\n",
      "18 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "18 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "18 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "18 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "18 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "18 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "18 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "18 input_layernorm.weight torch.Size([4096])\n",
      "18 post_attention_layernorm.weight torch.Size([4096])\n",
      "19 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "19 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "19 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "19 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "19 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "19 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "19 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "19 input_layernorm.weight torch.Size([4096])\n",
      "19 post_attention_layernorm.weight torch.Size([4096])\n",
      "20 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "20 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "20 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "20 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "20 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "20 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "20 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "20 input_layernorm.weight torch.Size([4096])\n",
      "20 post_attention_layernorm.weight torch.Size([4096])\n",
      "21 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "21 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "21 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "21 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "21 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "21 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "21 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "21 input_layernorm.weight torch.Size([4096])\n",
      "21 post_attention_layernorm.weight torch.Size([4096])\n",
      "22 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "22 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "22 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "22 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "22 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "22 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "22 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "22 input_layernorm.weight torch.Size([4096])\n",
      "22 post_attention_layernorm.weight torch.Size([4096])\n",
      "23 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "23 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "23 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "23 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "23 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "23 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "23 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "23 input_layernorm.weight torch.Size([4096])\n",
      "23 post_attention_layernorm.weight torch.Size([4096])\n",
      "24 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "24 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "24 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "24 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "24 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "24 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "24 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "24 input_layernorm.weight torch.Size([4096])\n",
      "24 post_attention_layernorm.weight torch.Size([4096])\n",
      "25 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "25 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "25 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "25 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "25 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "25 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "25 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "25 input_layernorm.weight torch.Size([4096])\n",
      "25 post_attention_layernorm.weight torch.Size([4096])\n",
      "26 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "26 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "26 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "26 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "26 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "26 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "26 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "26 input_layernorm.weight torch.Size([4096])\n",
      "26 post_attention_layernorm.weight torch.Size([4096])\n",
      "27 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "27 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "27 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "27 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "27 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "27 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "27 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "27 input_layernorm.weight torch.Size([4096])\n",
      "27 post_attention_layernorm.weight torch.Size([4096])\n",
      "28 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "28 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "28 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "28 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "28 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "28 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "28 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "28 input_layernorm.weight torch.Size([4096])\n",
      "28 post_attention_layernorm.weight torch.Size([4096])\n",
      "29 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "29 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "29 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "29 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "29 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "29 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "29 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "29 input_layernorm.weight torch.Size([4096])\n",
      "29 post_attention_layernorm.weight torch.Size([4096])\n",
      "30 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "30 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "30 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "30 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "30 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "30 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "30 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "30 input_layernorm.weight torch.Size([4096])\n",
      "30 post_attention_layernorm.weight torch.Size([4096])\n",
      "31 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "31 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "31 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "31 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "31 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "31 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "31 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "31 input_layernorm.weight torch.Size([4096])\n",
      "31 post_attention_layernorm.weight torch.Size([4096])\n",
      "32 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "32 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "32 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "32 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "32 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "32 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "32 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "32 input_layernorm.weight torch.Size([4096])\n",
      "32 post_attention_layernorm.weight torch.Size([4096])\n",
      "33 norm.weight torch.Size([4096])\n",
      "33 lm_head.weight torch.Size([40014, 4096])\n",
      "33 num_head.fc1.weight torch.Size([16384, 4096])\n",
      "33 num_head.fc1.bias torch.Size([16384])\n",
      "33 num_head.fc2.weight torch.Size([1, 16384])\n",
      "33 num_head.fc2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# load instructed-tuned ckpt\n",
    "from inference_module import SFMGenerator\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "ckpt_home = '/home/yinxia/blob1.v2/yinxia/scigpt/7bv3/overall_instruct_20240430/run2/global_step24128'\n",
    "generator = SFMGenerator(ckpt_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some post processors\n",
    "from rdkit import Chem, RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "\n",
    "def parse_smiles(smi):\n",
    "    r = smi.replace('<mol>', '').replace('</mol>', '')\n",
    "    r = r.replace('<m>', '')\n",
    "    r = r.replace(' ', '')\n",
    "    m = Chem.MolFromSmiles(r)\n",
    "    if m is None:\n",
    "        return None\n",
    "\n",
    "    return Chem.MolToSmiles(m)\n",
    "\n",
    "def parse_protein(fasta):\n",
    "    if '</protein>' not in fasta:\n",
    "        return None\n",
    "    fasta = fasta.replace('<protein>', '').replace('</protein>', '')\n",
    "    fasta = fasta.replace('<a>', '')\n",
    "    fasta = fasta.replace(' ', '')\n",
    "    return fasta.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam search output\n",
      "Cc1cccc(C)c1OCC(=O)N[C@@H](Cc1ccccc1)[C@@H](O)C[C@H](Cc1ccccc1)NC(=O)[C@H](C(C)C)N1CCCNC1=O\n",
      "CC(C)(C)NC(=O)[C@@H]1C[C@@H]2CCCC[C@@H]2CN1C[C@@H](O)[C@H](Cc1ccccc1)NC(=O)[C@H](CC(N)=O)NC(=O)c1ccc2ccccc2n1\n",
      "Cc1cc(C#N)cc(C)c1Oc1nc(Nc2ccc(C#N)cc2)nc(N)c1Br\n",
      "Cc1ccnc2c1NC(=O)c1cccnc1N2C1CC1\n",
      "random sample search output\n",
      "('Cc1cccc(C)c1OCC(=O)N[C@@H](Cc1ccccc1)[C@@H](O)C[C@H](Cc1ccccc1)NC(=O)[C@H](C(C)C)N1CCCNC1=O', 7)\n",
      "('Cc1ccnc2c1NC(=O)c1cccnc1N2C1CC1', 3)\n",
      "('Cc1cc(C#N)cc(C)c1Oc1nc(Nc2ccc(C#N)cc2)nc(N)c1Br', 3)\n",
      "('CC(C)(C)NC(=O)[C@@H]1CN(Cc2cccnc2)CCN1C[C@@H](O)C[C@@H](Cc1ccccc1)C(=O)N[C@H]1c2ccccc2C[C@H]1O', 2)\n",
      "('CC(C)c1nc(CN(C)C(=O)N[C@H](C(=O)N[C@@H](Cc2ccccc2)[C@H](O)CN2CCCCC2)C(C)(C)C)cs1', 2)\n",
      "('CC(C)(C)NC(=O)[C@@H]1C[C@@H]2CCCC[C@@H]2CN1C[C@@H](O)[C@H](Cc1ccccc1)NC(=O)[C@H](CC(N)=O)NC(=O)c1ccc2ccccc2n1', 1)\n",
      "('CC(C)(C)C(=O)OCOP(=O)(COCCn1cnc2c(N)ncnc21)OCOC(=O)C(C)(C)C', 1)\n",
      "('CC(C)Nc1cccnc1N1CCN(C(=O)c2cc3cc(NS(C)(=O)=O)ccc3[nH]2)CC1', 1)\n",
      "('CC(C)OC(=O)[C@H](C)N[P@](=O)(CO[C@H](C)Cn1cnc2c(N)ncnc21)Oc1ccccc1', 1)\n",
      "('CC(C)c1ccc2oc3nc(N)c(C(=O)O)cc3c(=O)c2c1', 1)\n",
      "('COC(=O)N[C@H](C(=O)N[C@@H](Cc1ccccc1)[C@@H](O)CN(Cc1ccc(-c2ccccn2)cc1)NC(=O)[C@@H](NC(=O)OC)C(C)(C)C)C(C)(C)C', 1)\n",
      "('CC(C)c1nc(CN(C)C(=O)N[C@H](C(=O)N[C@@H](Cc2ccccc2)[C@H](O)CN2CCCCC2)C(C)C)cn1Cc1ccccc1', 1)\n",
      "('Nc1nc(NC2CC2)c2ncn([C@H]3C=C[C@@H](CO)C3)c2n1', 1)\n",
      "('CCN(CC)C(=O)[C@H]1CN2CCc3cc(OC)c(OC)cc3[C@@H]2C[C@@H]1OC(C)=O', 1)\n",
      "('COc1cccc(C(=O)Nc2ccc(OCCN3CCOCC3)c(-c3ccnn3C)c2)c1', 1)\n",
      "('CC(C)c1c(C(=O)Nc2ccccc2)c(-c2ccccc2)c(-c2ccc(F)cc2)n1CC[C@@H](O)C[C@@H](O)CC(=O)O', 1)\n",
      "('Cc1ccccc1N=C(N)c1ccc(-c2ccc(C(=O)N(Cc3ccccn3)c3ccccn3)s2)s1', 1)\n",
      "('Cc1nnc(C(=O)NC(C)(C)c2nc(C(=O)NCc3ccc(F)cc3)c(O)c(=O)n2C)o1', 1)\n",
      "('COc1cc(Cc2cnc(N)nc2N)cc(OC)c1OC', 1)\n",
      "('CC(C)CN(C[C@@H](O)[C@H](Cc1ccccc1)NC(=O)[C@H](CC(N)=O)NC(=O)c1ccc2ccccc2n1)C(=O)NC(C)(C)C', 1)\n",
      "('CC(C)c1nc(CN(C)C(=O)N[C@H](C(=O)N[C@@H](Cc2ccccc2)[C@H](O)CN2CCCCC2)C(C)(C)C)cn1Cc1ccccc1', 1)\n",
      "('Nc1ccn([C@H]2CC[C@@H](CO)O2)c(=O)n1', 1)\n",
      "('CCN(CC)C(=S)SSC(=S)N(CC)CC', 1)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "query = 'generate an inhibitor for HIV'\n",
    "\n",
    "beam_outs = []\n",
    "random_outs = defaultdict(int)\n",
    "\n",
    "# beam seaarch outputs\n",
    "ret = generator.chat(query, do_sample=False)\n",
    "for r in ret:\n",
    "    s = parse_smiles(r)\n",
    "    if s is not None:\n",
    "        beam_outs.append(s)\n",
    "\n",
    "print('beam search output')\n",
    "for s in beam_outs:\n",
    "    print(s)\n",
    "\n",
    "# random s\n",
    "for _ in range(10):\n",
    "    ret = generator.chat(query, do_sample=True)\n",
    "    for r in ret:\n",
    "        s = parse_smiles(r)\n",
    "        if s is not None:\n",
    "            random_outs[s] += 1\n",
    "\n",
    "sorted_random_outs = sorted(random_outs.items(), key=lambda x: x[1], reverse=True)\n",
    "print('random sample search output')\n",
    "for s in sorted_random_outs:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of generated cmpds\n",
      "O=C(O)CCC(=O)O 0.5437714031598524\n",
      "O=C(O)/C=C/C(=O)O 0.4859839100851221\n",
      "O=C(O)CCCCCCCCC(=O)O 0.5622977280227048\n",
      "O=C(O)CCCC(=O)O 0.5774203601545895\n",
      "Results of the original cmpd\n",
      "0.5501217966938848\n"
     ]
    }
   ],
   "source": [
    "prop = 'QED' \n",
    "# You can replace $pror to any property you want, because we use GPT4 to extract the data\n",
    "\n",
    "from rdkit.Chem.QED import qed\n",
    "\n",
    "query_mol = r\"CC(=O)OC1=CC=CC=C1C(=O)O\"\n",
    "s = Chem.MolFromSmiles(query_mol)\n",
    "s2 = Chem.MolToSmiles(s)\n",
    "\n",
    "query = f'improve the {prop} of {s2}'\n",
    "filtered_mol = []\n",
    "ret = generator.chat(query, do_sample=False)\n",
    "for r in ret:\n",
    "    s = parse_smiles(r)\n",
    "    if s:\n",
    "        filtered_mol.append(s)\n",
    "\n",
    "print('Results of generated cmpds')\n",
    "for s in filtered_mol:\n",
    "    m = Chem.MolFromSmiles(s)\n",
    "    print(s, qed(m))\n",
    "\n",
    "print('Results of the original cmpd')\n",
    "m = Chem.MolFromSmiles(query_mol)\n",
    "print(qed(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:12<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "query_mol = r\"C2=NC(=NC=C2)NC3=CC=C(C=C3)N4CCOCC4\"\n",
    "m = Chem.MolFromSmiles(query_mol)\n",
    "s2 = Chem.MolToSmiles(m)\n",
    "\n",
    "query = 'Improve the oral bioactivity of <mol>' + s2 + '</mol>'\n",
    "\n",
    "filtered_mol = defaultdict(int)\n",
    "for _ in tqdm(range(10)):\n",
    "    ret = generator.chat(query, do_sample=True)\n",
    "    for r in ret:\n",
    "        s = parse_smiles(r)\n",
    "        if s:\n",
    "            filtered_mol[s] += 1\n",
    "\n",
    "sorted_filtered_mol = sorted(filtered_mol.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNRYIMPIIIILMVGCALCVMFIYYPALKYVMYWKAKSELSKWNIFHREWIKHGKN\n",
      "MGKCCVTTCKNKQQYDCVCCNEGCAKKICECVNHKCVTKECERCK\n",
      "MGSMSSWLRATHGQNEMRCHTCSPSASHLSRKLVPWAPGPGPVGPAGRAAARGRRARPWRAAAAVGPGRPRAAPRSPAPGPAGAPWRLWARPPAARAGGRGAGPESSSAGPRPRRRARRPRAPGAAAAGGGALRRPAEAAGARAEAGPDPAVRRRGGSGRRRRLHRDLALLVLEALAAAHDGERSAQQQTEEELEAEVAALRRALGRLRLVEEEARLVVLTPEAVRAELEPEPEPEVQEAWEDHLQEGARGRRDPGSC\n",
      "MADKRGGGGRGGGGPRRGDRRPRDEQEEAAAAAGAGGEEAGGAAAGGVRAERRERGGRAAAGRRGREGGRRRRGAARRGRRRGRGRGRSRGGDGARARGG\n"
     ]
    }
   ],
   "source": [
    "query = 'Generate a membrane protein'\n",
    "ret = generator.chat(query, do_sample=True, repetition_penalty=1.2, max_new_tokens=300)\n",
    "for r in ret:\n",
    "    r2 = parse_protein(r)\n",
    "    if r2:\n",
    "        print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
