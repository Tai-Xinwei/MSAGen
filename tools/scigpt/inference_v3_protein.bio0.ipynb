{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sfm.data.sci_data.SFMDecTokenizer import SFMDecTokenizer\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ckpt(name, ckpt):\n",
    "    for k, v in ckpt.items():\n",
    "        if 'dummy' not in k:\n",
    "            print(name, k, v.shape)\n",
    "\n",
    "def process_protein(output):\n",
    "    if '</protein>' not in output:\n",
    "        return None\n",
    "    m = re.search(r'<protein>(.*?)</protein>', output)\n",
    "    if m:\n",
    "        s = m.group(1)\n",
    "        s = s.replace('<a>', '')\n",
    "        s = s.replace(' ', '')\n",
    "        return s.strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_home = '/hai1/mfm/ds_dataset/llama2/llama-2-7b'\n",
    "tokenizer = SFMDecTokenizer.from_pretrained(\n",
    "    tokenizer_home,\n",
    "    prot_spm_path='/blob/shufxi/data/scigpt/ur50bpe/bpe',\n",
    "    dna_spm_path='/blob/shufxi/data/scigpt/dnabpe/bpe',\n",
    "    rna_spm_path='/blob/shufxi/data/scigpt/rnabpe/bpe',\n",
    ")\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.tokenize('<protein>AABBCCDD</protein>'))\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_home,\n",
    ")\n",
    "print(len(llama_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(tokenizer_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_home = r'/hai1/mfm/shufxi/scigpt/7bv3/stageA_200k/global_step140999/'\n",
    "# ckpt_home = r'/hai1/mfm/shufxi/scigpt/7bv3/stageA_200k/global_step999/'\n",
    "# ckpt_home = r'/hai1/mfm/shufxi/scigpt/7bv3/stageA_prot_e10_bs256/global_step19999/' # full finetune\n",
    "ckpt_home = r\"/hai1/mfm/shufxi/scigpt/7bv3/stageA_prot_e10_bs512_emb_8xG8H100/global_step5781/\" # emb finetune, load llama emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = deepcopy(model)\n",
    "model_dict = test_model.state_dict()\n",
    "ckpt_dict = {}\n",
    "\n",
    "layer0 = torch.load(os.path.join(ckpt_home, \"layer_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict['model.embed_tokens.weight'] = layer0['embed_tokens.weight']#[:32000]\n",
    "show_ckpt('layer0', layer0)\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 1).zfill(2)\n",
    "    layer = torch.load(os.path.join(ckpt_home, f\"layer_{l_index}-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    show_ckpt(l_index, layer)\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        ckpt_dict[f\"model.layers.{l}.{k}\"] = layer[k]\n",
    "\n",
    "layer = torch.load(os.path.join(ckpt_home, \"layer_33-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "show_ckpt(33, layer)\n",
    "ckpt_dict[\"model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(ckpt_home, \"layer_34-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "show_ckpt(34, layer)\n",
    "ckpt_dict[\"lm_head.weight\"] = layer[\"lm_head.weight\"]#[:32000]\n",
    "model_dict.update(ckpt_dict)\n",
    "\n",
    "test_model.resize_token_embeddings(len(tokenizer))\n",
    "test_model.load_state_dict(model_dict)\n",
    "test_model = test_model.cuda()\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(torch.abs(model.state_dict()['model.embed_tokens.weight'] - test_model.state_dict()['model.embed_tokens.weight'][:32000].cpu())))\n",
    "print(torch.sum(torch.abs(model.state_dict()['model.layers.10.self_attn.k_proj.weight'] - test_model.state_dict()['model.layers.10.self_attn.k_proj.weight'].cpu())))\n",
    "print(torch.sum(torch.abs(model.state_dict()['lm_head.weight'] - test_model.state_dict()['lm_head.weight'][:32000].cpu())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.state_dict()['model.embed_tokens.weight'].shape)\n",
    "print(model.state_dict()['lm_head.weight'].shape)\n",
    "print(ckpt_dict['model.embed_tokens.weight'].shape)\n",
    "print(ckpt_dict['lm_head.weight'].shape)\n",
    "print(test_model.state_dict()['model.embed_tokens.weight'].shape)\n",
    "print(test_model.state_dict()['lm_head.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model = model.cuda()\n",
    "encodings = tokenizer([\"An apple a day\", \"An apple a day keeps the doctor away.\"], padding=True, return_tensors='pt')\n",
    "input_ids = encodings.input_ids.cuda()\n",
    "target_ids = input_ids.clone()\n",
    "target_ids[target_ids == tokenizer.pad_token_id] = -100\n",
    "with torch.no_grad():  \n",
    "    outputs = test_model(input_ids, labels=input_ids)\n",
    "    neg_log_likelihood = outputs.loss\n",
    "    perplexity = torch.exp(neg_log_likelihood)\n",
    "print(perplexity.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer('An apple a day', return_tensors=\"pt\").input_ids.cuda()\n",
    "# input_ids = tokenizer('<protein>', return_tensors=\"pt\").input_ids.cuda()\n",
    "output = test_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=100,\n",
    "    num_return_sequences=4,\n",
    "    return_dict_in_generate=True,\n",
    "    # output_scores=True,\n",
    "    #do_sample=True,\n",
    "    # repetition_penalty=1.2,\n",
    "    # num_beams=5,\n",
    "    # max_new_tokens=512,\n",
    "    # num_return_sequences=1,\n",
    "    # return_dict_in_generate=True,\n",
    "    # output_scores=True,\n",
    "    # do_sample=True,\n",
    "    # top_p=0.95,\n",
    "    # repetition_penalty=1.5,\n",
    ")\n",
    "\n",
    "#res = tokenizer.decode(output.sequences[0], skip_special_tokens=False)\n",
    "#print(res)\n",
    "for i in range(len(output.sequences)):\n",
    "    # print(s, output.sequences_scores[i].item())\n",
    "    s = tokenizer.decode(output.sequences[i])\n",
    "    print(s)\n",
    "    # print(s, output.sequences_scores[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sfm.data.sci_data.dataset import ProcessedSciDataset\n",
    "from sfm.data.sci_data.SFMDecTokenizer import SFMDecTokenizer\n",
    "from sfm.logging import logger\n",
    "from sfm.models.scigpt.config import (\n",
    "    ScigptConfig,\n",
    "    scigpt_7b_config,\n",
    "    scigpt_13b_config,\n",
    "    scigpt_350m_config,\n",
    "    scigpt_shallow_config,\n",
    "    scigpt_tiny_config,\n",
    ")\n",
    "from sfm.models.scigpt.scigpt import ScigptModel\n",
    "from sfm.pipeline.accelerator.trainer import Trainer\n",
    "from sfm.utils import arg_utils\n",
    "from sfm.utils.cli_utils import cli\n",
    "\n",
    "config_registry = {\n",
    "    \"scigpt_tiny\": scigpt_tiny_config,\n",
    "    \"scigpt_shallow\": scigpt_shallow_config,\n",
    "    \"scigpt_350m\": scigpt_350m_config,\n",
    "    \"scigpt\": scigpt_shallow_config,\n",
    "    \"scigpt_7b\": scigpt_7b_config,\n",
    "    \"scigpt_13b\": scigpt_13b_config,\n",
    "}\n",
    "\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "args = ArgumentParser()\n",
    "args.model_type=\"scigpt_7b_config\"\n",
    "args.vocab_size=40014\n",
    "args.pad_token_id=32000\n",
    "args.max_position_embeddings=4096\n",
    "args.bf16=True\n",
    "args.strategy=\"Pipeline\"\n",
    "args.pipeline_model_parallel_size=1\n",
    "args.pp_partition_layer_name=\"LlamaDecoderLayerPP\"\n",
    "args.load_ckpt=True\n",
    "args.pretrained_ckpt_path=\"/hai1/mfm/ds_dataset/llama2/llama-2-7b/\"\n",
    "args.unfreeze_param_list=\"lm_head.weight,embed_tokens.weight\"\n",
    "args.learnable_cutoff=32000\n",
    "args.infer=True\n",
    "args.llm_model_name_or_path=\"/hai1/mfm/ds_dataset/llama2/llama-2-7b/\"\n",
    "\n",
    "\n",
    "\n",
    "config = arg_utils.from_args(args, ScigptConfig)\n",
    "config = config_registry.get(config.model_type, scigpt_tiny_config)(config)\n",
    "config.llm_model_name_or_path=\"/hai1/mfm/ds_dataset/llama2/llama-2-7b/\"\n",
    "\n",
    "model = ScigptModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_home = '/hai1/mfm/ds_dataset/llama2/llama-2-7b'\n",
    "tokenizer = SFMDecTokenizer.from_pretrained(\n",
    "    tokenizer_home,\n",
    "    prot_spm_path='/blob/shufxi/data/scigpt/ur50bpe/bpe',\n",
    "    dna_spm_path='/blob/shufxi/data/scigpt/dnabpe/bpe',\n",
    "    rna_spm_path='/blob/shufxi/data/scigpt/rnabpe/bpe',\n",
    ")\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.tokenize('<protein>AABBCCDD</protein>'))\n",
    "print(tokenizer.tokenize('An apple  a day\\nYes\\nThis is\\n\\nan applenade\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model=model.cuda()\n",
    "\n",
    "input_ids = tokenizer('An apple a day', return_tensors=\"pt\").input_ids.cuda()\n",
    "#input_ids = tokenizer('<protein>AA', return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.decoder.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=100,\n",
    "    num_return_sequences=4,\n",
    "    return_dict_in_generate=True,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "res = tokenizer.decode(output.sequences[0], skip_special_tokens=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/blob/renqian/data/sfm/ur90/valid.uniref90.shuf.10k\", \"r\") as f:\n",
    "    lines = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_lengths = []\n",
    "sfm_lengths = []\n",
    "for line in lines:\n",
    "    input_ids = llama_tokenizer(line, return_tensors=\"pt\").input_ids\n",
    "    llama_lengths.append(input_ids.shape[1])\n",
    "    input_ids = tokenizer(line, return_tensors=\"pt\").input_ids\n",
    "    sfm_lengths.append(input_ids.shape[1])\n",
    "print(f\"llama: max {max(llama_lengths)}, min {min(llama_lengths)}, avg {sum(llama_lengths) / len(llama_lengths)}\")\n",
    "print(f\"sfm: max {max(sfm_lengths)}, min {min(sfm_lengths)}, avg {sum(sfm_lengths) / len(sfm_lengths)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
