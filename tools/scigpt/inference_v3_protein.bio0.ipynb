{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sfm.data.sci_data.SFMDecTokenizer import SFMDecTokenizer\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ckpt(name, ckpt):\n",
    "    for k, v in ckpt.items():\n",
    "        if 'dummy' not in k:\n",
    "            print(name, k, v.shape)\n",
    "\n",
    "def process_protein(output):\n",
    "    if '</protein>' not in output:\n",
    "        return None\n",
    "    m = re.search(r'<protein>(.*?)</protein>', output)\n",
    "    if m:\n",
    "        s = m.group(1)\n",
    "        s = s.replace('<a>', '')\n",
    "        s = s.replace(' ', '')\n",
    "        return s.strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'SFMDecTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40014\n",
      "['<protein>', '▁A', 'AB', 'B', 'CC', 'DD', '</protein>']\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "tokenizer_home = '/hai1/mfm/ds_dataset/llama2/llama-2-7b'\n",
    "tokenizer = SFMDecTokenizer.from_pretrained(\n",
    "    tokenizer_home,\n",
    "    prot_spm_path='/blob/shufxi/data/scigpt/ur50bpe/bpe',\n",
    "    dna_spm_path='/blob/shufxi/data/scigpt/dnabpe/bpe',\n",
    "    rna_spm_path='/blob/shufxi/data/scigpt/rnabpe/bpe',\n",
    ")\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.tokenize('<protein>AABBCCDD</protein>'))\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_home,\n",
    ")\n",
    "print(len(llama_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.64s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(tokenizer_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_home = '/hai1/shufxi/scigpt/7bv2/stageB/global_step26999/'\n",
    "# ckpt_home = r'/home/yinxia/hai1/shufxi/scigpt/7bv3/stageB/global_step32999'\n",
    "# ckpt_home = r'/home/yinxia/blob1.v2/shufxi/scigpt/7bv3/inst/20240227121523/global_step3585'\n",
    "# ckpt_home = r'/home/yinxia//blob1.v2/shufxi/scigpt/7bv3/inst/20240301131100/global_step4753'\n",
    "# ckpt_home = r'/home/yinxia/hai1/shufxi/scigpt/7bv3/stageB.prot/global_step224655'\n",
    "\n",
    "# ckpt_home = r'/hai1/mfm/shufxi/scigpt/7bv3/stageA_200k/global_step140999/'\n",
    "# ckpt_home = r'/hai1/mfm/shufxi/scigpt/7bv3/stageA_200k/global_step999/'\n",
    "ckpt_home = r'/hai1/mfm/shufxi/scigpt/7bv3/stageA_prot_e10_bs256/global_step19999/' # full finetune\n",
    "# ckpt_home = r'/hai1/mfm/shufxi/scigpt/7bv3/stageA_prot_e10_bs256_emb/global_step29999/' # emb finetune, but not load llama emb\n",
    "# ckpt_home = r\"/hai1/mfm/shufxi/scigpt/7bv3/stageA_prot_e10_bs256_emb_bugfix_a100/global_step4999/\" # emb finetune, load llama emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer0 embed_tokens.weight torch.Size([40014, 4096])\n",
      "01 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "01 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "01 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "01 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "01 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "01 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "01 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "01 input_layernorm.weight torch.Size([4096])\n",
      "01 post_attention_layernorm.weight torch.Size([4096])\n",
      "02 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "02 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "02 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "02 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "02 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "02 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "02 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "02 input_layernorm.weight torch.Size([4096])\n",
      "02 post_attention_layernorm.weight torch.Size([4096])\n",
      "03 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "03 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "03 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "03 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "03 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "03 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "03 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "03 input_layernorm.weight torch.Size([4096])\n",
      "03 post_attention_layernorm.weight torch.Size([4096])\n",
      "04 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "04 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "04 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "04 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "04 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "04 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "04 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "04 input_layernorm.weight torch.Size([4096])\n",
      "04 post_attention_layernorm.weight torch.Size([4096])\n",
      "05 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "05 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "05 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "05 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "05 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "05 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "05 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "05 input_layernorm.weight torch.Size([4096])\n",
      "05 post_attention_layernorm.weight torch.Size([4096])\n",
      "06 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "06 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "06 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "06 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "06 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "06 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "06 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "06 input_layernorm.weight torch.Size([4096])\n",
      "06 post_attention_layernorm.weight torch.Size([4096])\n",
      "07 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "07 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "07 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "07 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "07 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "07 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "07 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "07 input_layernorm.weight torch.Size([4096])\n",
      "07 post_attention_layernorm.weight torch.Size([4096])\n",
      "08 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "08 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "08 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "08 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "08 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "08 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "08 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "08 input_layernorm.weight torch.Size([4096])\n",
      "08 post_attention_layernorm.weight torch.Size([4096])\n",
      "09 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "09 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "09 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "09 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "09 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "09 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "09 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "09 input_layernorm.weight torch.Size([4096])\n",
      "09 post_attention_layernorm.weight torch.Size([4096])\n",
      "10 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "10 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "10 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "10 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "10 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "10 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "10 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "10 input_layernorm.weight torch.Size([4096])\n",
      "10 post_attention_layernorm.weight torch.Size([4096])\n",
      "11 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "11 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "11 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "11 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "11 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "11 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "11 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "11 input_layernorm.weight torch.Size([4096])\n",
      "11 post_attention_layernorm.weight torch.Size([4096])\n",
      "12 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "12 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "12 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "12 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "12 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "12 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "12 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "12 input_layernorm.weight torch.Size([4096])\n",
      "12 post_attention_layernorm.weight torch.Size([4096])\n",
      "13 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "13 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "13 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "13 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "13 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "13 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "13 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "13 input_layernorm.weight torch.Size([4096])\n",
      "13 post_attention_layernorm.weight torch.Size([4096])\n",
      "14 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "14 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "14 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "14 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "14 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "14 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "14 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "14 input_layernorm.weight torch.Size([4096])\n",
      "14 post_attention_layernorm.weight torch.Size([4096])\n",
      "15 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "15 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "15 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "15 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "15 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "15 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "15 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "15 input_layernorm.weight torch.Size([4096])\n",
      "15 post_attention_layernorm.weight torch.Size([4096])\n",
      "16 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "16 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "16 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "16 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "16 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "16 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "16 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "16 input_layernorm.weight torch.Size([4096])\n",
      "16 post_attention_layernorm.weight torch.Size([4096])\n",
      "17 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "17 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "17 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "17 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "17 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "17 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "17 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "17 input_layernorm.weight torch.Size([4096])\n",
      "17 post_attention_layernorm.weight torch.Size([4096])\n",
      "18 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "18 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "18 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "18 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "18 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "18 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "18 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "18 input_layernorm.weight torch.Size([4096])\n",
      "18 post_attention_layernorm.weight torch.Size([4096])\n",
      "19 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "19 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "19 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "19 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "19 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "19 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "19 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "19 input_layernorm.weight torch.Size([4096])\n",
      "19 post_attention_layernorm.weight torch.Size([4096])\n",
      "20 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "20 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "20 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "20 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "20 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "20 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "20 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "20 input_layernorm.weight torch.Size([4096])\n",
      "20 post_attention_layernorm.weight torch.Size([4096])\n",
      "21 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "21 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "21 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "21 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "21 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "21 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "21 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "21 input_layernorm.weight torch.Size([4096])\n",
      "21 post_attention_layernorm.weight torch.Size([4096])\n",
      "22 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "22 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "22 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "22 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "22 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "22 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "22 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "22 input_layernorm.weight torch.Size([4096])\n",
      "22 post_attention_layernorm.weight torch.Size([4096])\n",
      "23 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "23 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "23 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "23 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "23 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "23 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "23 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "23 input_layernorm.weight torch.Size([4096])\n",
      "23 post_attention_layernorm.weight torch.Size([4096])\n",
      "24 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "24 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "24 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "24 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "24 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "24 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "24 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "24 input_layernorm.weight torch.Size([4096])\n",
      "24 post_attention_layernorm.weight torch.Size([4096])\n",
      "25 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "25 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "25 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "25 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "25 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "25 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "25 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "25 input_layernorm.weight torch.Size([4096])\n",
      "25 post_attention_layernorm.weight torch.Size([4096])\n",
      "26 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "26 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "26 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "26 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "26 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "26 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "26 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "26 input_layernorm.weight torch.Size([4096])\n",
      "26 post_attention_layernorm.weight torch.Size([4096])\n",
      "27 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "27 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "27 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "27 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "27 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "27 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "27 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "27 input_layernorm.weight torch.Size([4096])\n",
      "27 post_attention_layernorm.weight torch.Size([4096])\n",
      "28 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "28 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "28 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "28 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "28 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "28 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "28 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "28 input_layernorm.weight torch.Size([4096])\n",
      "28 post_attention_layernorm.weight torch.Size([4096])\n",
      "29 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "29 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "29 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "29 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "29 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "29 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "29 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "29 input_layernorm.weight torch.Size([4096])\n",
      "29 post_attention_layernorm.weight torch.Size([4096])\n",
      "30 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "30 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "30 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "30 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "30 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "30 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "30 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "30 input_layernorm.weight torch.Size([4096])\n",
      "30 post_attention_layernorm.weight torch.Size([4096])\n",
      "31 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "31 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "31 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "31 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "31 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "31 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "31 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "31 input_layernorm.weight torch.Size([4096])\n",
      "31 post_attention_layernorm.weight torch.Size([4096])\n",
      "32 self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "32 self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "32 self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "32 self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "32 mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "32 mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "32 mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "32 input_layernorm.weight torch.Size([4096])\n",
      "32 post_attention_layernorm.weight torch.Size([4096])\n",
      "33 norm.weight torch.Size([4096])\n",
      "34 lm_head.weight torch.Size([40014, 4096])\n",
      "34 num_head.fc1.weight torch.Size([16384, 4096])\n",
      "34 num_head.fc1.bias torch.Size([16384])\n",
      "34 num_head.fc2.weight torch.Size([1, 16384])\n",
      "34 num_head.fc2.bias torch.Size([1])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 78.99 GiB memory in use. Of the allocated memory 78.49 GiB is allocated by PyTorch, and 12.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m test_model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(\u001b[38;5;28mlen\u001b[39m(tokenizer))\n\u001b[1;32m     27\u001b[0m test_model\u001b[38;5;241m.\u001b[39mload_state_dict(model_dict)\n\u001b[0;32m---> 28\u001b[0m test_model \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m test_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.9/site-packages/torch/nn/modules/module.py:911\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.9/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.9/site-packages/torch/nn/modules/module.py:911\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    895\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 8.06 MiB is free. Including non-PyTorch memory, this process has 78.99 GiB memory in use. Of the allocated memory 78.49 GiB is allocated by PyTorch, and 12.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "test_model = deepcopy(model)\n",
    "model_dict = test_model.state_dict()\n",
    "ckpt_dict = {}\n",
    "\n",
    "layer0 = torch.load(os.path.join(ckpt_home, \"layer_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict['model.embed_tokens.weight'] = layer0['embed_tokens.weight']#[:32000]\n",
    "show_ckpt('layer0', layer0)\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 1).zfill(2)\n",
    "    layer = torch.load(os.path.join(ckpt_home, f\"layer_{l_index}-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    show_ckpt(l_index, layer)\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        ckpt_dict[f\"model.layers.{l}.{k}\"] = layer[k]\n",
    "\n",
    "layer = torch.load(os.path.join(ckpt_home, \"layer_33-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "show_ckpt(33, layer)\n",
    "ckpt_dict[\"model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(ckpt_home, \"layer_34-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "show_ckpt(34, layer)\n",
    "ckpt_dict[\"lm_head.weight\"] = layer[\"lm_head.weight\"]#[:32000]\n",
    "model_dict.update(ckpt_dict)\n",
    "\n",
    "test_model.resize_token_embeddings(len(tokenizer))\n",
    "test_model.load_state_dict(model_dict)\n",
    "test_model = test_model.cuda()\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0063)\n",
      "tensor(6.8878e-05)\n",
      "tensor(0.0007)\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(torch.abs(model.state_dict()['model.embed_tokens.weight'] - test_model.state_dict()['model.embed_tokens.weight'][:32000].cpu())))\n",
    "print(torch.sum(torch.abs(model.state_dict()['model.layers.10.self_attn.k_proj.weight'] - test_model.state_dict()['model.layers.10.self_attn.k_proj.weight'].cpu())))\n",
    "print(torch.sum(torch.abs(model.state_dict()['lm_head.weight'] - test_model.state_dict()['lm_head.weight'][:32000].cpu())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40014, 4096]) torch.Size([40014, 4096])\n",
      "torch.Size([32000, 4096]) torch.Size([32000, 4096])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40014, 4096]) torch.Size([40014, 4096])\n"
     ]
    }
   ],
   "source": [
    "model_dict.update(ckpt_dict)\n",
    "print(model_dict['model.embed_tokens.weight'].shape, model_dict['lm_head.weight'].shape)\n",
    "emb=model.state_dict()['model.embed_tokens.weight']\n",
    "lm_head=model.state_dict()['lm_head.weight']\n",
    "print(emb.shape, lm_head.shape)\n",
    "model_dict['model.embed_tokens.weight'][:emb.shape[0]] = emb[:]\n",
    "#model_dict['lm_head.weight'][:lm_head.shape[0]] = lm_head[:]\n",
    "# model_dict['model.embed_tokens.weight'] = emb[:]\n",
    "# model_dict['lm_head.weight'] = lm_head[:]\n",
    "\n",
    "#test_model.resize_token_embeddings(emb.shape[0])\n",
    "#test_model.resize_token_embeddings(len(tokenizer))\n",
    "test_model.load_state_dict(model_dict)\n",
    "\n",
    "print(model_dict['model.embed_tokens.weight'].shape, model_dict['lm_head.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32000, 4096])\n",
      "torch.Size([32000, 4096])\n",
      "torch.Size([40014, 4096])\n",
      "torch.Size([40014, 4096])\n",
      "torch.Size([40014, 4096])\n",
      "torch.Size([40014, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict()['model.embed_tokens.weight'].shape)\n",
    "print(model.state_dict()['lm_head.weight'].shape)\n",
    "print(ckpt_dict['model.embed_tokens.weight'].shape)\n",
    "print(ckpt_dict['lm_head.weight'].shape)\n",
    "print(test_model.state_dict()['model.embed_tokens.weight'].shape)\n",
    "print(test_model.state_dict()['lm_head.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <protein> I'm not sure what you're trying to do, but I'm pretty sure you're not trying to do it. I'm not sure what you're trying to do, but I'm pretty sure you're not trying to do it. I'm not sure what you're trying to do, but I'm pretty sure you're not trying to do it. I'm not sure what you're trying to do, but I'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\t\\xe0F%\\xa9t\\xdb\\xbc}\\x1b\\x9f\\x0b\\\\[V\\xe6\"y\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0\\'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c']\n",
      "Bad pipe message: %s [b'\\xacM\\xa8\\xa0\\xee\\xdcB\\x15\\xe6\\x84|Qu\\xd9\\x85(\\xe8\\xac\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0']\n",
      "Bad pipe message: %s [b\"a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\"]\n",
      "Bad pipe message: %s [b'\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00']\n",
      "Bad pipe message: %s [b'\\x956\\xd6\\x9778\\x81\\xd8A\\x90\\x8el\\x838\\xcf\\xe7H\\x1f\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01\\x15']\n",
      "Bad pipe message: %s [b'\\x00\\x02']\n",
      "Bad pipe message: %s [b'\\xdf\\xbf\\xa3,\\x19X\\x87\\xfe\\xbf\\x9d\\xc5\\x9b\\x8f!\\xb1cE!\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x00']\n",
      "Bad pipe message: %s [b'0\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0']\n",
      "Bad pipe message: %s [b'\\x05']\n",
      "Bad pipe message: %s [b\"*;h\\xbb\\xf6\\xd6\\xe7n\\xe9\\xb0UTI@\\x9c\\xfa3D\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\"]\n",
      "Bad pipe message: %s [b'\\xdc\\x02\\xb6\\xaa\\xce\\x9aO=$U\\xb0', b\"\\xe6\\xf7t\\x8d\\xea\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\"]\n",
      "Bad pipe message: %s [b'\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00;\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\r\\x00 \\x00\\x1e\\x06\\x01\\x06\\x02\\x06\\x03\\x05\\x01\\x05\\x02\\x05\\x03\\x04\\x01\\x04\\x02', b'\\x03\\x01\\x03', b'\\x03', b'\\x02', b'\\x03']\n",
      "Bad pipe message: %s [b'9D?\\\\\\xbatb\\x94\\x85\\x11J\\xec\\xc2\\x87\\xe17gW \\xb7\\x84NJ\\xd6=^*\\x9e\\xb3\\xd3E\\x11\\xf2\\x16r\\xf7\\x0f\\x0eW$\\xce>2\\xcbC\\x00\\xb3*\\xca\\x9es\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00']\n",
      "Bad pipe message: %s [b'#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03']\n",
      "Bad pipe message: %s [b'\\x08\\x08\\x08\\t\\x08\\n\\x08', b'\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xee\\xb7K\\xbc8S\\x0c\\xe8\\x93\\xa1\\x9c\\xff\\xf9\\xb0\\xe0\\xd8\\xd0\\xcd\\xa6\\xa1\\xd7\\x1f']\n",
      "Bad pipe message: %s [b'ox\\xa2\\x07\\x03\\xba}\\xdc_\\x92\\xfa\\xc5f+\\x94\\n\\x1a\\x8c\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0', b\"+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\"]\n",
      "Bad pipe message: %s [b'\\x05+\\\\\\xc8\\xf5E\\x1asT\\x11\\xae6\\x9eC\\xae\\x8e\\x97\\xcf\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s']\n",
      "Bad pipe message: %s [b\"\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\", b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'\\xd2\\xd9\\x13\\xb4}\\xfa\\xbc\\x8c\\xe7{\\x9f\\x19\\xc0\\xa5\\x86\\xcak\\xaa\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19']\n",
      "Bad pipe message: %s [b'w\\xc3\\xff\\r|pBQ`)@\\x95Z\\x82SU\\\\>\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000']\n",
      "Bad pipe message: %s [b\"e\\x10\\xc9\\x86\\n\\xe7DwO\\xc9tL\\xce\\x98+\\xc2\\xa74\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\"]\n",
      "Bad pipe message: %s [b\"]\\x82\\xb9\\xa7\\x0f\\xdb\\xe2\\xc2/\\xc7\\x9c\\xfe\\x8f.PI\\xdb\\x07\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\"]\n",
      "Bad pipe message: %s [b\"\\xce\\x8c\\x83\\x17\\nm*\\xdb.\\x08{m\\x16\\xa5-\\x99;\\xef \\xc2b\\xf9\\xb3\\x14\\x04\\xf6#9\\xbc\\xb8C~U\\x13('O\\x98\\xc1\\xae\\xe4\\xe9\\x02sX0\\x98\\xe5\\xbbPz\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\", b'\\x08\\t\\x08\\n\\x08\\x0b\\x08']\n",
      "Bad pipe message: %s [b'\\x05\\x08\\x06']\n",
      "Bad pipe message: %s [b'\\x05\\x01\\x06', b'']\n",
      "Bad pipe message: %s [b'\\x85\\xe5\\xcc0\\xff\\x8a\\xf5Z\\xb1\\x88\\x04\\x0b\\xb0\\xe3\\xcedE^ \\xc5#4\\x80\\x87\\xe7l!\\xea\\xa1\\xeb\\n(\\xdc>\\x93S\\x91g\\xde\\xb3v\\xc6\\x03\\x95\\x1b\\xcbY3\\xe9\\x17\\xa2\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04']\n",
      "Bad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\x93\\xfd\\xf7\\x8c\\x0b\\x91\\xfe\\xa8\\xdfc\\xcd\\xfa\\xfa\\x11$g\\xf6\\xc7\\xdfH\\xb3\\x14']\n",
      "Bad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'']\n",
      "Bad pipe message: %s [b\"\\xe8v\\x1d)\\x8b\\xf1\\xa7R/\\x94\\x83\\xa6\\xc4]\\xb2u\\x9bW\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0\", b'=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00']\n",
      "Bad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\x9d\\xea\\x04\\xcb\\xa8\\xd1\\xc3\\x97\\x96\\xd4\\xd6\\x91\\x1cVv\\t\\xdbtV\\xea\\xba\\xf9']\n",
      "Bad pipe message: %s [b'\\xb2\\xa4\\x01\\xdc\\xf3\\xeb\\x14\\x93\\x0eh\\xc6', b\"*\\xa5\\x9c\\xe9J\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\"]\n",
      "Bad pipe message: %s [b'\\xe7#\\xeb\\x9f\\xc5\\xf4M\\x84', b'K\\xa1y\\x8d\\xf1\\x1a*\\x17\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00']\n",
      "Bad pipe message: %s [b'\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00']\n",
      "Bad pipe message: %s [b'\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06']\n",
      "Bad pipe message: %s [b'T\\xb5\\xf8%+\\x90\\xde\\x0b\\xeaT@Z\\x8fG\\\\\\x18\\xc6\\xec\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00']\n",
      "Bad pipe message: %s [b'9L\\xfc\\xa8\\xe7\\x8f\\x0bi\\x8c\\xb1\\x8a\\xd8&\\xe19\\x90\\x1c\\xcc\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0']\n",
      "Bad pipe message: %s [b'\\x16\\x00\\x13\\x00\\x10\\x00\\r']\n",
      "Bad pipe message: %s [b\"a\\x05\\xa7\\xe08{\\xe4\\xa7\\x04\\x88\\xcd6\\xcaG\\t\\x89\\xcf\\xcb\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\"]\n",
      "Bad pipe message: %s [b'\\xac\\xedD^\\xca\\x04\\xde\\x00\\xb3\\x1aH\\x1c\\xf8K\\x1e\\xfa(\\xc0\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*']\n",
      "Bad pipe message: %s [b\"\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00;\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\"]\n",
      "Bad pipe message: %s [b'\\xb9\\x98t{\\xe8O\\xbf(\\t&\\x1d\\xebq\\xf0\\xd3\\xbc\\xc7\\xc2 *P6\\xa0\\xc1{J\\xd4\\xd9\\x17\\x90\\x85\\xd5n1n\\xed\\t\\x8a\\x8a\\x9cQn8\\xda!\\xc9\\x1a\",\\x1f\\xed\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1']\n",
      "Bad pipe message: %s [b's\\x9ak\\xa8?\\xd2\\xa2\\xc5\\xdcN\\xae%\\x92\\xe2\\xf8\\xb8m[\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0', b\"W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\"]\n",
      "Bad pipe message: %s [b'\\x8e;\\xbc\\xa6g\\x9c\\x9e\\x0cA\\x073^J\\xbeKMwy\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V', b\"\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\"]\n",
      "Bad pipe message: %s [b'\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06']\n",
      "Bad pipe message: %s [b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'\\xc8\\xe0P\\xa9\\xa0\\x9c\\xba$\\xf7\\x03\\x11\\x0b\\xfbzey9\\xcb\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00', b'\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n']\n",
      "Bad pipe message: %s [b'3}\\xc9\\xa9\\xfcX7\\x82\\x8b\\xb1\\x01a\\xb2\\xcf\\xf8Q`\\xef\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00', b'\\x0c\\x00\\x00\\t127.0.0.1']\n",
      "Bad pipe message: %s [b'\\xc5\\xb9Y\\xf2\\xa3\\xf6\\x85^\\x7f', b\"t\\x8b\\xb7|ZV\\xfb\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\"]\n"
     ]
    }
   ],
   "source": [
    "#input_ids = llama_tokenizer('An apple a day', return_tensors=\"pt\").input_ids.cuda()\n",
    "input_ids = tokenizer('An apple a day', return_tensors=\"pt\").input_ids.cuda()\n",
    "input_ids = tokenizer('<protein>', return_tensors=\"pt\").input_ids.cuda()\n",
    "output = test_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=100,\n",
    "    num_return_sequences=4,\n",
    "    return_dict_in_generate=True,\n",
    "    # output_scores=True,\n",
    "    do_sample=False,\n",
    "    # repetition_penalty=1.2,\n",
    "    # num_beams=5,\n",
    "    # max_new_tokens=512,\n",
    "    # num_return_sequences=1,\n",
    "    # return_dict_in_generate=True,\n",
    "    # output_scores=True,\n",
    "    # do_sample=True,\n",
    "    # top_p=0.95,\n",
    "    # repetition_penalty=1.5,\n",
    ")\n",
    "\n",
    "res = tokenizer.decode(output.sequences[0], skip_special_tokens=False)\n",
    "print(res)\n",
    "# for i in range(len(output)):\n",
    "#     s = llama_tokenizer.decode(output[i])\n",
    "#     print(s)\n",
    "#     # print(s, output.sequences_scores[i].item())\n",
    "#     # s = tokenizer.decode(output.sequences[i])\n",
    "#     # print(s, output.sequences_scores[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'> <class 'sfm.data.sci_data.SFMDecTokenizer.SFMDecTokenizer'>\n",
      "['▁An', '▁apple', '▁', '▁a', '▁day', '<0x0A>', 'Yes', '<0x0A>', 'This', '▁is', '<0x0A>', 'an', '▁apple', '<0x0A>']\n",
      "['▁An', '▁apple', '▁', '▁a', '▁day', '<0x0A>', 'Yes', '<0x0A>', 'This', '▁is', '<0x0A>', '<0x0A>', 'an', '▁apple']\n"
     ]
    }
   ],
   "source": [
    "print(type(llama_tokenizer), type(tokenizer))\n",
    "print(llama_tokenizer.tokenize('An apple  a day\\nYes\\nThis is\\nan apple\\n'))\n",
    "print(tokenizer.tokenize('An apple  a day\\nYes\\nThis is\\n\\nan apple\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sfm.data.sci_data.dataset import ProcessedSciDataset\n",
    "from sfm.data.sci_data.SFMDecTokenizer import SFMDecTokenizer\n",
    "from sfm.logging import logger\n",
    "from sfm.models.scigpt.config import (\n",
    "    ScigptConfig,\n",
    "    scigpt_7b_config,\n",
    "    scigpt_13b_config,\n",
    "    scigpt_350m_config,\n",
    "    scigpt_shallow_config,\n",
    "    scigpt_tiny_config,\n",
    ")\n",
    "from sfm.models.scigpt.scigpt import ScigptModel\n",
    "from sfm.pipeline.accelerator.trainer import Trainer\n",
    "from sfm.utils import arg_utils\n",
    "from sfm.utils.cli_utils import cli\n",
    "\n",
    "config_registry = {\n",
    "    \"scigpt_tiny\": scigpt_tiny_config,\n",
    "    \"scigpt_shallow\": scigpt_shallow_config,\n",
    "    \"scigpt_350m\": scigpt_350m_config,\n",
    "    \"scigpt\": scigpt_shallow_config,\n",
    "    \"scigpt_7b\": scigpt_7b_config,\n",
    "    \"scigpt_13b\": scigpt_13b_config,\n",
    "}\n",
    "\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "args = ArgumentParser()\n",
    "args.model_type=\"scigpt_7b_config\"\n",
    "args.vocab_size=40014\n",
    "args.pad_token_id=32000\n",
    "args.max_position_embeddings=4096\n",
    "args.bf16=True\n",
    "args.strategy=\"Pipeline\"\n",
    "args.pipeline_model_parallel_size=1\n",
    "args.pp_partition_layer_name=\"LlamaDecoderLayerPP\"\n",
    "args.load_ckpt=True\n",
    "args.pretrained_ckpt_path=\"/hai1/mfm/ds_dataset/llama2/llama-2-7b/\"\n",
    "args.unfreeze_param_list=\"lm_head.weight,embed_tokens.weight\"\n",
    "args.learnable_cutoff=32000\n",
    "args.infer=True\n",
    "args.llm_model_name_or_path=\"/hai1/mfm/ds_dataset/llama2/llama-2-7b/\"\n",
    "\n",
    "\n",
    "\n",
    "config = arg_utils.from_args(args, ScigptConfig)\n",
    "config = config_registry.get(config.model_type, scigpt_tiny_config)(config)\n",
    "config.llm_model_name_or_path=\"/hai1/mfm/ds_dataset/llama2/llama-2-7b/\"\n",
    "\n",
    "model = ScigptModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'SFMDecTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40014\n",
      "['<protein>', '▁A', 'AB', 'B', 'CC', 'DD', '</protein>']\n",
      "['▁An', '▁apple', '▁', '▁a', '▁day', '<0x0A>', 'Yes', '<0x0A>', 'This', '▁is', '<0x0A>', '<0x0A>', 'an', '▁app', 'len', 'ade']\n"
     ]
    }
   ],
   "source": [
    "tokenizer_home = '/hai1/mfm/ds_dataset/llama2/llama-2-7b'\n",
    "tokenizer = SFMDecTokenizer.from_pretrained(\n",
    "    tokenizer_home,\n",
    "    prot_spm_path='/blob/shufxi/data/scigpt/ur50bpe/bpe',\n",
    "    dna_spm_path='/blob/shufxi/data/scigpt/dnabpe/bpe',\n",
    "    rna_spm_path='/blob/shufxi/data/scigpt/rnabpe/bpe',\n",
    ")\n",
    "print(len(tokenizer))\n",
    "print(tokenizer.tokenize('<protein>AABBCCDD</protein>'))\n",
    "print(tokenizer.tokenize('An apple  a day\\nYes\\nThis is\\n\\nan applenade\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ScigptModel' object has no attribute 'decoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn apple a day\u001b[39m\u001b[38;5;124m'\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#input_ids = tokenizer('<protein>AA', return_tensors=\"pt\").input_ids.cuda()\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m      7\u001b[0m     input_ids,\n\u001b[1;32m      8\u001b[0m     num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m      9\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     10\u001b[0m     num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     11\u001b[0m     return_dict_in_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m res \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output\u001b[38;5;241m.\u001b[39msequences[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(res)\n",
      "File \u001b[0;32m/anaconda/envs/sfm/lib/python3.9/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ScigptModel' object has no attribute 'decoder'"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model=model.cuda()\n",
    "\n",
    "input_ids = tokenizer('An apple a day', return_tensors=\"pt\").input_ids.cuda()\n",
    "#input_ids = tokenizer('<protein>AA', return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.decoder.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=100,\n",
    "    num_return_sequences=4,\n",
    "    return_dict_in_generate=True,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "res = tokenizer.decode(output.sequences[0], skip_special_tokens=False)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
