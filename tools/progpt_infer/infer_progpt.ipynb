{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import transformers\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sfm.models.progpt.progpt import ProGPTModel\n",
    "from sfm.models.progpt.progpt_config import ProGPTConfig\n",
    "from sfm.models.pfm.pfm_config import PFMConfig\n",
    "from sfm.data.sci_data.SFMDecTokenizer import SFMDecTokenizer\n",
    "from sfm.utils import arg_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_and_tokenizer():\n",
    "    parser = ArgumentParser()\n",
    "    cfg_classes = [PFMConfig, ProGPTConfig]\n",
    "    parser = arg_utils.add_dataclass_to_parser(cfg_classes, parser)\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.llm_model_name_or_path = \"/fastdata/peiran/scigpt/ckpt/stageB.prot/global_step224655\"\n",
    "    args.tokenizer_path = \"/fastdata/peiran/scigpt\"\n",
    "    args.save_dir = '/fastdata/peiran/nlm/checkpoints/stageB/global_step12386/'\n",
    "    args.load_ckpt = False\n",
    "    args.strategy = \"DDP\"\n",
    "    args.encoder_layers = 33\n",
    "    args.encoder_embed_dim = 1280\n",
    "    args.encoder_ffn_embed_dim = 5120\n",
    "    args.encoder_attention_heads = 20\n",
    "\n",
    "    tokenizer = SFMDecTokenizer.from_pretrained(\n",
    "        args.llm_model_name_or_path,\n",
    "        prot_spm_path=os.path.join(args.tokenizer_path, \"ur50bpe/bpe\"),\n",
    "        dna_spm_path=os.path.join(args.tokenizer_path, \"dnabpe/bpe\"),\n",
    "        rna_spm_path=os.path.join(args.tokenizer_path, \"rnabpe/bpe\"),\n",
    "    )\n",
    "    args.vocab_size = len(tokenizer)  # now we have new tokens\n",
    "    args.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return args, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args, tokenizer = get_args_and_tokenizer()\n",
    "\n",
    "# with init_empty_weights():\n",
    "model = ProGPTModel(args, len(tokenizer))\n",
    "# print(model.state_dict().keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dict = {}\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer0.items():\n",
    "    new_k = \"pfm_encoder.\" + k\n",
    "    ckpt_dict[new_k] = v\n",
    "\n",
    "layer1 = torch.load(os.path.join(args.save_dir, \"layer_01-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict['decoder.model.embed_tokens.weight'] = layer1['embed_tokens.weight']\n",
    "\n",
    "layer2 = torch.load(os.path.join(args.save_dir, \"layer_02-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer2.items():\n",
    "    ckpt_dict[k] = v\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 3).zfill(2)\n",
    "    layer = torch.load(os.path.join(args.save_dir, f\"layer_{l_index}-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        ckpt_dict[f\"decoder.model.layers.{l}.{k}\"] = layer[k]\n",
    "    del layer\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_35-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_36-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "\n",
    "# model_dict.update(ckpt_dict)\n",
    "model.load_state_dict(model_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scigpt_vacab = {'L': 33874, 'A': 33875, 'G': 33878, 'V': 33877, 'S': 33876, 'E': 33879, 'R': 33880, 'T': 33881, 'I': 33882, 'D': 33884, 'P': 33886, 'K': 33883, 'Q': 33885, 'N': 33887, 'F': 33888, 'Y': 33890, 'M': 33873, 'H': 33889, 'W': 33891, 'C': 33892, 'X': 34276, 'B': 37965, 'U': 37967, 'Z': 37966, 'O': 0}\n",
    "\n",
    "vocab = {'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7, 'S': 8, 'E': 9, 'R': 10, 'T': 11, 'I': 12, 'D': 13, 'P': 14, 'K': 15, 'Q': 16, 'N': 17, 'F': 18, 'Y': 19, 'M': 20, 'H': 21, 'W': 22, 'C': 23, 'X': 24, 'B': 25, 'U': 26, 'Z': 27, 'O': 28, '.': 29, '-': 30, '<mask>': 31}\n",
    "\n",
    "def protein_process(protein):\n",
    "    protein_id = [vocab[tok] for tok in protein]\n",
    "    protein_bpe_id = [scigpt_vacab[tok] for tok in protein]\n",
    "\n",
    "    return protein_id, protein_bpe_id\n",
    "\n",
    "def process(text):\n",
    "    # find the part of protein seq that surrounded by <protein> and </protein> in text\n",
    "    protein = []\n",
    "    res = []\n",
    "    text1 = text.split(\"<protein>\")\n",
    "    res.append(text1[0])\n",
    "    for i in range(1, len(text1)):\n",
    "        text2 = text1[i].split(\"</protein>\")\n",
    "        protein.append(text2[0])\n",
    "        res.append(text2[1])\n",
    "\n",
    "    return res, protein\n",
    "\n",
    "def tokenize(text):\n",
    "    # split text with <protein> and </protein>\n",
    "    text_list, protein = process(text)\n",
    "    protein_id_list = []\n",
    "    protein_bpe_id_list = []\n",
    "\n",
    "    if len(protein) == 0:\n",
    "        return tokenizer.encode(text), protein_id_list, protein_bpe_id_list\n",
    "    else:\n",
    "        for p in protein:\n",
    "            protein_id, protein_bpe_id = protein_process(p)\n",
    "            protein_id_list.append([0] + protein_id + [2])\n",
    "            protein_bpe_id_list.append(protein_bpe_id)\n",
    "\n",
    "    input_ids = []\n",
    "    for i in range(len(text_list)):\n",
    "        if i == 0:\n",
    "            input_ids.extend(tokenizer.encode(text_list[i] + \" <protein>\"))\n",
    "        elif i != len(text_list) - 1:\n",
    "            input_ids.append(-1)\n",
    "            input_ids.extend(tokenizer.encode(\"</protein> \" + text_list[i] + \" <protein>\")[1:])\n",
    "        else:\n",
    "            input_ids.append(-1)\n",
    "            input_ids.extend(tokenizer.encode(\"</protein> \" + text_list[i])[1:])\n",
    "\n",
    "    return input_ids, protein_id_list, protein_bpe_id_list\n",
    "\n",
    "\n",
    "def collator(input_ids, protein_id_list, protein_bpe_id_list, device):\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int64)\n",
    "    for i in range(len(protein_bpe_id_list)):\n",
    "        protein_bpe_id_list[i] = torch.tensor(protein_bpe_id_list[i], dtype=torch.int64)\n",
    "\n",
    "    new_input_ids = []\n",
    "    mol_pos = torch.nonzero(input_ids < 0).squeeze(-1)\n",
    "    mol_pos = torch.cat(\n",
    "        [torch.tensor([0]), mol_pos, torch.tensor([len(input_ids)])]\n",
    "    )\n",
    "\n",
    "    for i in range(mol_pos.size(0) - 1):\n",
    "        if i == 0:\n",
    "            new_input_ids.extend(input_ids[mol_pos[i] : mol_pos[i + 1]])\n",
    "        else:\n",
    "            new_input_ids.extend(input_ids[mol_pos[i] + 1 : mol_pos[i + 1]])\n",
    "\n",
    "        if i < len(mol_pos) - 2:\n",
    "            len_protein = len(protein_id_list[i])\n",
    "            mol_idx = input_ids[mol_pos[i + 1]]\n",
    "            if len_protein > 1:\n",
    "                new_input_ids.extend(torch.ones([len_protein]) * mol_idx)\n",
    "\n",
    "\n",
    "    input_ids = torch.tensor(new_input_ids).to(dtype=torch.int64)\n",
    "    if len(protein_id_list) == 0:\n",
    "        protein = torch.tensor([0, 2]).to(dtype=torch.int64).unsqueeze(0)\n",
    "    else:\n",
    "        protein = torch.tensor(protein_id_list[0]).to(dtype=torch.int64).unsqueeze(0)\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids.to(device),\n",
    "        proteins=protein.long().to(device),\n",
    "        llm_mask=input_ids.ne(tokenizer.pad_token_id).to(device),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "text = \"Describe the <protein>AAAGSGAGU</protein> .\"\n",
    "input_ids, protein_id_list, protein_bpe_id_list = tokenize(text)\n",
    "batched_data = collator(input_ids, protein_id_list, protein_bpe_id_list, device)\n",
    "# residue_seq = batched_data[\"proteins\"]\n",
    "# print(batched_data[\"input_ids\"])\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "model.generate(batched_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca_mfm_pp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
