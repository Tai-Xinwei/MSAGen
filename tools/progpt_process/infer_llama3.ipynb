{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fastdata/peiran/miniconda3/envs/sfm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 18:59:15,320] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[\u001b[32m2024-05-09 18:59:15.682\u001b[0m][\u001b[36mINFO\u001b[0m]: apex is installed, using FusedAdam with fp16 optimizer states\n",
      "[\u001b[32m2024-05-09 18:59:16.236\u001b[0m][\u001b[36mINFO\u001b[0m]: Using TEColumnParallelLinear and TERowParallelLinear in tensor parallel\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "sys.path.extend([\".\", \"..\", \"../..\"])\n",
    "\n",
    "import transformers\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sfm.models.scigpt.scigpt import ScigptModel\n",
    "from sfm.models.scigpt.config import ScigptConfig\n",
    "from sfm.utils import arg_utils\n",
    "from transformers import LlamaForCausalLM\n",
    "from transformers.models.llama.configuration_llama import LlamaConfig\n",
    "from sfm.utils.science_tokens import SCIENCE_TAG_TOKENS\n",
    "\n",
    "from sfm.models.llama2.llama_modules_3dmp_te import TELlamaModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'> 128384\n"
     ]
    }
   ],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def get_args_and_tokenizer(use_llama=False):\n",
    "    parser = ArgumentParser()\n",
    "    cfg_classes = [ScigptConfig]\n",
    "    parser = arg_utils.add_dataclass_to_parser(cfg_classes, parser)\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.load_ckpt = False\n",
    "    args.strategy = \"DDP\"\n",
    "    args.encoder_layers = 33\n",
    "    args.encoder_embed_dim = 1280\n",
    "    args.encoder_ffn_embed_dim = 5120\n",
    "    args.encoder_attention_heads = 20\n",
    "    args.infer = True\n",
    "    args.bf16 = True\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/data/peiran/blob/hai1data/sfm/llama/Meta-Llama-3-8B/original\")\n",
    "    args.save_dir = \"/data/peiran/expresult/llama3_8B_stageB/global_step16999\"\n",
    "    args.llm_model_name_or_path = \"/data/peiran/blob/hai1data/sfm/llama/Meta-Llama-3-8B/original\"\n",
    "\n",
    "    special_tokens_dict = dict()\n",
    "    if tokenizer.pad_token is None:\n",
    "        special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "    if tokenizer.eos_token is None:\n",
    "        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "    if tokenizer.bos_token is None:\n",
    "        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "    if tokenizer.unk_token is None:\n",
    "        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "    special_tokens_dict[\"additional_special_tokens\"] = SCIENCE_TAG_TOKENS\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        \n",
    "\n",
    "    return args, tokenizer\n",
    "\n",
    "args, tokenizer = get_args_and_tokenizer()\n",
    "print(type(tokenizer), len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n"
     ]
    }
   ],
   "source": [
    "ckpt_dict = {}\n",
    "llama_config = LlamaConfig.from_pretrained(\"/data/peiran/blob/hai1data/sfm/llama/Meta-Llama-3-8B/original\")\n",
    "model = TELlamaModel(args, llama_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      "), TELlamaDecoderLayer(\n",
      "  (self_attention): MultiheadAttention(\n",
      "    (layernorm_qkv): LayerNormLinear()\n",
      "    (core_attention): DotProductAttention(\n",
      "      (flash_attention): FlashAttention()\n",
      "      (fused_attention): FusedAttention()\n",
      "      (unfused_attention): UnfusedDotProductAttention(\n",
      "        (scale_mask_softmax): FusedScaleMaskSoftmax()\n",
      "        (attention_dropout): Dropout(p=0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (proj): Linear()\n",
      "  )\n",
      "  (layernorm_mlp): LayerNormMLP()\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['dummy', 'word_embeddings.weight', 'norm.weight', 'lm_head.weight'])\n"
     ]
    }
   ],
   "source": [
    "model_dict = model.state_dict()\n",
    "print(model_dict.keys())\n",
    "\n",
    "# model_dict[\"model.embed_tokens.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt_dict: dict_keys(['model.embed_tokens.weight', 'model.layers.0.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.0.self_attention.layernorm_qkv.query_weight', 'model.layers.0.self_attention.layernorm_qkv.key_weight', 'model.layers.0.self_attention.layernorm_qkv.value_weight', 'model.layers.0.self_attention.proj.weight', 'model.layers.0.layernorm_mlp.layer_norm_weight', 'model.layers.0.layernorm_mlp.fc1_weight', 'model.layers.0.layernorm_mlp.fc2_weight', 'model.layers.1.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.1.self_attention.layernorm_qkv.query_weight', 'model.layers.1.self_attention.layernorm_qkv.key_weight', 'model.layers.1.self_attention.layernorm_qkv.value_weight', 'model.layers.1.self_attention.proj.weight', 'model.layers.1.layernorm_mlp.layer_norm_weight', 'model.layers.1.layernorm_mlp.fc1_weight', 'model.layers.1.layernorm_mlp.fc2_weight', 'model.layers.2.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.2.self_attention.layernorm_qkv.query_weight', 'model.layers.2.self_attention.layernorm_qkv.key_weight', 'model.layers.2.self_attention.layernorm_qkv.value_weight', 'model.layers.2.self_attention.proj.weight', 'model.layers.2.layernorm_mlp.layer_norm_weight', 'model.layers.2.layernorm_mlp.fc1_weight', 'model.layers.2.layernorm_mlp.fc2_weight', 'model.layers.3.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.3.self_attention.layernorm_qkv.query_weight', 'model.layers.3.self_attention.layernorm_qkv.key_weight', 'model.layers.3.self_attention.layernorm_qkv.value_weight', 'model.layers.3.self_attention.proj.weight', 'model.layers.3.layernorm_mlp.layer_norm_weight', 'model.layers.3.layernorm_mlp.fc1_weight', 'model.layers.3.layernorm_mlp.fc2_weight', 'model.layers.4.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.4.self_attention.layernorm_qkv.query_weight', 'model.layers.4.self_attention.layernorm_qkv.key_weight', 'model.layers.4.self_attention.layernorm_qkv.value_weight', 'model.layers.4.self_attention.proj.weight', 'model.layers.4.layernorm_mlp.layer_norm_weight', 'model.layers.4.layernorm_mlp.fc1_weight', 'model.layers.4.layernorm_mlp.fc2_weight', 'model.layers.5.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.5.self_attention.layernorm_qkv.query_weight', 'model.layers.5.self_attention.layernorm_qkv.key_weight', 'model.layers.5.self_attention.layernorm_qkv.value_weight', 'model.layers.5.self_attention.proj.weight', 'model.layers.5.layernorm_mlp.layer_norm_weight', 'model.layers.5.layernorm_mlp.fc1_weight', 'model.layers.5.layernorm_mlp.fc2_weight', 'model.layers.6.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.6.self_attention.layernorm_qkv.query_weight', 'model.layers.6.self_attention.layernorm_qkv.key_weight', 'model.layers.6.self_attention.layernorm_qkv.value_weight', 'model.layers.6.self_attention.proj.weight', 'model.layers.6.layernorm_mlp.layer_norm_weight', 'model.layers.6.layernorm_mlp.fc1_weight', 'model.layers.6.layernorm_mlp.fc2_weight', 'model.layers.7.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.7.self_attention.layernorm_qkv.query_weight', 'model.layers.7.self_attention.layernorm_qkv.key_weight', 'model.layers.7.self_attention.layernorm_qkv.value_weight', 'model.layers.7.self_attention.proj.weight', 'model.layers.7.layernorm_mlp.layer_norm_weight', 'model.layers.7.layernorm_mlp.fc1_weight', 'model.layers.7.layernorm_mlp.fc2_weight', 'model.layers.8.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.8.self_attention.layernorm_qkv.query_weight', 'model.layers.8.self_attention.layernorm_qkv.key_weight', 'model.layers.8.self_attention.layernorm_qkv.value_weight', 'model.layers.8.self_attention.proj.weight', 'model.layers.8.layernorm_mlp.layer_norm_weight', 'model.layers.8.layernorm_mlp.fc1_weight', 'model.layers.8.layernorm_mlp.fc2_weight', 'model.layers.9.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.9.self_attention.layernorm_qkv.query_weight', 'model.layers.9.self_attention.layernorm_qkv.key_weight', 'model.layers.9.self_attention.layernorm_qkv.value_weight', 'model.layers.9.self_attention.proj.weight', 'model.layers.9.layernorm_mlp.layer_norm_weight', 'model.layers.9.layernorm_mlp.fc1_weight', 'model.layers.9.layernorm_mlp.fc2_weight', 'model.layers.10.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.10.self_attention.layernorm_qkv.query_weight', 'model.layers.10.self_attention.layernorm_qkv.key_weight', 'model.layers.10.self_attention.layernorm_qkv.value_weight', 'model.layers.10.self_attention.proj.weight', 'model.layers.10.layernorm_mlp.layer_norm_weight', 'model.layers.10.layernorm_mlp.fc1_weight', 'model.layers.10.layernorm_mlp.fc2_weight', 'model.layers.11.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.11.self_attention.layernorm_qkv.query_weight', 'model.layers.11.self_attention.layernorm_qkv.key_weight', 'model.layers.11.self_attention.layernorm_qkv.value_weight', 'model.layers.11.self_attention.proj.weight', 'model.layers.11.layernorm_mlp.layer_norm_weight', 'model.layers.11.layernorm_mlp.fc1_weight', 'model.layers.11.layernorm_mlp.fc2_weight', 'model.layers.12.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.12.self_attention.layernorm_qkv.query_weight', 'model.layers.12.self_attention.layernorm_qkv.key_weight', 'model.layers.12.self_attention.layernorm_qkv.value_weight', 'model.layers.12.self_attention.proj.weight', 'model.layers.12.layernorm_mlp.layer_norm_weight', 'model.layers.12.layernorm_mlp.fc1_weight', 'model.layers.12.layernorm_mlp.fc2_weight', 'model.layers.13.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.13.self_attention.layernorm_qkv.query_weight', 'model.layers.13.self_attention.layernorm_qkv.key_weight', 'model.layers.13.self_attention.layernorm_qkv.value_weight', 'model.layers.13.self_attention.proj.weight', 'model.layers.13.layernorm_mlp.layer_norm_weight', 'model.layers.13.layernorm_mlp.fc1_weight', 'model.layers.13.layernorm_mlp.fc2_weight', 'model.layers.14.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.14.self_attention.layernorm_qkv.query_weight', 'model.layers.14.self_attention.layernorm_qkv.key_weight', 'model.layers.14.self_attention.layernorm_qkv.value_weight', 'model.layers.14.self_attention.proj.weight', 'model.layers.14.layernorm_mlp.layer_norm_weight', 'model.layers.14.layernorm_mlp.fc1_weight', 'model.layers.14.layernorm_mlp.fc2_weight', 'model.layers.15.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.15.self_attention.layernorm_qkv.query_weight', 'model.layers.15.self_attention.layernorm_qkv.key_weight', 'model.layers.15.self_attention.layernorm_qkv.value_weight', 'model.layers.15.self_attention.proj.weight', 'model.layers.15.layernorm_mlp.layer_norm_weight', 'model.layers.15.layernorm_mlp.fc1_weight', 'model.layers.15.layernorm_mlp.fc2_weight', 'model.layers.16.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.16.self_attention.layernorm_qkv.query_weight', 'model.layers.16.self_attention.layernorm_qkv.key_weight', 'model.layers.16.self_attention.layernorm_qkv.value_weight', 'model.layers.16.self_attention.proj.weight', 'model.layers.16.layernorm_mlp.layer_norm_weight', 'model.layers.16.layernorm_mlp.fc1_weight', 'model.layers.16.layernorm_mlp.fc2_weight', 'model.layers.17.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.17.self_attention.layernorm_qkv.query_weight', 'model.layers.17.self_attention.layernorm_qkv.key_weight', 'model.layers.17.self_attention.layernorm_qkv.value_weight', 'model.layers.17.self_attention.proj.weight', 'model.layers.17.layernorm_mlp.layer_norm_weight', 'model.layers.17.layernorm_mlp.fc1_weight', 'model.layers.17.layernorm_mlp.fc2_weight', 'model.layers.18.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.18.self_attention.layernorm_qkv.query_weight', 'model.layers.18.self_attention.layernorm_qkv.key_weight', 'model.layers.18.self_attention.layernorm_qkv.value_weight', 'model.layers.18.self_attention.proj.weight', 'model.layers.18.layernorm_mlp.layer_norm_weight', 'model.layers.18.layernorm_mlp.fc1_weight', 'model.layers.18.layernorm_mlp.fc2_weight', 'model.layers.19.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.19.self_attention.layernorm_qkv.query_weight', 'model.layers.19.self_attention.layernorm_qkv.key_weight', 'model.layers.19.self_attention.layernorm_qkv.value_weight', 'model.layers.19.self_attention.proj.weight', 'model.layers.19.layernorm_mlp.layer_norm_weight', 'model.layers.19.layernorm_mlp.fc1_weight', 'model.layers.19.layernorm_mlp.fc2_weight', 'model.layers.20.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.20.self_attention.layernorm_qkv.query_weight', 'model.layers.20.self_attention.layernorm_qkv.key_weight', 'model.layers.20.self_attention.layernorm_qkv.value_weight', 'model.layers.20.self_attention.proj.weight', 'model.layers.20.layernorm_mlp.layer_norm_weight', 'model.layers.20.layernorm_mlp.fc1_weight', 'model.layers.20.layernorm_mlp.fc2_weight', 'model.layers.21.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.21.self_attention.layernorm_qkv.query_weight', 'model.layers.21.self_attention.layernorm_qkv.key_weight', 'model.layers.21.self_attention.layernorm_qkv.value_weight', 'model.layers.21.self_attention.proj.weight', 'model.layers.21.layernorm_mlp.layer_norm_weight', 'model.layers.21.layernorm_mlp.fc1_weight', 'model.layers.21.layernorm_mlp.fc2_weight', 'model.layers.22.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.22.self_attention.layernorm_qkv.query_weight', 'model.layers.22.self_attention.layernorm_qkv.key_weight', 'model.layers.22.self_attention.layernorm_qkv.value_weight', 'model.layers.22.self_attention.proj.weight', 'model.layers.22.layernorm_mlp.layer_norm_weight', 'model.layers.22.layernorm_mlp.fc1_weight', 'model.layers.22.layernorm_mlp.fc2_weight', 'model.layers.23.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.23.self_attention.layernorm_qkv.query_weight', 'model.layers.23.self_attention.layernorm_qkv.key_weight', 'model.layers.23.self_attention.layernorm_qkv.value_weight', 'model.layers.23.self_attention.proj.weight', 'model.layers.23.layernorm_mlp.layer_norm_weight', 'model.layers.23.layernorm_mlp.fc1_weight', 'model.layers.23.layernorm_mlp.fc2_weight', 'model.layers.24.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.24.self_attention.layernorm_qkv.query_weight', 'model.layers.24.self_attention.layernorm_qkv.key_weight', 'model.layers.24.self_attention.layernorm_qkv.value_weight', 'model.layers.24.self_attention.proj.weight', 'model.layers.24.layernorm_mlp.layer_norm_weight', 'model.layers.24.layernorm_mlp.fc1_weight', 'model.layers.24.layernorm_mlp.fc2_weight', 'model.layers.25.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.25.self_attention.layernorm_qkv.query_weight', 'model.layers.25.self_attention.layernorm_qkv.key_weight', 'model.layers.25.self_attention.layernorm_qkv.value_weight', 'model.layers.25.self_attention.proj.weight', 'model.layers.25.layernorm_mlp.layer_norm_weight', 'model.layers.25.layernorm_mlp.fc1_weight', 'model.layers.25.layernorm_mlp.fc2_weight', 'model.layers.26.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.26.self_attention.layernorm_qkv.query_weight', 'model.layers.26.self_attention.layernorm_qkv.key_weight', 'model.layers.26.self_attention.layernorm_qkv.value_weight', 'model.layers.26.self_attention.proj.weight', 'model.layers.26.layernorm_mlp.layer_norm_weight', 'model.layers.26.layernorm_mlp.fc1_weight', 'model.layers.26.layernorm_mlp.fc2_weight', 'model.layers.27.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.27.self_attention.layernorm_qkv.query_weight', 'model.layers.27.self_attention.layernorm_qkv.key_weight', 'model.layers.27.self_attention.layernorm_qkv.value_weight', 'model.layers.27.self_attention.proj.weight', 'model.layers.27.layernorm_mlp.layer_norm_weight', 'model.layers.27.layernorm_mlp.fc1_weight', 'model.layers.27.layernorm_mlp.fc2_weight', 'model.layers.28.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.28.self_attention.layernorm_qkv.query_weight', 'model.layers.28.self_attention.layernorm_qkv.key_weight', 'model.layers.28.self_attention.layernorm_qkv.value_weight', 'model.layers.28.self_attention.proj.weight', 'model.layers.28.layernorm_mlp.layer_norm_weight', 'model.layers.28.layernorm_mlp.fc1_weight', 'model.layers.28.layernorm_mlp.fc2_weight', 'model.layers.29.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.29.self_attention.layernorm_qkv.query_weight', 'model.layers.29.self_attention.layernorm_qkv.key_weight', 'model.layers.29.self_attention.layernorm_qkv.value_weight', 'model.layers.29.self_attention.proj.weight', 'model.layers.29.layernorm_mlp.layer_norm_weight', 'model.layers.29.layernorm_mlp.fc1_weight', 'model.layers.29.layernorm_mlp.fc2_weight', 'model.layers.30.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.30.self_attention.layernorm_qkv.query_weight', 'model.layers.30.self_attention.layernorm_qkv.key_weight', 'model.layers.30.self_attention.layernorm_qkv.value_weight', 'model.layers.30.self_attention.proj.weight', 'model.layers.30.layernorm_mlp.layer_norm_weight', 'model.layers.30.layernorm_mlp.fc1_weight', 'model.layers.30.layernorm_mlp.fc2_weight', 'model.layers.31.self_attention.layernorm_qkv.layer_norm_weight', 'model.layers.31.self_attention.layernorm_qkv.query_weight', 'model.layers.31.self_attention.layernorm_qkv.key_weight', 'model.layers.31.self_attention.layernorm_qkv.value_weight', 'model.layers.31.self_attention.proj.weight', 'model.layers.31.layernorm_mlp.layer_norm_weight', 'model.layers.31.layernorm_mlp.fc1_weight', 'model.layers.31.layernorm_mlp.fc2_weight', 'model.norm.weight', 'model.dummy.weight', 'model.dummy.bias', 'model.lm_head.weight', 'model.num_head.fc1.weight', 'model.num_head.fc1.bias', 'model.num_head.fc2.weight', 'model.num_head.fc2.bias'])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LlamaForCausalLM:\n\tUnexpected key(s) in state_dict: \"model.word_embeddings.weight\", \"model.dummy.weight\", \"model.dummy.bias\", \"model.lm_head.weight\", \"model.num_head.fc1.weight\", \"model.num_head.fc1.bias\", \"model.num_head.fc2.weight\", \"model.num_head.fc2.bias\", \"model.layers.0.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.0.self_attention.layernorm_qkv.query_weight\", \"model.layers.0.self_attention.layernorm_qkv.key_weight\", \"model.layers.0.self_attention.layernorm_qkv.value_weight\", \"model.layers.0.self_attention.proj.weight\", \"model.layers.0.layernorm_mlp.layer_norm_weight\", \"model.layers.0.layernorm_mlp.fc1_weight\", \"model.layers.0.layernorm_mlp.fc2_weight\", \"model.layers.1.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.1.self_attention.layernorm_qkv.query_weight\", \"model.layers.1.self_attention.layernorm_qkv.key_weight\", \"model.layers.1.self_attention.layernorm_qkv.value_weight\", \"model.layers.1.self_attention.proj.weight\", \"model.layers.1.layernorm_mlp.layer_norm_weight\", \"model.layers.1.layernorm_mlp.fc1_weight\", \"model.layers.1.layernorm_mlp.fc2_weight\", \"model.layers.2.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.2.self_attention.layernorm_qkv.query_weight\", \"model.layers.2.self_attention.layernorm_qkv.key_weight\", \"model.layers.2.self_attention.layernorm_qkv.value_weight\", \"model.layers.2.self_attention.proj.weight\", \"model.layers.2.layernorm_mlp.layer_norm_weight\", \"model.layers.2.layernorm_mlp.fc1_weight\", \"model.layers.2.layernorm_mlp.fc2_weight\", \"model.layers.3.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.3.self_attention.layernorm_qkv.query_weight\", \"model.layers.3.self_attention.layernorm_qkv.key_weight\", \"model.layers.3.self_attention.layernorm_qkv.value_weight\", \"model.layers.3.self_attention.proj.weight\", \"model.layers.3.layernorm_mlp.layer_norm_weight\", \"model.layers.3.layernorm_mlp.fc1_weight\", \"model.layers.3.layernorm_mlp.fc2_weight\", \"model.layers.4.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.4.self_attention.layernorm_qkv.query_weight\", \"model.layers.4.self_attention.layernorm_qkv.key_weight\", \"model.layers.4.self_attention.layernorm_qkv.value_weight\", \"model.layers.4.self_attention.proj.weight\", \"model.layers.4.layernorm_mlp.layer_norm_weight\", \"model.layers.4.layernorm_mlp.fc1_weight\", \"model.layers.4.layernorm_mlp.fc2_weight\", \"model.layers.5.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.5.self_attention.layernorm_qkv.query_weight\", \"model.layers.5.self_attention.layernorm_qkv.key_weight\", \"model.layers.5.self_attention.layernorm_qkv.value_weight\", \"model.layers.5.self_attention.proj.weight\", \"model.layers.5.layernorm_mlp.layer_norm_weight\", \"model.layers.5.layernorm_mlp.fc1_weight\", \"model.layers.5.layernorm_mlp.fc2_weight\", \"model.layers.6.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.6.self_attention.layernorm_qkv.query_weight\", \"model.layers.6.self_attention.layernorm_qkv.key_weight\", \"model.layers.6.self_attention.layernorm_qkv.value_weight\", \"model.layers.6.self_attention.proj.weight\", \"model.layers.6.layernorm_mlp.layer_norm_weight\", \"model.layers.6.layernorm_mlp.fc1_weight\", \"model.layers.6.layernorm_mlp.fc2_weight\", \"model.layers.7.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.7.self_attention.layernorm_qkv.query_weight\", \"model.layers.7.self_attention.layernorm_qkv.key_weight\", \"model.layers.7.self_attention.layernorm_qkv.value_weight\", \"model.layers.7.self_attention.proj.weight\", \"model.layers.7.layernorm_mlp.layer_norm_weight\", \"model.layers.7.layernorm_mlp.fc1_weight\", \"model.layers.7.layernorm_mlp.fc2_weight\", \"model.layers.8.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.8.self_attention.layernorm_qkv.query_weight\", \"model.layers.8.self_attention.layernorm_qkv.key_weight\", \"model.layers.8.self_attention.layernorm_qkv.value_weight\", \"model.layers.8.self_attention.proj.weight\", \"model.layers.8.layernorm_mlp.layer_norm_weight\", \"model.layers.8.layernorm_mlp.fc1_weight\", \"model.layers.8.layernorm_mlp.fc2_weight\", \"model.layers.9.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.9.self_attention.layernorm_qkv.query_weight\", \"model.layers.9.self_attention.layernorm_qkv.key_weight\", \"model.layers.9.self_attention.layernorm_qkv.value_weight\", \"model.layers.9.self_attention.proj.weight\", \"model.layers.9.layernorm_mlp.layer_norm_weight\", \"model.layers.9.layernorm_mlp.fc1_weight\", \"model.layers.9.layernorm_mlp.fc2_weight\", \"model.layers.10.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.10.self_attention.layernorm_qkv.query_weight\", \"model.layers.10.self_attention.layernorm_qkv.key_weight\", \"model.layers.10.self_attention.layernorm_qkv.value_weight\", \"model.layers.10.self_attention.proj.weight\", \"model.layers.10.layernorm_mlp.layer_norm_weight\", \"model.layers.10.layernorm_mlp.fc1_weight\", \"model.layers.10.layernorm_mlp.fc2_weight\", \"model.layers.11.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.11.self_attention.layernorm_qkv.query_weight\", \"model.layers.11.self_attention.layernorm_qkv.key_weight\", \"model.layers.11.self_attention.layernorm_qkv.value_weight\", \"model.layers.11.self_attention.proj.weight\", \"model.layers.11.layernorm_mlp.layer_norm_weight\", \"model.layers.11.layernorm_mlp.fc1_weight\", \"model.layers.11.layernorm_mlp.fc2_weight\", \"model.layers.12.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.12.self_attention.layernorm_qkv.query_weight\", \"model.layers.12.self_attention.layernorm_qkv.key_weight\", \"model.layers.12.self_attention.layernorm_qkv.value_weight\", \"model.layers.12.self_attention.proj.weight\", \"model.layers.12.layernorm_mlp.layer_norm_weight\", \"model.layers.12.layernorm_mlp.fc1_weight\", \"model.layers.12.layernorm_mlp.fc2_weight\", \"model.layers.13.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.13.self_attention.layernorm_qkv.query_weight\", \"model.layers.13.self_attention.layernorm_qkv.key_weight\", \"model.layers.13.self_attention.layernorm_qkv.value_weight\", \"model.layers.13.self_attention.proj.weight\", \"model.layers.13.layernorm_mlp.layer_norm_weight\", \"model.layers.13.layernorm_mlp.fc1_weight\", \"model.layers.13.layernorm_mlp.fc2_weight\", \"model.layers.14.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.14.self_attention.layernorm_qkv.query_weight\", \"model.layers.14.self_attention.layernorm_qkv.key_weight\", \"model.layers.14.self_attention.layernorm_qkv.value_weight\", \"model.layers.14.self_attention.proj.weight\", \"model.layers.14.layernorm_mlp.layer_norm_weight\", \"model.layers.14.layernorm_mlp.fc1_weight\", \"model.layers.14.layernorm_mlp.fc2_weight\", \"model.layers.15.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.15.self_attention.layernorm_qkv.query_weight\", \"model.layers.15.self_attention.layernorm_qkv.key_weight\", \"model.layers.15.self_attention.layernorm_qkv.value_weight\", \"model.layers.15.self_attention.proj.weight\", \"model.layers.15.layernorm_mlp.layer_norm_weight\", \"model.layers.15.layernorm_mlp.fc1_weight\", \"model.layers.15.layernorm_mlp.fc2_weight\", \"model.layers.16.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.16.self_attention.layernorm_qkv.query_weight\", \"model.layers.16.self_attention.layernorm_qkv.key_weight\", \"model.layers.16.self_attention.layernorm_qkv.value_weight\", \"model.layers.16.self_attention.proj.weight\", \"model.layers.16.layernorm_mlp.layer_norm_weight\", \"model.layers.16.layernorm_mlp.fc1_weight\", \"model.layers.16.layernorm_mlp.fc2_weight\", \"model.layers.17.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.17.self_attention.layernorm_qkv.query_weight\", \"model.layers.17.self_attention.layernorm_qkv.key_weight\", \"model.layers.17.self_attention.layernorm_qkv.value_weight\", \"model.layers.17.self_attention.proj.weight\", \"model.layers.17.layernorm_mlp.layer_norm_weight\", \"model.layers.17.layernorm_mlp.fc1_weight\", \"model.layers.17.layernorm_mlp.fc2_weight\", \"model.layers.18.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.18.self_attention.layernorm_qkv.query_weight\", \"model.layers.18.self_attention.layernorm_qkv.key_weight\", \"model.layers.18.self_attention.layernorm_qkv.value_weight\", \"model.layers.18.self_attention.proj.weight\", \"model.layers.18.layernorm_mlp.layer_norm_weight\", \"model.layers.18.layernorm_mlp.fc1_weight\", \"model.layers.18.layernorm_mlp.fc2_weight\", \"model.layers.19.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.19.self_attention.layernorm_qkv.query_weight\", \"model.layers.19.self_attention.layernorm_qkv.key_weight\", \"model.layers.19.self_attention.layernorm_qkv.value_weight\", \"model.layers.19.self_attention.proj.weight\", \"model.layers.19.layernorm_mlp.layer_norm_weight\", \"model.layers.19.layernorm_mlp.fc1_weight\", \"model.layers.19.layernorm_mlp.fc2_weight\", \"model.layers.20.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.20.self_attention.layernorm_qkv.query_weight\", \"model.layers.20.self_attention.layernorm_qkv.key_weight\", \"model.layers.20.self_attention.layernorm_qkv.value_weight\", \"model.layers.20.self_attention.proj.weight\", \"model.layers.20.layernorm_mlp.layer_norm_weight\", \"model.layers.20.layernorm_mlp.fc1_weight\", \"model.layers.20.layernorm_mlp.fc2_weight\", \"model.layers.21.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.21.self_attention.layernorm_qkv.query_weight\", \"model.layers.21.self_attention.layernorm_qkv.key_weight\", \"model.layers.21.self_attention.layernorm_qkv.value_weight\", \"model.layers.21.self_attention.proj.weight\", \"model.layers.21.layernorm_mlp.layer_norm_weight\", \"model.layers.21.layernorm_mlp.fc1_weight\", \"model.layers.21.layernorm_mlp.fc2_weight\", \"model.layers.22.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.22.self_attention.layernorm_qkv.query_weight\", \"model.layers.22.self_attention.layernorm_qkv.key_weight\", \"model.layers.22.self_attention.layernorm_qkv.value_weight\", \"model.layers.22.self_attention.proj.weight\", \"model.layers.22.layernorm_mlp.layer_norm_weight\", \"model.layers.22.layernorm_mlp.fc1_weight\", \"model.layers.22.layernorm_mlp.fc2_weight\", \"model.layers.23.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.23.self_attention.layernorm_qkv.query_weight\", \"model.layers.23.self_attention.layernorm_qkv.key_weight\", \"model.layers.23.self_attention.layernorm_qkv.value_weight\", \"model.layers.23.self_attention.proj.weight\", \"model.layers.23.layernorm_mlp.layer_norm_weight\", \"model.layers.23.layernorm_mlp.fc1_weight\", \"model.layers.23.layernorm_mlp.fc2_weight\", \"model.layers.24.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.24.self_attention.layernorm_qkv.query_weight\", \"model.layers.24.self_attention.layernorm_qkv.key_weight\", \"model.layers.24.self_attention.layernorm_qkv.value_weight\", \"model.layers.24.self_attention.proj.weight\", \"model.layers.24.layernorm_mlp.layer_norm_weight\", \"model.layers.24.layernorm_mlp.fc1_weight\", \"model.layers.24.layernorm_mlp.fc2_weight\", \"model.layers.25.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.25.self_attention.layernorm_qkv.query_weight\", \"model.layers.25.self_attention.layernorm_qkv.key_weight\", \"model.layers.25.self_attention.layernorm_qkv.value_weight\", \"model.layers.25.self_attention.proj.weight\", \"model.layers.25.layernorm_mlp.layer_norm_weight\", \"model.layers.25.layernorm_mlp.fc1_weight\", \"model.layers.25.layernorm_mlp.fc2_weight\", \"model.layers.26.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.26.self_attention.layernorm_qkv.query_weight\", \"model.layers.26.self_attention.layernorm_qkv.key_weight\", \"model.layers.26.self_attention.layernorm_qkv.value_weight\", \"model.layers.26.self_attention.proj.weight\", \"model.layers.26.layernorm_mlp.layer_norm_weight\", \"model.layers.26.layernorm_mlp.fc1_weight\", \"model.layers.26.layernorm_mlp.fc2_weight\", \"model.layers.27.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.27.self_attention.layernorm_qkv.query_weight\", \"model.layers.27.self_attention.layernorm_qkv.key_weight\", \"model.layers.27.self_attention.layernorm_qkv.value_weight\", \"model.layers.27.self_attention.proj.weight\", \"model.layers.27.layernorm_mlp.layer_norm_weight\", \"model.layers.27.layernorm_mlp.fc1_weight\", \"model.layers.27.layernorm_mlp.fc2_weight\", \"model.layers.28.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.28.self_attention.layernorm_qkv.query_weight\", \"model.layers.28.self_attention.layernorm_qkv.key_weight\", \"model.layers.28.self_attention.layernorm_qkv.value_weight\", \"model.layers.28.self_attention.proj.weight\", \"model.layers.28.layernorm_mlp.layer_norm_weight\", \"model.layers.28.layernorm_mlp.fc1_weight\", \"model.layers.28.layernorm_mlp.fc2_weight\", \"model.layers.29.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.29.self_attention.layernorm_qkv.query_weight\", \"model.layers.29.self_attention.layernorm_qkv.key_weight\", \"model.layers.29.self_attention.layernorm_qkv.value_weight\", \"model.layers.29.self_attention.proj.weight\", \"model.layers.29.layernorm_mlp.layer_norm_weight\", \"model.layers.29.layernorm_mlp.fc1_weight\", \"model.layers.29.layernorm_mlp.fc2_weight\", \"model.layers.30.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.30.self_attention.layernorm_qkv.query_weight\", \"model.layers.30.self_attention.layernorm_qkv.key_weight\", \"model.layers.30.self_attention.layernorm_qkv.value_weight\", \"model.layers.30.self_attention.proj.weight\", \"model.layers.30.layernorm_mlp.layer_norm_weight\", \"model.layers.30.layernorm_mlp.fc1_weight\", \"model.layers.30.layernorm_mlp.fc2_weight\", \"model.layers.31.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.31.self_attention.layernorm_qkv.query_weight\", \"model.layers.31.self_attention.layernorm_qkv.key_weight\", \"model.layers.31.self_attention.layernorm_qkv.value_weight\", \"model.layers.31.self_attention.proj.weight\", \"model.layers.31.layernorm_mlp.layer_norm_weight\", \"model.layers.31.layernorm_mlp.fc1_weight\", \"model.layers.31.layernorm_mlp.fc2_weight\". \n\tsize mismatch for model.norm.weight: copying a param with shape torch.Size([8192]) from checkpoint, the shape in current model is torch.Size([4096]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mckpt_dict: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mckpt_dict\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m model_dict\u001b[38;5;241m.\u001b[39mupdate(ckpt_dict)\n\u001b[0;32m---> 52\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/fastdata/peiran/miniconda3/envs/sfm/lib/python3.11/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LlamaForCausalLM:\n\tUnexpected key(s) in state_dict: \"model.word_embeddings.weight\", \"model.dummy.weight\", \"model.dummy.bias\", \"model.lm_head.weight\", \"model.num_head.fc1.weight\", \"model.num_head.fc1.bias\", \"model.num_head.fc2.weight\", \"model.num_head.fc2.bias\", \"model.layers.0.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.0.self_attention.layernorm_qkv.query_weight\", \"model.layers.0.self_attention.layernorm_qkv.key_weight\", \"model.layers.0.self_attention.layernorm_qkv.value_weight\", \"model.layers.0.self_attention.proj.weight\", \"model.layers.0.layernorm_mlp.layer_norm_weight\", \"model.layers.0.layernorm_mlp.fc1_weight\", \"model.layers.0.layernorm_mlp.fc2_weight\", \"model.layers.1.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.1.self_attention.layernorm_qkv.query_weight\", \"model.layers.1.self_attention.layernorm_qkv.key_weight\", \"model.layers.1.self_attention.layernorm_qkv.value_weight\", \"model.layers.1.self_attention.proj.weight\", \"model.layers.1.layernorm_mlp.layer_norm_weight\", \"model.layers.1.layernorm_mlp.fc1_weight\", \"model.layers.1.layernorm_mlp.fc2_weight\", \"model.layers.2.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.2.self_attention.layernorm_qkv.query_weight\", \"model.layers.2.self_attention.layernorm_qkv.key_weight\", \"model.layers.2.self_attention.layernorm_qkv.value_weight\", \"model.layers.2.self_attention.proj.weight\", \"model.layers.2.layernorm_mlp.layer_norm_weight\", \"model.layers.2.layernorm_mlp.fc1_weight\", \"model.layers.2.layernorm_mlp.fc2_weight\", \"model.layers.3.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.3.self_attention.layernorm_qkv.query_weight\", \"model.layers.3.self_attention.layernorm_qkv.key_weight\", \"model.layers.3.self_attention.layernorm_qkv.value_weight\", \"model.layers.3.self_attention.proj.weight\", \"model.layers.3.layernorm_mlp.layer_norm_weight\", \"model.layers.3.layernorm_mlp.fc1_weight\", \"model.layers.3.layernorm_mlp.fc2_weight\", \"model.layers.4.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.4.self_attention.layernorm_qkv.query_weight\", \"model.layers.4.self_attention.layernorm_qkv.key_weight\", \"model.layers.4.self_attention.layernorm_qkv.value_weight\", \"model.layers.4.self_attention.proj.weight\", \"model.layers.4.layernorm_mlp.layer_norm_weight\", \"model.layers.4.layernorm_mlp.fc1_weight\", \"model.layers.4.layernorm_mlp.fc2_weight\", \"model.layers.5.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.5.self_attention.layernorm_qkv.query_weight\", \"model.layers.5.self_attention.layernorm_qkv.key_weight\", \"model.layers.5.self_attention.layernorm_qkv.value_weight\", \"model.layers.5.self_attention.proj.weight\", \"model.layers.5.layernorm_mlp.layer_norm_weight\", \"model.layers.5.layernorm_mlp.fc1_weight\", \"model.layers.5.layernorm_mlp.fc2_weight\", \"model.layers.6.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.6.self_attention.layernorm_qkv.query_weight\", \"model.layers.6.self_attention.layernorm_qkv.key_weight\", \"model.layers.6.self_attention.layernorm_qkv.value_weight\", \"model.layers.6.self_attention.proj.weight\", \"model.layers.6.layernorm_mlp.layer_norm_weight\", \"model.layers.6.layernorm_mlp.fc1_weight\", \"model.layers.6.layernorm_mlp.fc2_weight\", \"model.layers.7.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.7.self_attention.layernorm_qkv.query_weight\", \"model.layers.7.self_attention.layernorm_qkv.key_weight\", \"model.layers.7.self_attention.layernorm_qkv.value_weight\", \"model.layers.7.self_attention.proj.weight\", \"model.layers.7.layernorm_mlp.layer_norm_weight\", \"model.layers.7.layernorm_mlp.fc1_weight\", \"model.layers.7.layernorm_mlp.fc2_weight\", \"model.layers.8.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.8.self_attention.layernorm_qkv.query_weight\", \"model.layers.8.self_attention.layernorm_qkv.key_weight\", \"model.layers.8.self_attention.layernorm_qkv.value_weight\", \"model.layers.8.self_attention.proj.weight\", \"model.layers.8.layernorm_mlp.layer_norm_weight\", \"model.layers.8.layernorm_mlp.fc1_weight\", \"model.layers.8.layernorm_mlp.fc2_weight\", \"model.layers.9.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.9.self_attention.layernorm_qkv.query_weight\", \"model.layers.9.self_attention.layernorm_qkv.key_weight\", \"model.layers.9.self_attention.layernorm_qkv.value_weight\", \"model.layers.9.self_attention.proj.weight\", \"model.layers.9.layernorm_mlp.layer_norm_weight\", \"model.layers.9.layernorm_mlp.fc1_weight\", \"model.layers.9.layernorm_mlp.fc2_weight\", \"model.layers.10.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.10.self_attention.layernorm_qkv.query_weight\", \"model.layers.10.self_attention.layernorm_qkv.key_weight\", \"model.layers.10.self_attention.layernorm_qkv.value_weight\", \"model.layers.10.self_attention.proj.weight\", \"model.layers.10.layernorm_mlp.layer_norm_weight\", \"model.layers.10.layernorm_mlp.fc1_weight\", \"model.layers.10.layernorm_mlp.fc2_weight\", \"model.layers.11.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.11.self_attention.layernorm_qkv.query_weight\", \"model.layers.11.self_attention.layernorm_qkv.key_weight\", \"model.layers.11.self_attention.layernorm_qkv.value_weight\", \"model.layers.11.self_attention.proj.weight\", \"model.layers.11.layernorm_mlp.layer_norm_weight\", \"model.layers.11.layernorm_mlp.fc1_weight\", \"model.layers.11.layernorm_mlp.fc2_weight\", \"model.layers.12.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.12.self_attention.layernorm_qkv.query_weight\", \"model.layers.12.self_attention.layernorm_qkv.key_weight\", \"model.layers.12.self_attention.layernorm_qkv.value_weight\", \"model.layers.12.self_attention.proj.weight\", \"model.layers.12.layernorm_mlp.layer_norm_weight\", \"model.layers.12.layernorm_mlp.fc1_weight\", \"model.layers.12.layernorm_mlp.fc2_weight\", \"model.layers.13.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.13.self_attention.layernorm_qkv.query_weight\", \"model.layers.13.self_attention.layernorm_qkv.key_weight\", \"model.layers.13.self_attention.layernorm_qkv.value_weight\", \"model.layers.13.self_attention.proj.weight\", \"model.layers.13.layernorm_mlp.layer_norm_weight\", \"model.layers.13.layernorm_mlp.fc1_weight\", \"model.layers.13.layernorm_mlp.fc2_weight\", \"model.layers.14.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.14.self_attention.layernorm_qkv.query_weight\", \"model.layers.14.self_attention.layernorm_qkv.key_weight\", \"model.layers.14.self_attention.layernorm_qkv.value_weight\", \"model.layers.14.self_attention.proj.weight\", \"model.layers.14.layernorm_mlp.layer_norm_weight\", \"model.layers.14.layernorm_mlp.fc1_weight\", \"model.layers.14.layernorm_mlp.fc2_weight\", \"model.layers.15.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.15.self_attention.layernorm_qkv.query_weight\", \"model.layers.15.self_attention.layernorm_qkv.key_weight\", \"model.layers.15.self_attention.layernorm_qkv.value_weight\", \"model.layers.15.self_attention.proj.weight\", \"model.layers.15.layernorm_mlp.layer_norm_weight\", \"model.layers.15.layernorm_mlp.fc1_weight\", \"model.layers.15.layernorm_mlp.fc2_weight\", \"model.layers.16.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.16.self_attention.layernorm_qkv.query_weight\", \"model.layers.16.self_attention.layernorm_qkv.key_weight\", \"model.layers.16.self_attention.layernorm_qkv.value_weight\", \"model.layers.16.self_attention.proj.weight\", \"model.layers.16.layernorm_mlp.layer_norm_weight\", \"model.layers.16.layernorm_mlp.fc1_weight\", \"model.layers.16.layernorm_mlp.fc2_weight\", \"model.layers.17.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.17.self_attention.layernorm_qkv.query_weight\", \"model.layers.17.self_attention.layernorm_qkv.key_weight\", \"model.layers.17.self_attention.layernorm_qkv.value_weight\", \"model.layers.17.self_attention.proj.weight\", \"model.layers.17.layernorm_mlp.layer_norm_weight\", \"model.layers.17.layernorm_mlp.fc1_weight\", \"model.layers.17.layernorm_mlp.fc2_weight\", \"model.layers.18.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.18.self_attention.layernorm_qkv.query_weight\", \"model.layers.18.self_attention.layernorm_qkv.key_weight\", \"model.layers.18.self_attention.layernorm_qkv.value_weight\", \"model.layers.18.self_attention.proj.weight\", \"model.layers.18.layernorm_mlp.layer_norm_weight\", \"model.layers.18.layernorm_mlp.fc1_weight\", \"model.layers.18.layernorm_mlp.fc2_weight\", \"model.layers.19.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.19.self_attention.layernorm_qkv.query_weight\", \"model.layers.19.self_attention.layernorm_qkv.key_weight\", \"model.layers.19.self_attention.layernorm_qkv.value_weight\", \"model.layers.19.self_attention.proj.weight\", \"model.layers.19.layernorm_mlp.layer_norm_weight\", \"model.layers.19.layernorm_mlp.fc1_weight\", \"model.layers.19.layernorm_mlp.fc2_weight\", \"model.layers.20.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.20.self_attention.layernorm_qkv.query_weight\", \"model.layers.20.self_attention.layernorm_qkv.key_weight\", \"model.layers.20.self_attention.layernorm_qkv.value_weight\", \"model.layers.20.self_attention.proj.weight\", \"model.layers.20.layernorm_mlp.layer_norm_weight\", \"model.layers.20.layernorm_mlp.fc1_weight\", \"model.layers.20.layernorm_mlp.fc2_weight\", \"model.layers.21.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.21.self_attention.layernorm_qkv.query_weight\", \"model.layers.21.self_attention.layernorm_qkv.key_weight\", \"model.layers.21.self_attention.layernorm_qkv.value_weight\", \"model.layers.21.self_attention.proj.weight\", \"model.layers.21.layernorm_mlp.layer_norm_weight\", \"model.layers.21.layernorm_mlp.fc1_weight\", \"model.layers.21.layernorm_mlp.fc2_weight\", \"model.layers.22.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.22.self_attention.layernorm_qkv.query_weight\", \"model.layers.22.self_attention.layernorm_qkv.key_weight\", \"model.layers.22.self_attention.layernorm_qkv.value_weight\", \"model.layers.22.self_attention.proj.weight\", \"model.layers.22.layernorm_mlp.layer_norm_weight\", \"model.layers.22.layernorm_mlp.fc1_weight\", \"model.layers.22.layernorm_mlp.fc2_weight\", \"model.layers.23.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.23.self_attention.layernorm_qkv.query_weight\", \"model.layers.23.self_attention.layernorm_qkv.key_weight\", \"model.layers.23.self_attention.layernorm_qkv.value_weight\", \"model.layers.23.self_attention.proj.weight\", \"model.layers.23.layernorm_mlp.layer_norm_weight\", \"model.layers.23.layernorm_mlp.fc1_weight\", \"model.layers.23.layernorm_mlp.fc2_weight\", \"model.layers.24.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.24.self_attention.layernorm_qkv.query_weight\", \"model.layers.24.self_attention.layernorm_qkv.key_weight\", \"model.layers.24.self_attention.layernorm_qkv.value_weight\", \"model.layers.24.self_attention.proj.weight\", \"model.layers.24.layernorm_mlp.layer_norm_weight\", \"model.layers.24.layernorm_mlp.fc1_weight\", \"model.layers.24.layernorm_mlp.fc2_weight\", \"model.layers.25.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.25.self_attention.layernorm_qkv.query_weight\", \"model.layers.25.self_attention.layernorm_qkv.key_weight\", \"model.layers.25.self_attention.layernorm_qkv.value_weight\", \"model.layers.25.self_attention.proj.weight\", \"model.layers.25.layernorm_mlp.layer_norm_weight\", \"model.layers.25.layernorm_mlp.fc1_weight\", \"model.layers.25.layernorm_mlp.fc2_weight\", \"model.layers.26.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.26.self_attention.layernorm_qkv.query_weight\", \"model.layers.26.self_attention.layernorm_qkv.key_weight\", \"model.layers.26.self_attention.layernorm_qkv.value_weight\", \"model.layers.26.self_attention.proj.weight\", \"model.layers.26.layernorm_mlp.layer_norm_weight\", \"model.layers.26.layernorm_mlp.fc1_weight\", \"model.layers.26.layernorm_mlp.fc2_weight\", \"model.layers.27.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.27.self_attention.layernorm_qkv.query_weight\", \"model.layers.27.self_attention.layernorm_qkv.key_weight\", \"model.layers.27.self_attention.layernorm_qkv.value_weight\", \"model.layers.27.self_attention.proj.weight\", \"model.layers.27.layernorm_mlp.layer_norm_weight\", \"model.layers.27.layernorm_mlp.fc1_weight\", \"model.layers.27.layernorm_mlp.fc2_weight\", \"model.layers.28.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.28.self_attention.layernorm_qkv.query_weight\", \"model.layers.28.self_attention.layernorm_qkv.key_weight\", \"model.layers.28.self_attention.layernorm_qkv.value_weight\", \"model.layers.28.self_attention.proj.weight\", \"model.layers.28.layernorm_mlp.layer_norm_weight\", \"model.layers.28.layernorm_mlp.fc1_weight\", \"model.layers.28.layernorm_mlp.fc2_weight\", \"model.layers.29.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.29.self_attention.layernorm_qkv.query_weight\", \"model.layers.29.self_attention.layernorm_qkv.key_weight\", \"model.layers.29.self_attention.layernorm_qkv.value_weight\", \"model.layers.29.self_attention.proj.weight\", \"model.layers.29.layernorm_mlp.layer_norm_weight\", \"model.layers.29.layernorm_mlp.fc1_weight\", \"model.layers.29.layernorm_mlp.fc2_weight\", \"model.layers.30.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.30.self_attention.layernorm_qkv.query_weight\", \"model.layers.30.self_attention.layernorm_qkv.key_weight\", \"model.layers.30.self_attention.layernorm_qkv.value_weight\", \"model.layers.30.self_attention.proj.weight\", \"model.layers.30.layernorm_mlp.layer_norm_weight\", \"model.layers.30.layernorm_mlp.fc1_weight\", \"model.layers.30.layernorm_mlp.fc2_weight\", \"model.layers.31.self_attention.layernorm_qkv.layer_norm_weight\", \"model.layers.31.self_attention.layernorm_qkv.query_weight\", \"model.layers.31.self_attention.layernorm_qkv.key_weight\", \"model.layers.31.self_attention.layernorm_qkv.value_weight\", \"model.layers.31.self_attention.proj.weight\", \"model.layers.31.layernorm_mlp.layer_norm_weight\", \"model.layers.31.layernorm_mlp.fc1_weight\", \"model.layers.31.layernorm_mlp.fc2_weight\". \n\tsize mismatch for model.norm.weight: copying a param with shape torch.Size([8192]) from checkpoint, the shape in current model is torch.Size([4096])."
     ]
    }
   ],
   "source": [
    "ckpt_dict = {}\n",
    "\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_00-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "layer1 = torch.load(os.path.join(args.save_dir, \"layer_00-model_01-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "\n",
    "for k, _ in layer0.items():\n",
    "    new_k = \"model.embed_tokens.weight\"\n",
    "    v = torch.cat([layer0[k], layer1[k]], dim=0)\n",
    "    ckpt_dict[new_k] = v\n",
    "\n",
    "del layer0, layer1\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 1).zfill(2)\n",
    "    layer0 = torch.load(os.path.join(args.save_dir, f\"layer_{l_index}-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    layer1 = torch.load(os.path.join(args.save_dir, f\"layer_{l_index}-model_01-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "\n",
    "    for k in layer0:\n",
    "        if k.find(\"norm\") != -1:\n",
    "            v = layer0[k]\n",
    "        elif layer0[k].shape[0] == 4096:\n",
    "            v = torch.cat([layer0[k], layer1[k]], dim=1)\n",
    "        elif layer0[k].shape[1] == 4096:\n",
    "            v = torch.cat([layer0[k], layer1[k]], dim=0)\n",
    "\n",
    "        ckpt_dict[f\"model.layers.{l}.{k}\"] = v\n",
    "\n",
    "    del layer0, layer1\n",
    "\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_33-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "layer1 = torch.load(os.path.join(args.save_dir, \"layer_33-model_01-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "\n",
    "for k, _ in layer0.items():\n",
    "    new_k = \"model.\" + k\n",
    "    v = torch.cat([layer0[k], layer1[k]], dim=0)\n",
    "    ckpt_dict[new_k] = v\n",
    "\n",
    "del layer0, layer1\n",
    "\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_34-model_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "layer1 = torch.load(os.path.join(args.save_dir, \"layer_34-model_01-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "\n",
    "for k, _ in layer0.items():\n",
    "    new_k = \"model.\" + k\n",
    "    v = torch.cat([layer0[k], layer1[k]], dim=0)\n",
    "    ckpt_dict[new_k] = v\n",
    "\n",
    "del layer0, layer1\n",
    "\n",
    "print(f\"ckpt_dict: {ckpt_dict.keys()}\")\n",
    "model_dict.update(ckpt_dict)\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# print(f\"input: {text},\\n output: {res}\")\n",
    "\n",
    "# # output = model.generate(\n",
    "# #     input_ids=batched_data['input_ids'],\n",
    "# #     num_return_sequences=10,\n",
    "# #     num_beams=20,\n",
    "# # )\n",
    "# for i in range(10):\n",
    "#     print(tokenizer.decode(output[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.decoder.generate(\n",
    "    input_ids=torch.tensor(tokenizer.encode(\"Football is a \", return_tensors=\"pt\")).to(device),\n",
    "    num_beams=5,\n",
    "    max_new_tokens=512,\n",
    "    num_return_sequences=1,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "res = tokenizer.decode(output.sequences[0], skip_special_tokens=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt1 = torch.load(\"/data/peiran/blob/hai1data/sfm/llama/Meta-Llama-3-8B/original/layer_34-model_states.pt\", map_location=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt2 = torch.load(\"/data/peiran/blob/hai1data/sfm/nlm/output/llama3_stageA_tp2/backup/global_step1999/layer_34-model_00-model_states.pt\", map_location=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ckpt1.keys(), ckpt2.keys())\n",
    "# print(ckpt1[\"embed_tokens.weight\"].shape)\n",
    "print(ckpt2[\"word_embeddings.weight\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(torch.sum(torch.abs(ckpt1[\"embed_tokens.weight\"][65152:,:]-ckpt2[\"word_embeddings.weight\"][:128256-65152, :])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=10000000)\n",
    "print(torch.mean(torch.abs(ckpt1[\"embed_tokens.weight\"][:65152,:].to(torch.bfloat16)-ckpt2[\"word_embeddings.weight\"][:65152, :].to(torch.bfloat16))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1 = \"self_attn.k_proj.weight\"\n",
    "key2 = \"self_attention.layernorm_qkv.key_weight\"\n",
    "print(ckpt1[key1].shape, ckpt2[key2].shape)\n",
    "print(torch.sum(torch.abs(ckpt1[\"self_attn.k_proj.weight\"][:512,:]-ckpt2[\"self_attention.layernorm_qkv.key_weight\"][:512,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(torch.abs(ckpt1[\"lm_head.weight\"][:65152, :]-ckpt2[\"lm_head.weight\"][:65152, :])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63104, 4096]) torch.Size([63104, 4096])\n"
     ]
    }
   ],
   "source": [
    "ckpt3 = ckpt2\n",
    "# print(ckpt1[\"lm_head.weight\"].shape, ckpt1[\"lm_head.weight\"].dtype)\n",
    "ckpt3[\"lm_head.weight\"][:, :] = ckpt1[\"lm_head.weight\"][:65152, :]\n",
    "print(ckpt3[\"lm_head.weight\"][:128256-65152, :].shape, ckpt1[\"lm_head.weight\"][65152:, :].shape)\n",
    "ckpt3[\"lm_head.weight\"][:128256-65152, :] = ckpt1[\"lm_head.weight\"][65152:, :]\n",
    "ckpt3[\"lm_head.weight\"][128256-65152:, :] = ckpt2[\"lm_head.weight\"][128256-65152:, :]\n",
    "torch.save(ckpt3, \"/data/peiran/blob/hai1data/sfm/nlm/output/llama3_stageA_tp2/backup/global_step1999/layer_34-model_01-model_states.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
