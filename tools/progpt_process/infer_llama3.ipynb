{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fastdata/peiran/miniconda3/envs/sfm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-25 01:10:07,124] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[\u001b[32m2024-04-25 01:10:07.533\u001b[0m][\u001b[36mINFO\u001b[0m]: apex is installed, using FusedAdam with fp16 optimizer states\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import transformers\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sfm.models.scigpt.scigpt import ScigptModel\n",
    "from sfm.models.scigpt.config import ScigptConfig\n",
    "from sfm.utils import arg_utils\n",
    "from sfm.utils.science_tokens import SCIENCE_TAG_TOKENS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def get_args_and_tokenizer(use_llama=False):\n",
    "    parser = ArgumentParser()\n",
    "    cfg_classes = [ScigptConfig]\n",
    "    parser = arg_utils.add_dataclass_to_parser(cfg_classes, parser)\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.load_ckpt = False\n",
    "    args.strategy = \"DDP\"\n",
    "    args.encoder_layers = 33\n",
    "    args.encoder_embed_dim = 1280\n",
    "    args.encoder_ffn_embed_dim = 5120\n",
    "    args.encoder_attention_heads = 20\n",
    "    args.infer = True\n",
    "    args.bf16 = True\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/data/peiran/blob/hai1data/sfm/llama/Meta-Llama-3-8B/original\")\n",
    "    args.save_dir = \"/data/peiran/blob/hai1data/sfm/llama/Meta-Llama-3-8B/original\"\n",
    "    args.llm_model_name_or_path = \"/data/peiran/blob/hai1data/sfm/llama/Meta-Llama-3-8B/original\"\n",
    "\n",
    "    special_tokens_dict = dict()\n",
    "    if tokenizer.pad_token is None:\n",
    "        special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "    if tokenizer.eos_token is None:\n",
    "        special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "    if tokenizer.bos_token is None:\n",
    "        special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "    if tokenizer.unk_token is None:\n",
    "        special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "    # special_tokens_dict[\"additional_special_tokens\"] = SCIENCE_TAG_TOKENS\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        \n",
    "\n",
    "    return args, tokenizer\n",
    "\n",
    "args, tokenizer = get_args_and_tokenizer()\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_dict: odict_keys(['decoder.model.embed_tokens.weight', 'decoder.model.layers.0.self_attn.q_proj.weight', 'decoder.model.layers.0.self_attn.k_proj.weight', 'decoder.model.layers.0.self_attn.v_proj.weight', 'decoder.model.layers.0.self_attn.o_proj.weight', 'decoder.model.layers.0.mlp.gate_proj.weight', 'decoder.model.layers.0.mlp.up_proj.weight', 'decoder.model.layers.0.mlp.down_proj.weight', 'decoder.model.layers.0.input_layernorm.weight', 'decoder.model.layers.0.post_attention_layernorm.weight', 'decoder.model.layers.1.self_attn.q_proj.weight', 'decoder.model.layers.1.self_attn.k_proj.weight', 'decoder.model.layers.1.self_attn.v_proj.weight', 'decoder.model.layers.1.self_attn.o_proj.weight', 'decoder.model.layers.1.mlp.gate_proj.weight', 'decoder.model.layers.1.mlp.up_proj.weight', 'decoder.model.layers.1.mlp.down_proj.weight', 'decoder.model.layers.1.input_layernorm.weight', 'decoder.model.layers.1.post_attention_layernorm.weight', 'decoder.model.layers.2.self_attn.q_proj.weight', 'decoder.model.layers.2.self_attn.k_proj.weight', 'decoder.model.layers.2.self_attn.v_proj.weight', 'decoder.model.layers.2.self_attn.o_proj.weight', 'decoder.model.layers.2.mlp.gate_proj.weight', 'decoder.model.layers.2.mlp.up_proj.weight', 'decoder.model.layers.2.mlp.down_proj.weight', 'decoder.model.layers.2.input_layernorm.weight', 'decoder.model.layers.2.post_attention_layernorm.weight', 'decoder.model.layers.3.self_attn.q_proj.weight', 'decoder.model.layers.3.self_attn.k_proj.weight', 'decoder.model.layers.3.self_attn.v_proj.weight', 'decoder.model.layers.3.self_attn.o_proj.weight', 'decoder.model.layers.3.mlp.gate_proj.weight', 'decoder.model.layers.3.mlp.up_proj.weight', 'decoder.model.layers.3.mlp.down_proj.weight', 'decoder.model.layers.3.input_layernorm.weight', 'decoder.model.layers.3.post_attention_layernorm.weight', 'decoder.model.layers.4.self_attn.q_proj.weight', 'decoder.model.layers.4.self_attn.k_proj.weight', 'decoder.model.layers.4.self_attn.v_proj.weight', 'decoder.model.layers.4.self_attn.o_proj.weight', 'decoder.model.layers.4.mlp.gate_proj.weight', 'decoder.model.layers.4.mlp.up_proj.weight', 'decoder.model.layers.4.mlp.down_proj.weight', 'decoder.model.layers.4.input_layernorm.weight', 'decoder.model.layers.4.post_attention_layernorm.weight', 'decoder.model.layers.5.self_attn.q_proj.weight', 'decoder.model.layers.5.self_attn.k_proj.weight', 'decoder.model.layers.5.self_attn.v_proj.weight', 'decoder.model.layers.5.self_attn.o_proj.weight', 'decoder.model.layers.5.mlp.gate_proj.weight', 'decoder.model.layers.5.mlp.up_proj.weight', 'decoder.model.layers.5.mlp.down_proj.weight', 'decoder.model.layers.5.input_layernorm.weight', 'decoder.model.layers.5.post_attention_layernorm.weight', 'decoder.model.layers.6.self_attn.q_proj.weight', 'decoder.model.layers.6.self_attn.k_proj.weight', 'decoder.model.layers.6.self_attn.v_proj.weight', 'decoder.model.layers.6.self_attn.o_proj.weight', 'decoder.model.layers.6.mlp.gate_proj.weight', 'decoder.model.layers.6.mlp.up_proj.weight', 'decoder.model.layers.6.mlp.down_proj.weight', 'decoder.model.layers.6.input_layernorm.weight', 'decoder.model.layers.6.post_attention_layernorm.weight', 'decoder.model.layers.7.self_attn.q_proj.weight', 'decoder.model.layers.7.self_attn.k_proj.weight', 'decoder.model.layers.7.self_attn.v_proj.weight', 'decoder.model.layers.7.self_attn.o_proj.weight', 'decoder.model.layers.7.mlp.gate_proj.weight', 'decoder.model.layers.7.mlp.up_proj.weight', 'decoder.model.layers.7.mlp.down_proj.weight', 'decoder.model.layers.7.input_layernorm.weight', 'decoder.model.layers.7.post_attention_layernorm.weight', 'decoder.model.layers.8.self_attn.q_proj.weight', 'decoder.model.layers.8.self_attn.k_proj.weight', 'decoder.model.layers.8.self_attn.v_proj.weight', 'decoder.model.layers.8.self_attn.o_proj.weight', 'decoder.model.layers.8.mlp.gate_proj.weight', 'decoder.model.layers.8.mlp.up_proj.weight', 'decoder.model.layers.8.mlp.down_proj.weight', 'decoder.model.layers.8.input_layernorm.weight', 'decoder.model.layers.8.post_attention_layernorm.weight', 'decoder.model.layers.9.self_attn.q_proj.weight', 'decoder.model.layers.9.self_attn.k_proj.weight', 'decoder.model.layers.9.self_attn.v_proj.weight', 'decoder.model.layers.9.self_attn.o_proj.weight', 'decoder.model.layers.9.mlp.gate_proj.weight', 'decoder.model.layers.9.mlp.up_proj.weight', 'decoder.model.layers.9.mlp.down_proj.weight', 'decoder.model.layers.9.input_layernorm.weight', 'decoder.model.layers.9.post_attention_layernorm.weight', 'decoder.model.layers.10.self_attn.q_proj.weight', 'decoder.model.layers.10.self_attn.k_proj.weight', 'decoder.model.layers.10.self_attn.v_proj.weight', 'decoder.model.layers.10.self_attn.o_proj.weight', 'decoder.model.layers.10.mlp.gate_proj.weight', 'decoder.model.layers.10.mlp.up_proj.weight', 'decoder.model.layers.10.mlp.down_proj.weight', 'decoder.model.layers.10.input_layernorm.weight', 'decoder.model.layers.10.post_attention_layernorm.weight', 'decoder.model.layers.11.self_attn.q_proj.weight', 'decoder.model.layers.11.self_attn.k_proj.weight', 'decoder.model.layers.11.self_attn.v_proj.weight', 'decoder.model.layers.11.self_attn.o_proj.weight', 'decoder.model.layers.11.mlp.gate_proj.weight', 'decoder.model.layers.11.mlp.up_proj.weight', 'decoder.model.layers.11.mlp.down_proj.weight', 'decoder.model.layers.11.input_layernorm.weight', 'decoder.model.layers.11.post_attention_layernorm.weight', 'decoder.model.layers.12.self_attn.q_proj.weight', 'decoder.model.layers.12.self_attn.k_proj.weight', 'decoder.model.layers.12.self_attn.v_proj.weight', 'decoder.model.layers.12.self_attn.o_proj.weight', 'decoder.model.layers.12.mlp.gate_proj.weight', 'decoder.model.layers.12.mlp.up_proj.weight', 'decoder.model.layers.12.mlp.down_proj.weight', 'decoder.model.layers.12.input_layernorm.weight', 'decoder.model.layers.12.post_attention_layernorm.weight', 'decoder.model.layers.13.self_attn.q_proj.weight', 'decoder.model.layers.13.self_attn.k_proj.weight', 'decoder.model.layers.13.self_attn.v_proj.weight', 'decoder.model.layers.13.self_attn.o_proj.weight', 'decoder.model.layers.13.mlp.gate_proj.weight', 'decoder.model.layers.13.mlp.up_proj.weight', 'decoder.model.layers.13.mlp.down_proj.weight', 'decoder.model.layers.13.input_layernorm.weight', 'decoder.model.layers.13.post_attention_layernorm.weight', 'decoder.model.layers.14.self_attn.q_proj.weight', 'decoder.model.layers.14.self_attn.k_proj.weight', 'decoder.model.layers.14.self_attn.v_proj.weight', 'decoder.model.layers.14.self_attn.o_proj.weight', 'decoder.model.layers.14.mlp.gate_proj.weight', 'decoder.model.layers.14.mlp.up_proj.weight', 'decoder.model.layers.14.mlp.down_proj.weight', 'decoder.model.layers.14.input_layernorm.weight', 'decoder.model.layers.14.post_attention_layernorm.weight', 'decoder.model.layers.15.self_attn.q_proj.weight', 'decoder.model.layers.15.self_attn.k_proj.weight', 'decoder.model.layers.15.self_attn.v_proj.weight', 'decoder.model.layers.15.self_attn.o_proj.weight', 'decoder.model.layers.15.mlp.gate_proj.weight', 'decoder.model.layers.15.mlp.up_proj.weight', 'decoder.model.layers.15.mlp.down_proj.weight', 'decoder.model.layers.15.input_layernorm.weight', 'decoder.model.layers.15.post_attention_layernorm.weight', 'decoder.model.layers.16.self_attn.q_proj.weight', 'decoder.model.layers.16.self_attn.k_proj.weight', 'decoder.model.layers.16.self_attn.v_proj.weight', 'decoder.model.layers.16.self_attn.o_proj.weight', 'decoder.model.layers.16.mlp.gate_proj.weight', 'decoder.model.layers.16.mlp.up_proj.weight', 'decoder.model.layers.16.mlp.down_proj.weight', 'decoder.model.layers.16.input_layernorm.weight', 'decoder.model.layers.16.post_attention_layernorm.weight', 'decoder.model.layers.17.self_attn.q_proj.weight', 'decoder.model.layers.17.self_attn.k_proj.weight', 'decoder.model.layers.17.self_attn.v_proj.weight', 'decoder.model.layers.17.self_attn.o_proj.weight', 'decoder.model.layers.17.mlp.gate_proj.weight', 'decoder.model.layers.17.mlp.up_proj.weight', 'decoder.model.layers.17.mlp.down_proj.weight', 'decoder.model.layers.17.input_layernorm.weight', 'decoder.model.layers.17.post_attention_layernorm.weight', 'decoder.model.layers.18.self_attn.q_proj.weight', 'decoder.model.layers.18.self_attn.k_proj.weight', 'decoder.model.layers.18.self_attn.v_proj.weight', 'decoder.model.layers.18.self_attn.o_proj.weight', 'decoder.model.layers.18.mlp.gate_proj.weight', 'decoder.model.layers.18.mlp.up_proj.weight', 'decoder.model.layers.18.mlp.down_proj.weight', 'decoder.model.layers.18.input_layernorm.weight', 'decoder.model.layers.18.post_attention_layernorm.weight', 'decoder.model.layers.19.self_attn.q_proj.weight', 'decoder.model.layers.19.self_attn.k_proj.weight', 'decoder.model.layers.19.self_attn.v_proj.weight', 'decoder.model.layers.19.self_attn.o_proj.weight', 'decoder.model.layers.19.mlp.gate_proj.weight', 'decoder.model.layers.19.mlp.up_proj.weight', 'decoder.model.layers.19.mlp.down_proj.weight', 'decoder.model.layers.19.input_layernorm.weight', 'decoder.model.layers.19.post_attention_layernorm.weight', 'decoder.model.layers.20.self_attn.q_proj.weight', 'decoder.model.layers.20.self_attn.k_proj.weight', 'decoder.model.layers.20.self_attn.v_proj.weight', 'decoder.model.layers.20.self_attn.o_proj.weight', 'decoder.model.layers.20.mlp.gate_proj.weight', 'decoder.model.layers.20.mlp.up_proj.weight', 'decoder.model.layers.20.mlp.down_proj.weight', 'decoder.model.layers.20.input_layernorm.weight', 'decoder.model.layers.20.post_attention_layernorm.weight', 'decoder.model.layers.21.self_attn.q_proj.weight', 'decoder.model.layers.21.self_attn.k_proj.weight', 'decoder.model.layers.21.self_attn.v_proj.weight', 'decoder.model.layers.21.self_attn.o_proj.weight', 'decoder.model.layers.21.mlp.gate_proj.weight', 'decoder.model.layers.21.mlp.up_proj.weight', 'decoder.model.layers.21.mlp.down_proj.weight', 'decoder.model.layers.21.input_layernorm.weight', 'decoder.model.layers.21.post_attention_layernorm.weight', 'decoder.model.layers.22.self_attn.q_proj.weight', 'decoder.model.layers.22.self_attn.k_proj.weight', 'decoder.model.layers.22.self_attn.v_proj.weight', 'decoder.model.layers.22.self_attn.o_proj.weight', 'decoder.model.layers.22.mlp.gate_proj.weight', 'decoder.model.layers.22.mlp.up_proj.weight', 'decoder.model.layers.22.mlp.down_proj.weight', 'decoder.model.layers.22.input_layernorm.weight', 'decoder.model.layers.22.post_attention_layernorm.weight', 'decoder.model.layers.23.self_attn.q_proj.weight', 'decoder.model.layers.23.self_attn.k_proj.weight', 'decoder.model.layers.23.self_attn.v_proj.weight', 'decoder.model.layers.23.self_attn.o_proj.weight', 'decoder.model.layers.23.mlp.gate_proj.weight', 'decoder.model.layers.23.mlp.up_proj.weight', 'decoder.model.layers.23.mlp.down_proj.weight', 'decoder.model.layers.23.input_layernorm.weight', 'decoder.model.layers.23.post_attention_layernorm.weight', 'decoder.model.layers.24.self_attn.q_proj.weight', 'decoder.model.layers.24.self_attn.k_proj.weight', 'decoder.model.layers.24.self_attn.v_proj.weight', 'decoder.model.layers.24.self_attn.o_proj.weight', 'decoder.model.layers.24.mlp.gate_proj.weight', 'decoder.model.layers.24.mlp.up_proj.weight', 'decoder.model.layers.24.mlp.down_proj.weight', 'decoder.model.layers.24.input_layernorm.weight', 'decoder.model.layers.24.post_attention_layernorm.weight', 'decoder.model.layers.25.self_attn.q_proj.weight', 'decoder.model.layers.25.self_attn.k_proj.weight', 'decoder.model.layers.25.self_attn.v_proj.weight', 'decoder.model.layers.25.self_attn.o_proj.weight', 'decoder.model.layers.25.mlp.gate_proj.weight', 'decoder.model.layers.25.mlp.up_proj.weight', 'decoder.model.layers.25.mlp.down_proj.weight', 'decoder.model.layers.25.input_layernorm.weight', 'decoder.model.layers.25.post_attention_layernorm.weight', 'decoder.model.layers.26.self_attn.q_proj.weight', 'decoder.model.layers.26.self_attn.k_proj.weight', 'decoder.model.layers.26.self_attn.v_proj.weight', 'decoder.model.layers.26.self_attn.o_proj.weight', 'decoder.model.layers.26.mlp.gate_proj.weight', 'decoder.model.layers.26.mlp.up_proj.weight', 'decoder.model.layers.26.mlp.down_proj.weight', 'decoder.model.layers.26.input_layernorm.weight', 'decoder.model.layers.26.post_attention_layernorm.weight', 'decoder.model.layers.27.self_attn.q_proj.weight', 'decoder.model.layers.27.self_attn.k_proj.weight', 'decoder.model.layers.27.self_attn.v_proj.weight', 'decoder.model.layers.27.self_attn.o_proj.weight', 'decoder.model.layers.27.mlp.gate_proj.weight', 'decoder.model.layers.27.mlp.up_proj.weight', 'decoder.model.layers.27.mlp.down_proj.weight', 'decoder.model.layers.27.input_layernorm.weight', 'decoder.model.layers.27.post_attention_layernorm.weight', 'decoder.model.layers.28.self_attn.q_proj.weight', 'decoder.model.layers.28.self_attn.k_proj.weight', 'decoder.model.layers.28.self_attn.v_proj.weight', 'decoder.model.layers.28.self_attn.o_proj.weight', 'decoder.model.layers.28.mlp.gate_proj.weight', 'decoder.model.layers.28.mlp.up_proj.weight', 'decoder.model.layers.28.mlp.down_proj.weight', 'decoder.model.layers.28.input_layernorm.weight', 'decoder.model.layers.28.post_attention_layernorm.weight', 'decoder.model.layers.29.self_attn.q_proj.weight', 'decoder.model.layers.29.self_attn.k_proj.weight', 'decoder.model.layers.29.self_attn.v_proj.weight', 'decoder.model.layers.29.self_attn.o_proj.weight', 'decoder.model.layers.29.mlp.gate_proj.weight', 'decoder.model.layers.29.mlp.up_proj.weight', 'decoder.model.layers.29.mlp.down_proj.weight', 'decoder.model.layers.29.input_layernorm.weight', 'decoder.model.layers.29.post_attention_layernorm.weight', 'decoder.model.layers.30.self_attn.q_proj.weight', 'decoder.model.layers.30.self_attn.k_proj.weight', 'decoder.model.layers.30.self_attn.v_proj.weight', 'decoder.model.layers.30.self_attn.o_proj.weight', 'decoder.model.layers.30.mlp.gate_proj.weight', 'decoder.model.layers.30.mlp.up_proj.weight', 'decoder.model.layers.30.mlp.down_proj.weight', 'decoder.model.layers.30.input_layernorm.weight', 'decoder.model.layers.30.post_attention_layernorm.weight', 'decoder.model.layers.31.self_attn.q_proj.weight', 'decoder.model.layers.31.self_attn.k_proj.weight', 'decoder.model.layers.31.self_attn.v_proj.weight', 'decoder.model.layers.31.self_attn.o_proj.weight', 'decoder.model.layers.31.mlp.gate_proj.weight', 'decoder.model.layers.31.mlp.up_proj.weight', 'decoder.model.layers.31.mlp.down_proj.weight', 'decoder.model.layers.31.input_layernorm.weight', 'decoder.model.layers.31.post_attention_layernorm.weight', 'decoder.model.norm.weight', 'decoder.lm_head.weight'])\n",
      "ckpt_dict: dict_keys(['decoder.model.embed_tokens.weight', 'decoder.model.layers.0.self_attn.q_proj.weight', 'decoder.model.layers.0.self_attn.k_proj.weight', 'decoder.model.layers.0.self_attn.v_proj.weight', 'decoder.model.layers.0.self_attn.o_proj.weight', 'decoder.model.layers.0.mlp.gate_proj.weight', 'decoder.model.layers.0.mlp.up_proj.weight', 'decoder.model.layers.0.mlp.down_proj.weight', 'decoder.model.layers.0.input_layernorm.weight', 'decoder.model.layers.0.post_attention_layernorm.weight', 'decoder.model.layers.1.self_attn.q_proj.weight', 'decoder.model.layers.1.self_attn.k_proj.weight', 'decoder.model.layers.1.self_attn.v_proj.weight', 'decoder.model.layers.1.self_attn.o_proj.weight', 'decoder.model.layers.1.mlp.gate_proj.weight', 'decoder.model.layers.1.mlp.up_proj.weight', 'decoder.model.layers.1.mlp.down_proj.weight', 'decoder.model.layers.1.input_layernorm.weight', 'decoder.model.layers.1.post_attention_layernorm.weight', 'decoder.model.layers.2.self_attn.q_proj.weight', 'decoder.model.layers.2.self_attn.k_proj.weight', 'decoder.model.layers.2.self_attn.v_proj.weight', 'decoder.model.layers.2.self_attn.o_proj.weight', 'decoder.model.layers.2.mlp.gate_proj.weight', 'decoder.model.layers.2.mlp.up_proj.weight', 'decoder.model.layers.2.mlp.down_proj.weight', 'decoder.model.layers.2.input_layernorm.weight', 'decoder.model.layers.2.post_attention_layernorm.weight', 'decoder.model.layers.3.self_attn.q_proj.weight', 'decoder.model.layers.3.self_attn.k_proj.weight', 'decoder.model.layers.3.self_attn.v_proj.weight', 'decoder.model.layers.3.self_attn.o_proj.weight', 'decoder.model.layers.3.mlp.gate_proj.weight', 'decoder.model.layers.3.mlp.up_proj.weight', 'decoder.model.layers.3.mlp.down_proj.weight', 'decoder.model.layers.3.input_layernorm.weight', 'decoder.model.layers.3.post_attention_layernorm.weight', 'decoder.model.layers.4.self_attn.q_proj.weight', 'decoder.model.layers.4.self_attn.k_proj.weight', 'decoder.model.layers.4.self_attn.v_proj.weight', 'decoder.model.layers.4.self_attn.o_proj.weight', 'decoder.model.layers.4.mlp.gate_proj.weight', 'decoder.model.layers.4.mlp.up_proj.weight', 'decoder.model.layers.4.mlp.down_proj.weight', 'decoder.model.layers.4.input_layernorm.weight', 'decoder.model.layers.4.post_attention_layernorm.weight', 'decoder.model.layers.5.self_attn.q_proj.weight', 'decoder.model.layers.5.self_attn.k_proj.weight', 'decoder.model.layers.5.self_attn.v_proj.weight', 'decoder.model.layers.5.self_attn.o_proj.weight', 'decoder.model.layers.5.mlp.gate_proj.weight', 'decoder.model.layers.5.mlp.up_proj.weight', 'decoder.model.layers.5.mlp.down_proj.weight', 'decoder.model.layers.5.input_layernorm.weight', 'decoder.model.layers.5.post_attention_layernorm.weight', 'decoder.model.layers.6.self_attn.q_proj.weight', 'decoder.model.layers.6.self_attn.k_proj.weight', 'decoder.model.layers.6.self_attn.v_proj.weight', 'decoder.model.layers.6.self_attn.o_proj.weight', 'decoder.model.layers.6.mlp.gate_proj.weight', 'decoder.model.layers.6.mlp.up_proj.weight', 'decoder.model.layers.6.mlp.down_proj.weight', 'decoder.model.layers.6.input_layernorm.weight', 'decoder.model.layers.6.post_attention_layernorm.weight', 'decoder.model.layers.7.self_attn.q_proj.weight', 'decoder.model.layers.7.self_attn.k_proj.weight', 'decoder.model.layers.7.self_attn.v_proj.weight', 'decoder.model.layers.7.self_attn.o_proj.weight', 'decoder.model.layers.7.mlp.gate_proj.weight', 'decoder.model.layers.7.mlp.up_proj.weight', 'decoder.model.layers.7.mlp.down_proj.weight', 'decoder.model.layers.7.input_layernorm.weight', 'decoder.model.layers.7.post_attention_layernorm.weight', 'decoder.model.layers.8.self_attn.q_proj.weight', 'decoder.model.layers.8.self_attn.k_proj.weight', 'decoder.model.layers.8.self_attn.v_proj.weight', 'decoder.model.layers.8.self_attn.o_proj.weight', 'decoder.model.layers.8.mlp.gate_proj.weight', 'decoder.model.layers.8.mlp.up_proj.weight', 'decoder.model.layers.8.mlp.down_proj.weight', 'decoder.model.layers.8.input_layernorm.weight', 'decoder.model.layers.8.post_attention_layernorm.weight', 'decoder.model.layers.9.self_attn.q_proj.weight', 'decoder.model.layers.9.self_attn.k_proj.weight', 'decoder.model.layers.9.self_attn.v_proj.weight', 'decoder.model.layers.9.self_attn.o_proj.weight', 'decoder.model.layers.9.mlp.gate_proj.weight', 'decoder.model.layers.9.mlp.up_proj.weight', 'decoder.model.layers.9.mlp.down_proj.weight', 'decoder.model.layers.9.input_layernorm.weight', 'decoder.model.layers.9.post_attention_layernorm.weight', 'decoder.model.layers.10.self_attn.q_proj.weight', 'decoder.model.layers.10.self_attn.k_proj.weight', 'decoder.model.layers.10.self_attn.v_proj.weight', 'decoder.model.layers.10.self_attn.o_proj.weight', 'decoder.model.layers.10.mlp.gate_proj.weight', 'decoder.model.layers.10.mlp.up_proj.weight', 'decoder.model.layers.10.mlp.down_proj.weight', 'decoder.model.layers.10.input_layernorm.weight', 'decoder.model.layers.10.post_attention_layernorm.weight', 'decoder.model.layers.11.self_attn.q_proj.weight', 'decoder.model.layers.11.self_attn.k_proj.weight', 'decoder.model.layers.11.self_attn.v_proj.weight', 'decoder.model.layers.11.self_attn.o_proj.weight', 'decoder.model.layers.11.mlp.gate_proj.weight', 'decoder.model.layers.11.mlp.up_proj.weight', 'decoder.model.layers.11.mlp.down_proj.weight', 'decoder.model.layers.11.input_layernorm.weight', 'decoder.model.layers.11.post_attention_layernorm.weight', 'decoder.model.layers.12.self_attn.q_proj.weight', 'decoder.model.layers.12.self_attn.k_proj.weight', 'decoder.model.layers.12.self_attn.v_proj.weight', 'decoder.model.layers.12.self_attn.o_proj.weight', 'decoder.model.layers.12.mlp.gate_proj.weight', 'decoder.model.layers.12.mlp.up_proj.weight', 'decoder.model.layers.12.mlp.down_proj.weight', 'decoder.model.layers.12.input_layernorm.weight', 'decoder.model.layers.12.post_attention_layernorm.weight', 'decoder.model.layers.13.self_attn.q_proj.weight', 'decoder.model.layers.13.self_attn.k_proj.weight', 'decoder.model.layers.13.self_attn.v_proj.weight', 'decoder.model.layers.13.self_attn.o_proj.weight', 'decoder.model.layers.13.mlp.gate_proj.weight', 'decoder.model.layers.13.mlp.up_proj.weight', 'decoder.model.layers.13.mlp.down_proj.weight', 'decoder.model.layers.13.input_layernorm.weight', 'decoder.model.layers.13.post_attention_layernorm.weight', 'decoder.model.layers.14.self_attn.q_proj.weight', 'decoder.model.layers.14.self_attn.k_proj.weight', 'decoder.model.layers.14.self_attn.v_proj.weight', 'decoder.model.layers.14.self_attn.o_proj.weight', 'decoder.model.layers.14.mlp.gate_proj.weight', 'decoder.model.layers.14.mlp.up_proj.weight', 'decoder.model.layers.14.mlp.down_proj.weight', 'decoder.model.layers.14.input_layernorm.weight', 'decoder.model.layers.14.post_attention_layernorm.weight', 'decoder.model.layers.15.self_attn.q_proj.weight', 'decoder.model.layers.15.self_attn.k_proj.weight', 'decoder.model.layers.15.self_attn.v_proj.weight', 'decoder.model.layers.15.self_attn.o_proj.weight', 'decoder.model.layers.15.mlp.gate_proj.weight', 'decoder.model.layers.15.mlp.up_proj.weight', 'decoder.model.layers.15.mlp.down_proj.weight', 'decoder.model.layers.15.input_layernorm.weight', 'decoder.model.layers.15.post_attention_layernorm.weight', 'decoder.model.layers.16.self_attn.q_proj.weight', 'decoder.model.layers.16.self_attn.k_proj.weight', 'decoder.model.layers.16.self_attn.v_proj.weight', 'decoder.model.layers.16.self_attn.o_proj.weight', 'decoder.model.layers.16.mlp.gate_proj.weight', 'decoder.model.layers.16.mlp.up_proj.weight', 'decoder.model.layers.16.mlp.down_proj.weight', 'decoder.model.layers.16.input_layernorm.weight', 'decoder.model.layers.16.post_attention_layernorm.weight', 'decoder.model.layers.17.self_attn.q_proj.weight', 'decoder.model.layers.17.self_attn.k_proj.weight', 'decoder.model.layers.17.self_attn.v_proj.weight', 'decoder.model.layers.17.self_attn.o_proj.weight', 'decoder.model.layers.17.mlp.gate_proj.weight', 'decoder.model.layers.17.mlp.up_proj.weight', 'decoder.model.layers.17.mlp.down_proj.weight', 'decoder.model.layers.17.input_layernorm.weight', 'decoder.model.layers.17.post_attention_layernorm.weight', 'decoder.model.layers.18.self_attn.q_proj.weight', 'decoder.model.layers.18.self_attn.k_proj.weight', 'decoder.model.layers.18.self_attn.v_proj.weight', 'decoder.model.layers.18.self_attn.o_proj.weight', 'decoder.model.layers.18.mlp.gate_proj.weight', 'decoder.model.layers.18.mlp.up_proj.weight', 'decoder.model.layers.18.mlp.down_proj.weight', 'decoder.model.layers.18.input_layernorm.weight', 'decoder.model.layers.18.post_attention_layernorm.weight', 'decoder.model.layers.19.self_attn.q_proj.weight', 'decoder.model.layers.19.self_attn.k_proj.weight', 'decoder.model.layers.19.self_attn.v_proj.weight', 'decoder.model.layers.19.self_attn.o_proj.weight', 'decoder.model.layers.19.mlp.gate_proj.weight', 'decoder.model.layers.19.mlp.up_proj.weight', 'decoder.model.layers.19.mlp.down_proj.weight', 'decoder.model.layers.19.input_layernorm.weight', 'decoder.model.layers.19.post_attention_layernorm.weight', 'decoder.model.layers.20.self_attn.q_proj.weight', 'decoder.model.layers.20.self_attn.k_proj.weight', 'decoder.model.layers.20.self_attn.v_proj.weight', 'decoder.model.layers.20.self_attn.o_proj.weight', 'decoder.model.layers.20.mlp.gate_proj.weight', 'decoder.model.layers.20.mlp.up_proj.weight', 'decoder.model.layers.20.mlp.down_proj.weight', 'decoder.model.layers.20.input_layernorm.weight', 'decoder.model.layers.20.post_attention_layernorm.weight', 'decoder.model.layers.21.self_attn.q_proj.weight', 'decoder.model.layers.21.self_attn.k_proj.weight', 'decoder.model.layers.21.self_attn.v_proj.weight', 'decoder.model.layers.21.self_attn.o_proj.weight', 'decoder.model.layers.21.mlp.gate_proj.weight', 'decoder.model.layers.21.mlp.up_proj.weight', 'decoder.model.layers.21.mlp.down_proj.weight', 'decoder.model.layers.21.input_layernorm.weight', 'decoder.model.layers.21.post_attention_layernorm.weight', 'decoder.model.layers.22.self_attn.q_proj.weight', 'decoder.model.layers.22.self_attn.k_proj.weight', 'decoder.model.layers.22.self_attn.v_proj.weight', 'decoder.model.layers.22.self_attn.o_proj.weight', 'decoder.model.layers.22.mlp.gate_proj.weight', 'decoder.model.layers.22.mlp.up_proj.weight', 'decoder.model.layers.22.mlp.down_proj.weight', 'decoder.model.layers.22.input_layernorm.weight', 'decoder.model.layers.22.post_attention_layernorm.weight', 'decoder.model.layers.23.self_attn.q_proj.weight', 'decoder.model.layers.23.self_attn.k_proj.weight', 'decoder.model.layers.23.self_attn.v_proj.weight', 'decoder.model.layers.23.self_attn.o_proj.weight', 'decoder.model.layers.23.mlp.gate_proj.weight', 'decoder.model.layers.23.mlp.up_proj.weight', 'decoder.model.layers.23.mlp.down_proj.weight', 'decoder.model.layers.23.input_layernorm.weight', 'decoder.model.layers.23.post_attention_layernorm.weight', 'decoder.model.layers.24.self_attn.q_proj.weight', 'decoder.model.layers.24.self_attn.k_proj.weight', 'decoder.model.layers.24.self_attn.v_proj.weight', 'decoder.model.layers.24.self_attn.o_proj.weight', 'decoder.model.layers.24.mlp.gate_proj.weight', 'decoder.model.layers.24.mlp.up_proj.weight', 'decoder.model.layers.24.mlp.down_proj.weight', 'decoder.model.layers.24.input_layernorm.weight', 'decoder.model.layers.24.post_attention_layernorm.weight', 'decoder.model.layers.25.self_attn.q_proj.weight', 'decoder.model.layers.25.self_attn.k_proj.weight', 'decoder.model.layers.25.self_attn.v_proj.weight', 'decoder.model.layers.25.self_attn.o_proj.weight', 'decoder.model.layers.25.mlp.gate_proj.weight', 'decoder.model.layers.25.mlp.up_proj.weight', 'decoder.model.layers.25.mlp.down_proj.weight', 'decoder.model.layers.25.input_layernorm.weight', 'decoder.model.layers.25.post_attention_layernorm.weight', 'decoder.model.layers.26.self_attn.q_proj.weight', 'decoder.model.layers.26.self_attn.k_proj.weight', 'decoder.model.layers.26.self_attn.v_proj.weight', 'decoder.model.layers.26.self_attn.o_proj.weight', 'decoder.model.layers.26.mlp.gate_proj.weight', 'decoder.model.layers.26.mlp.up_proj.weight', 'decoder.model.layers.26.mlp.down_proj.weight', 'decoder.model.layers.26.input_layernorm.weight', 'decoder.model.layers.26.post_attention_layernorm.weight', 'decoder.model.layers.27.self_attn.q_proj.weight', 'decoder.model.layers.27.self_attn.k_proj.weight', 'decoder.model.layers.27.self_attn.v_proj.weight', 'decoder.model.layers.27.self_attn.o_proj.weight', 'decoder.model.layers.27.mlp.gate_proj.weight', 'decoder.model.layers.27.mlp.up_proj.weight', 'decoder.model.layers.27.mlp.down_proj.weight', 'decoder.model.layers.27.input_layernorm.weight', 'decoder.model.layers.27.post_attention_layernorm.weight', 'decoder.model.layers.28.self_attn.q_proj.weight', 'decoder.model.layers.28.self_attn.k_proj.weight', 'decoder.model.layers.28.self_attn.v_proj.weight', 'decoder.model.layers.28.self_attn.o_proj.weight', 'decoder.model.layers.28.mlp.gate_proj.weight', 'decoder.model.layers.28.mlp.up_proj.weight', 'decoder.model.layers.28.mlp.down_proj.weight', 'decoder.model.layers.28.input_layernorm.weight', 'decoder.model.layers.28.post_attention_layernorm.weight', 'decoder.model.layers.29.self_attn.q_proj.weight', 'decoder.model.layers.29.self_attn.k_proj.weight', 'decoder.model.layers.29.self_attn.v_proj.weight', 'decoder.model.layers.29.self_attn.o_proj.weight', 'decoder.model.layers.29.mlp.gate_proj.weight', 'decoder.model.layers.29.mlp.up_proj.weight', 'decoder.model.layers.29.mlp.down_proj.weight', 'decoder.model.layers.29.input_layernorm.weight', 'decoder.model.layers.29.post_attention_layernorm.weight', 'decoder.model.layers.30.self_attn.q_proj.weight', 'decoder.model.layers.30.self_attn.k_proj.weight', 'decoder.model.layers.30.self_attn.v_proj.weight', 'decoder.model.layers.30.self_attn.o_proj.weight', 'decoder.model.layers.30.mlp.gate_proj.weight', 'decoder.model.layers.30.mlp.up_proj.weight', 'decoder.model.layers.30.mlp.down_proj.weight', 'decoder.model.layers.30.input_layernorm.weight', 'decoder.model.layers.30.post_attention_layernorm.weight', 'decoder.model.layers.31.self_attn.q_proj.weight', 'decoder.model.layers.31.self_attn.k_proj.weight', 'decoder.model.layers.31.self_attn.v_proj.weight', 'decoder.model.layers.31.self_attn.o_proj.weight', 'decoder.model.layers.31.mlp.gate_proj.weight', 'decoder.model.layers.31.mlp.up_proj.weight', 'decoder.model.layers.31.mlp.down_proj.weight', 'decoder.model.layers.31.input_layernorm.weight', 'decoder.model.layers.31.post_attention_layernorm.weight', 'decoder.model.norm.weight', 'decoder.lm_head.weight'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_dict = {}\n",
    "\n",
    "model = ScigptModel(args)\n",
    "# model = AutoModelForCausalLM.from_pretrained(args.save_dir)\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "print(f\"model_dict: {model_dict.keys()}\")\n",
    "\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer0.items():\n",
    "    new_k = \"decoder.model.\" + k\n",
    "    ckpt_dict[new_k] = v\n",
    "\n",
    "# layer1 = torch.load(os.path.join(args.save_dir, \"layer_01-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "# ckpt_dict['embed_tokens.weight'] = layer1['embed_tokens.weight']\n",
    "\n",
    "# layer2 = torch.load(os.path.join(args.save_dir, \"layer_02-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "# for k, v in layer2.items():\n",
    "#     new_k = \"adaptor.\" + k\n",
    "#     ckpt_dict[new_k] = v\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 1).zfill(2)\n",
    "    layer = torch.load(os.path.join(args.save_dir, f\"layer_{l_index}-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        ckpt_dict[f\"decoder.model.layers.{l}.{k}\"] = layer[k]\n",
    "    del layer\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_33-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_34-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "\n",
    "print(f\"ckpt_dict: {ckpt_dict.keys()}\")\n",
    "model_dict.update(ckpt_dict)\n",
    "model.load_state_dict(model_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fastdata/peiran/tmp/ipykernel_958096/1253975920.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids=torch.tensor(tokenizer.encode(\"what to eat\", return_tensors=\"pt\")).to(device),\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what to eat, what to drink, where to go, how to get there, what to see, what to do, where to stay, where to shop, what to buy, where to eat, what to drink, where to go, how to get there, what to see, what to do, where to stay, where to shop, what to buy, where to eat, what to drink, where to go, how to get there, what to see, what to do, where to stay, where to shop, what to buy, where to eat, what to drink, where to go, how to get there, what to see, what to do, where to stay, where to shop, what to buy, where to eat, what to drink, where to go, how to get there, what to see, what to do, where to stay, where to shop, what to buy, where to eat, what to drink, where to go, how to get there, what to see, what to do, where to stay, where to shop, what to buy, where to eat, what to drink, where to go, how to get there, what to see, what to do, where to stay, where to shop, what to buy, where to eat, what to drink, where to go, how to get there, what to see, what to do, where to stay, where to shop, what to buy, where to eat, what to drink, where to go, how to get there, what to see, what to do, where to stay, where to shop, what to buy, where to eat, what to drink, where to go, how to get there, what to see, what to do, where to stay, where to shop, what to buy, where to eat, what to drink, where to go, how to get there, what to see, what to do, where to stay, where to shop, what to buy, where to eat, what to drink, where to go, how to get there, what to see, what to do, where to stay, where to shop, what to buy, where to eat, what to drink, where to go, how to get there, what to see, what to do, where to stay, where to shop, what to buy, where to eat, what to drink, where to go, how to get there, what to see, what to do, where to stay, where to shop, what\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "model = model.to(torch.bfloat16).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# print(f\"input: {text},\\n output: {res}\")\n",
    "\n",
    "# # output = model.generate(\n",
    "# #     input_ids=batched_data['input_ids'],\n",
    "# #     num_return_sequences=10,\n",
    "# #     num_beams=20,\n",
    "# # )\n",
    "# for i in range(10):\n",
    "#     print(tokenizer.decode(output[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fastdata/peiran/tmp/ipykernel_958096/1817206588.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids=torch.tensor(tokenizer.encode(\"Football is a \", return_tensors=\"pt\")).to(device),\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Football is a 90-minute game. Ninety minutes is all you get. If you don't win in 90 minutes, you deserve to lose.\"\n",
      "\"Winning isn't everything, but wanting to win is.\"\n",
      "\"The only way to be truly satisfied is to do what you believe is great work. And the only way to do great work is to love what you do. If you haven't found it yet, keep looking. Don't settle. As with all matters of the heart, you'll know when you find it. And, like any great relationship, it just gets better and better as the years roll on. So keep looking until you find it. Don't settle.\"\n",
      "\"I've missed more than 9000 shots in my career. I've lost almost 300 games. 26 times, I've been trusted to take the game winning shot and missed. I've failed over and over and over again in my life. And that is why I succeed.\"\n",
      "\"I can accept failure, everyone fails at something. But I can't accept not trying.\"\n",
      "\"Whatever you do, you need courage. Whatever course you decide upon, there is always someone to tell you that you are wrong. There are always difficulties arising that tempt you to believe your critics are right. To map out a course of action and follow it to an end requires some of the same courage that a soldier needs. Peace has its victories, but it takes brave men and women to win them.\"\n",
      "\"Success is not final, failure is not fatal: it is the courage to continue that counts.\"\n",
      "\"Courage is not the absence of fear, but rather the judgment that something else is more important than fear. The brave may not live forever, but the cautious do not live at all.\"\n",
      "\"It is not the critic who counts; not the man who points out how the strong man stumbles, or where the doer of deeds could have done them better. The credit belongs to the man who is actually in the arena, whose face is marred by dust and sweat and blood; who strives valiantly; who errs, who comes short again and again, because there is no effort without error and shortcoming; but who does actually strive to do the deeds; who knows great enthusiasms, the great devotions; who spends himself in a worthy cause; who at the best knows in the end the triumph of high achievement, and who at the worst, if he fails, at least fails while daring greatly, so that his place shall never be with those cold and timid souls who\n"
     ]
    }
   ],
   "source": [
    "output = model.decoder.generate(\n",
    "    input_ids=torch.tensor(tokenizer.encode(\"Football is a \", return_tensors=\"pt\")).to(device),\n",
    "    num_beams=5,\n",
    "    max_new_tokens=512,\n",
    "    num_return_sequences=1,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.5,\n",
    ")\n",
    "res = tokenizer.decode(output.sequences[0], skip_special_tokens=False)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "ckpt = torch.load(\"/data/peiran/blob/hai1data/sfm/pfmexp/output/stageB/global_step12386/layer_01-model_states.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
