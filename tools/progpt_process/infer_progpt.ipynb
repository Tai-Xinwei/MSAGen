{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fastdata/peiran/miniconda3/envs/sfm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 07:29:51,771] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[\u001b[32m2024-04-20 07:29:54.037\u001b[0m][\u001b[36mINFO\u001b[0m]: flash_attn not installed, use default attn\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import transformers\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sfm.models.progpt.progpt import ProGPTModel\n",
    "from sfm.models.progpt.progpt_config import ProGPTConfig\n",
    "from sfm.models.pfm.pfm_config import PFMConfig\n",
    "from sfm.data.sci_data.SFMDecTokenizer import SFMDecTokenizer\n",
    "from sfm.utils import arg_utils\n",
    "from sfm.utils.science_tokens import SCIENCE_TAG_TOKENS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[32m2024-04-20 07:29:54.064\u001b[0m][\u001b[36mWARNING\u001b[0m]: Duplicate config name: train_data_path, not added to parser\n",
      "[\u001b[32m2024-04-20 07:29:54.064\u001b[0m][\u001b[36mWARNING\u001b[0m]: Duplicate config name: valid_data_path, not added to parser\n",
      "[\u001b[32m2024-04-20 07:29:54.601\u001b[0m][\u001b[36mINFO\u001b[0m]: Trainer args: Namespace(num_classes=1, encoder_attention_heads=20, encoder_ffn_embed_dim=5120, encoder_embed_dim=1280, encoder_layers=33, num_pred_attn_layer=4, num_3d_bias_kernel=128, max_length=1024, pbc_expanded_token_cutoff=512, pbc_expanded_num_cell_per_direction=10, multi_hop_max_dist=20, droppath_prob=0.0, act_dropout=0.0, attn_dropout=0.0, dropout=0.0, sandwich_ln=True, noise_scale=0.2, mask_ratio=0.5, d_tilde=1.0, pbc_cutoff=40.0, data_path='', dataset_names='', loadcheck_path='', add_3d=False, no_2d=False, ft=False, infer=False, use_pbc=False, transformer_m_pretrain=True, mode_prob='0.6,0.2,0.2', num_timesteps=1000, ddpm_beta_start=0.0001, ddpm_beta_end=0.02, ddpm_schedule='linear', noise_mode='const', num_edges=1536, num_atom_features=5120, task_name='', data_basepath='', output_dim=1024, add_rope=True, flash_attn=False, stack_seq=False, num_residues=32, max_num_aa=1024, task='mae', mask_prob=0.15, train_data_path='', valid_data_path='', dataset_splits='', dataset_ratios='', pool_mode='full', embedding_length=20, model_max_length=512, smiles_dict_path='', loadbfmckpt_path='', llm_model_name_or_path='/data/peiran/blob/msralaphilly2/ml-la/v-kehanwu/SFM/scigpt/stageB.prot/global_step224655', mol_size_path='', tokenizer_path='/data/peiran/blob/msralaphilly2/ml-la/shufxi/data/scigpt', mfm_lora=False, llm_lora=False, btn_adaptor=False, fused_graphormer_llama=False, add_mol_attn_bias_in_llama=False, mol_attn_bias_in_llama_layerwise=False, path_edge_cutoff=0, skip_num_datasets='', num_data_loading_workers=16, use_global_padding=False, max_num_mol_per_sample=8, protein_max_size=1024, use_llama_tokenizer=False, load_ckpt=False, strategy='DDP', save_dir='/data/peiran/blob/msralaphilly2/ml-la/v-kehanwu/nlm/checkpoints/bfm_llama/global_step11499', attention_dropout=0.0, activation_fn='gelu', encoder_normalize_before=True, apply_graphormer_init=True, share_encoder_input_output_embed=False, no_token_positional_embeddings=False, pre_layernorm=False, encoder_learned_pos=False, num_segments=2, sentence_class_num=2, sent_loss=False, apply_bert_init=False, pooler_activation_fn='tanh', atom_loss_coeff=1.0, pos_loss_coeff=1.0, y_2d_loss_coeff=1.0, max_positions=512, num_atoms=4608, num_in_degree=512, num_out_degree=512, num_spatial=512, num_edge_dis=128, edge_type='multi_hop', layerdrop=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def get_args_and_tokenizer(use_llama=False):\n",
    "    parser = ArgumentParser()\n",
    "    cfg_classes = [PFMConfig, ProGPTConfig]\n",
    "    parser = arg_utils.add_dataclass_to_parser(cfg_classes, parser)\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.load_ckpt = False\n",
    "    args.strategy = \"DDP\"\n",
    "    args.encoder_layers = 33\n",
    "    args.encoder_embed_dim = 1280\n",
    "    args.encoder_ffn_embed_dim = 5120\n",
    "    args.encoder_attention_heads = 20\n",
    "    # args.fp16 = True\n",
    "    \n",
    "    mount_dir = \"/data/peiran/blob/msralaphilly2/ml-la\"\n",
    "    if not use_llama:\n",
    "        args.llm_model_name_or_path = mount_dir+\"/v-kehanwu/SFM/scigpt/stageB.prot/global_step224655\"\n",
    "        args.tokenizer_path = mount_dir+\"/shufxi/data/scigpt\"\n",
    "        args.save_dir = mount_dir+'/v-kehanwu/nlm/checkpoints/bfm_scigpt_prot/global_step11499'\n",
    "        # args.save_dir = \"/fastdata/peiran/nlm/checkpoints/stageB.prot/global_step1\"\n",
    "\n",
    "        tokenizer = SFMDecTokenizer.from_pretrained(\n",
    "            args.llm_model_name_or_path,\n",
    "            prot_spm_path=os.path.join(args.tokenizer_path, \"ur50bpe/bpe\"),\n",
    "            dna_spm_path=os.path.join(args.tokenizer_path, \"dnabpe/bpe\"),\n",
    "            rna_spm_path=os.path.join(args.tokenizer_path, \"rnabpe/bpe\"),\n",
    "        )\n",
    "        args.vocab_size = len(tokenizer)  # now we have new tokens\n",
    "        args.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        args.llm_model_name_or_path = mount_dir+\"/v-kehanwu/SFM/scigpt/stageB.prot/global_step224655\"\n",
    "        args.tokenizer_path = mount_dir+\"/shufxi/data/scigpt\"\n",
    "        args.save_dir = mount_dir+'/v-kehanwu/nlm/checkpoints/bfm_llama/global_step11499'\n",
    "        # args.save_dir = \"/fastdata/peiran/nlm/checkpoints/stageB.prot/global_step1\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            args.llm_model_name_or_path,\n",
    "            model_max_length=args.model_max_length,\n",
    "            padding_side=\"right\",\n",
    "            use_fast=False,\n",
    "        )\n",
    "\n",
    "        special_tokens_dict = dict()\n",
    "        if tokenizer.pad_token is None:\n",
    "            special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "        if tokenizer.eos_token is None:\n",
    "            special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "        if tokenizer.bos_token is None:\n",
    "            special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "        if tokenizer.unk_token is None:\n",
    "            special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "        special_tokens_dict[\"additional_special_tokens\"] = SCIENCE_TAG_TOKENS\n",
    "        tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        \n",
    "\n",
    "    return args, tokenizer\n",
    "\n",
    "args, tokenizer = get_args_and_tokenizer(use_llama=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_dict = {}\n",
    "\n",
    "model = ProGPTModel(args, len(tokenizer))\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer0.items():\n",
    "    new_k = \"pfm_encoder.\" + k\n",
    "    ckpt_dict[new_k] = v\n",
    "\n",
    "layer1 = torch.load(os.path.join(args.save_dir, \"layer_01-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict['decoder.model.embed_tokens.weight'] = layer1['embed_tokens.weight']\n",
    "\n",
    "layer2 = torch.load(os.path.join(args.save_dir, \"layer_02-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer2.items():\n",
    "    new_k = \"adaptor.\" + k\n",
    "    ckpt_dict[new_k] = v\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 3).zfill(2)\n",
    "    layer = torch.load(os.path.join(args.save_dir, f\"layer_{l_index}-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        ckpt_dict[f\"decoder.model.layers.{l}.{k}\"] = layer[k]\n",
    "    del layer\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_35-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_36-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "\n",
    "model_dict.update(ckpt_dict)\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "model.load_state_dict(model_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scigpt_vacab = {'L': 33874, 'A': 33875, 'G': 33878, 'V': 33877, 'S': 33876, 'E': 33879, 'R': 33880, 'T': 33881, 'I': 33882, 'D': 33884, 'P': 33886, 'K': 33883, 'Q': 33885, 'N': 33887, 'F': 33888, 'Y': 33890, 'M': 33873, 'H': 33889, 'W': 33891, 'C': 33892, 'X': 34276, 'B': 37965, 'U': 37967, 'Z': 37966, 'O': 0}\n",
    "\n",
    "vocab = {'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7, 'S': 8, 'E': 9, 'R': 10, 'T': 11, 'I': 12, 'D': 13, 'P': 14, 'K': 15, 'Q': 16, 'N': 17, 'F': 18, 'Y': 19, 'M': 20, 'H': 21, 'W': 22, 'C': 23, 'X': 24, 'B': 25, 'U': 26, 'Z': 27, 'O': 28, '.': 29, '-': 30, '<mask>': 31}\n",
    "\n",
    "def protein_process(protein):\n",
    "    protein_id = [vocab[tok] for tok in protein]\n",
    "    protein_bpe_id = [scigpt_vacab[tok] for tok in protein]\n",
    "\n",
    "    return protein_id, protein_bpe_id\n",
    "\n",
    "def process(text):\n",
    "    # find the part of protein seq that surrounded by <protein> and </protein> in text\n",
    "    protein = []\n",
    "    res = []\n",
    "    text1 = text.split(\"<protein>\")\n",
    "    res.append(text1[0])\n",
    "    for i in range(1, len(text1)):\n",
    "        text2 = text1[i].split(\"</protein>\")\n",
    "        protein.append(text2[0])\n",
    "        res.append(text2[1])\n",
    "\n",
    "    return res, protein\n",
    "\n",
    "def tokenize(text):\n",
    "    # split text with <protein> and </protein>\n",
    "    text_list, protein = process(text)\n",
    "    protein_id_list = []\n",
    "    protein_bpe_id_list = []\n",
    "\n",
    "    if len(protein) == 0:\n",
    "        return tokenizer.encode(text), protein_id_list, protein_bpe_id_list\n",
    "    else:\n",
    "        for p in protein:\n",
    "            protein_id, protein_bpe_id = protein_process(p)\n",
    "            protein_id_list.append(protein_id)\n",
    "            protein_bpe_id_list.append(protein_bpe_id)\n",
    "\n",
    "    input_ids = []\n",
    "    for i in range(len(text_list)):\n",
    "        if i == 0:\n",
    "            input_ids.extend(tokenizer.encode(text_list[i] + \" <protein>\"))\n",
    "        elif i != len(text_list) - 1:\n",
    "            input_ids.append(-1)\n",
    "            input_ids.extend(tokenizer.encode(\"</protein> \" + text_list[i] + \" <protein>\")[1:])\n",
    "        else:\n",
    "            input_ids.append(-1)\n",
    "            input_ids.extend(tokenizer.encode(\"</protein> \" + text_list[i])[1:])\n",
    "\n",
    "    return input_ids, protein_id_list, protein_bpe_id_list\n",
    "\n",
    "\n",
    "def collator(input_ids, protein_id_list, protein_bpe_id_list, device):\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int64)\n",
    "    for i in range(len(protein_bpe_id_list)):\n",
    "        protein_bpe_id_list[i] = torch.tensor(protein_bpe_id_list[i], dtype=torch.int64)\n",
    "\n",
    "    new_input_ids = []\n",
    "    original_input_ids_len = len(input_ids)\n",
    "    input_ids_len = len(input_ids)\n",
    "    mol_pos = torch.nonzero(input_ids < 0).squeeze(-1)\n",
    "    mol_pos = torch.cat(\n",
    "        [torch.tensor([0]), mol_pos, torch.tensor([len(input_ids)])]\n",
    "    )\n",
    "\n",
    "    for i in range(mol_pos.size(0) - 1):\n",
    "        if i == 0:\n",
    "            new_input_ids.extend(input_ids[mol_pos[i] : mol_pos[i + 1]])\n",
    "        else:\n",
    "            new_input_ids.extend(input_ids[mol_pos[i] + 1 : mol_pos[i + 1]])\n",
    "\n",
    "        if i < len(mol_pos) - 2:\n",
    "            len_protein = len(protein_id_list[i])\n",
    "            mol_idx = input_ids[mol_pos[i + 1]]\n",
    "            if len_protein > 1:\n",
    "                new_input_ids.extend(torch.ones([len_protein]) * mol_idx)\n",
    "            if mol_pos[i + 1] < original_input_ids_len:\n",
    "                input_ids_len += len_protein - 1\n",
    "\n",
    "\n",
    "    input_ids = torch.tensor(new_input_ids).to(dtype=torch.int64).unsqueeze(0)\n",
    "    if len(protein_id_list) == 0:\n",
    "        protein = torch.tensor([0, 2]).to(dtype=torch.int64).unsqueeze(0)\n",
    "    else:\n",
    "        protein = torch.tensor(protein_id_list[0]).to(dtype=torch.int64)\n",
    "        protein = torch.cat([torch.tensor([0]).to(dtype=torch.int64), protein, torch.tensor([2]).to(dtype=torch.int64)]).unsqueeze(0)\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids.to(device),\n",
    "        proteins=protein.long().to(device),\n",
    "        llm_mask=input_ids.ne(tokenizer.pad_token_id).to(device),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "from sfm.data.prot_data.util import bstr2obj\n",
    "import pickle as pkl\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "data_path = '/fastdata/peiran/nlm/progpt_valid_bpe.lmdb/'\n",
    "env = lmdb.open(\n",
    "    str(data_path), subdir=True, readonly=True, lock=False, readahead=False\n",
    ")\n",
    "txn = env.begin(write=False)\n",
    "metadata = bstr2obj(txn.get(\"metadata\".encode()))\n",
    "size, keys = metadata[\"size\"], metadata[\"keys\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 364])\n"
     ]
    }
   ],
   "source": [
    "key = keys[-1]\n",
    "value = txn.get(str(key).encode())\n",
    "input_ids, proteins, proteins_bpeid = pkl.loads(value)\n",
    "\n",
    "batched_data = collator(input_ids, proteins, proteins_bpeid, device)\n",
    "\n",
    "print(f\"input_ids: {batched_data['input_ids'].shape}\")\n",
    "# batched_data['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "text = \"Describe the <protein>AAAGSGAGU</protein> .\"\n",
    "# text = \"Hello, what to eat tonight?\"\n",
    "\n",
    "input_ids, protein_id_list, protein_bpe_id_list = tokenize(text)\n",
    "input_ids = tokenizer.encode(text)\n",
    "batched_data = collator(input_ids, protein_id_list, protein_bpe_id_list, device)\n",
    "\n",
    "batched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input_ids: torch.Size([1, 364])\n",
      "torch.Size([220, 1, 1280]) torch.Size([1, 220]) torch.Size([1, 364, 4096]) torch.Size([1, 364]) torch.Size([1, 364])\n",
      "torch.Size([1, 364, 4096])\n",
      "The similarity of this protein : Belongs to the muscleblind family. The sequence caution of this protein : Sequence=AACAAGAAGAAGAAGGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAAGAA\n"
     ]
    }
   ],
   "source": [
    "model = model.to(torch.float16).to(device)\n",
    "print(f\"shape of input_ids: {batched_data['input_ids'].shape}\")\n",
    "model.eval()\n",
    "output = model.generate(\n",
    "    batched_data,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=300,\n",
    "    num_return_sequences=1,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.5\n",
    ")\n",
    "res = tokenizer.decode(output.sequences[0])\n",
    "print(res)\n",
    "\n",
    "# # output = model.generate(\n",
    "# #     input_ids=batched_data['input_ids'],\n",
    "# #     num_return_sequences=10,\n",
    "# #     num_beams=20,\n",
    "# # )\n",
    "# for i in range(10):\n",
    "#     print(tokenizer.decode(output[i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
