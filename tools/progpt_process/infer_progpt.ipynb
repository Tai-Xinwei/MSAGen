{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fastdata/peiran/miniconda3/envs/sfm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 07:40:07,939] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[\u001b[32m2024-04-20 07:40:10.106\u001b[0m][\u001b[36mINFO\u001b[0m]: flash_attn not installed, use default attn\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import transformers\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sfm.models.progpt.progpt import ProGPTModel\n",
    "from sfm.models.progpt.progpt_config import ProGPTConfig\n",
    "from sfm.models.pfm.pfm_config import PFMConfig\n",
    "from sfm.data.sci_data.SFMDecTokenizer import SFMDecTokenizer\n",
    "from sfm.utils import arg_utils\n",
    "from sfm.utils.science_tokens import SCIENCE_TAG_TOKENS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[32m2024-04-20 07:40:10.129\u001b[0m][\u001b[36mWARNING\u001b[0m]: Duplicate config name: train_data_path, not added to parser\n",
      "[\u001b[32m2024-04-20 07:40:10.129\u001b[0m][\u001b[36mWARNING\u001b[0m]: Duplicate config name: valid_data_path, not added to parser\n",
      "[\u001b[32m2024-04-20 07:40:10.513\u001b[0m][\u001b[36mINFO\u001b[0m]: Loading protein sentencepiece model from /data/peiran/blob/msralaphilly2/ml-la/shufxi/data/scigpt/ur50bpe/bpe.model and /data/peiran/blob/msralaphilly2/ml-la/shufxi/data/scigpt/ur50bpe/bpe.vocab\n",
      "[\u001b[32m2024-04-20 07:40:10.618\u001b[0m][\u001b[36mINFO\u001b[0m]: Loading DNA sentencepiece model from /data/peiran/blob/msralaphilly2/ml-la/shufxi/data/scigpt/dnabpe/bpe.model and /data/peiran/blob/msralaphilly2/ml-la/shufxi/data/scigpt/dnabpe/bpe.vocab\n",
      "[\u001b[32m2024-04-20 07:40:10.662\u001b[0m][\u001b[36mINFO\u001b[0m]: Loading RNA sentencepiece model from /data/peiran/blob/msralaphilly2/ml-la/shufxi/data/scigpt/rnabpe/bpe.model and /data/peiran/blob/msralaphilly2/ml-la/shufxi/data/scigpt/rnabpe/bpe.vocab\n",
      "[\u001b[32m2024-04-20 07:40:10.974\u001b[0m][\u001b[36mINFO\u001b[0m]: Tokenizer has 40014 tokens\n",
      "[\u001b[32m2024-04-20 07:40:11.045\u001b[0m][\u001b[36mINFO\u001b[0m]: Trainer args: Namespace(num_classes=1, encoder_attention_heads=20, encoder_ffn_embed_dim=5120, encoder_embed_dim=1280, encoder_layers=33, num_pred_attn_layer=4, num_3d_bias_kernel=128, max_length=1024, pbc_expanded_token_cutoff=512, pbc_expanded_num_cell_per_direction=10, multi_hop_max_dist=20, droppath_prob=0.0, act_dropout=0.0, attn_dropout=0.0, dropout=0.0, sandwich_ln=True, noise_scale=0.2, mask_ratio=0.5, d_tilde=1.0, pbc_cutoff=40.0, data_path='', dataset_names='', loadcheck_path='', add_3d=False, no_2d=False, ft=False, infer=False, use_pbc=False, transformer_m_pretrain=True, mode_prob='0.6,0.2,0.2', num_timesteps=1000, ddpm_beta_start=0.0001, ddpm_beta_end=0.02, ddpm_schedule='linear', noise_mode='const', num_edges=1536, num_atom_features=5120, task_name='', data_basepath='', output_dim=1024, add_rope=True, flash_attn=False, stack_seq=False, num_residues=32, max_num_aa=1024, task='mae', mask_prob=0.15, train_data_path='', valid_data_path='', dataset_splits='', dataset_ratios='', pool_mode='full', embedding_length=20, model_max_length=512, smiles_dict_path='', loadbfmckpt_path='', llm_model_name_or_path='/data/peiran/blob/msralaphilly2/ml-la/v-kehanwu/SFM/scigpt/stageB.prot/global_step224655', mol_size_path='', tokenizer_path='/data/peiran/blob/msralaphilly2/ml-la/shufxi/data/scigpt', mfm_lora=False, llm_lora=False, btn_adaptor=False, fused_graphormer_llama=False, add_mol_attn_bias_in_llama=False, mol_attn_bias_in_llama_layerwise=False, path_edge_cutoff=0, skip_num_datasets='', num_data_loading_workers=16, use_global_padding=False, max_num_mol_per_sample=8, protein_max_size=1024, use_llama_tokenizer=False, load_ckpt=False, strategy='DDP', save_dir='/data/peiran/blob/msralaphilly2/ml-la/v-kehanwu/nlm/checkpoints/bfm_scigpt_prot/global_step11499', vocab_size=40014, pad_token_id=32000, attention_dropout=0.0, activation_fn='gelu', encoder_normalize_before=True, apply_graphormer_init=True, share_encoder_input_output_embed=False, no_token_positional_embeddings=False, pre_layernorm=False, encoder_learned_pos=False, num_segments=2, sentence_class_num=2, sent_loss=False, apply_bert_init=False, pooler_activation_fn='tanh', atom_loss_coeff=1.0, pos_loss_coeff=1.0, y_2d_loss_coeff=1.0, max_positions=512, num_atoms=4608, num_in_degree=512, num_out_degree=512, num_spatial=512, num_edge_dis=128, edge_type='multi_hop', layerdrop=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'SFMDecTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'sfm.data.sci_data.SFMDecTokenizer.SFMDecTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def get_args_and_tokenizer(use_llama=False):\n",
    "    parser = ArgumentParser()\n",
    "    cfg_classes = [PFMConfig, ProGPTConfig]\n",
    "    parser = arg_utils.add_dataclass_to_parser(cfg_classes, parser)\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.load_ckpt = False\n",
    "    args.strategy = \"DDP\"\n",
    "    args.encoder_layers = 33\n",
    "    args.encoder_embed_dim = 1280\n",
    "    args.encoder_ffn_embed_dim = 5120\n",
    "    args.encoder_attention_heads = 20\n",
    "    # args.fp16 = True\n",
    "    \n",
    "    mount_dir = \"/data/peiran/blob/msralaphilly2/ml-la\"\n",
    "    if not use_llama:\n",
    "        args.llm_model_name_or_path = mount_dir+\"/v-kehanwu/SFM/scigpt/stageB.prot/global_step224655\"\n",
    "        args.tokenizer_path = mount_dir+\"/shufxi/data/scigpt\"\n",
    "        args.save_dir = mount_dir+'/v-kehanwu/nlm/checkpoints/bfm_scigpt_prot/global_step11499'\n",
    "        # args.save_dir = \"/fastdata/peiran/nlm/checkpoints/stageB.prot/global_step1\"\n",
    "\n",
    "        tokenizer = SFMDecTokenizer.from_pretrained(\n",
    "            args.llm_model_name_or_path,\n",
    "            prot_spm_path=os.path.join(args.tokenizer_path, \"ur50bpe/bpe\"),\n",
    "            dna_spm_path=os.path.join(args.tokenizer_path, \"dnabpe/bpe\"),\n",
    "            rna_spm_path=os.path.join(args.tokenizer_path, \"rnabpe/bpe\"),\n",
    "        )\n",
    "        args.vocab_size = len(tokenizer)  # now we have new tokens\n",
    "        args.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        args.llm_model_name_or_path = mount_dir+\"/v-kehanwu/SFM/scigpt/stageB.prot/global_step224655\"\n",
    "        args.tokenizer_path = mount_dir+\"/shufxi/data/scigpt\"\n",
    "        args.save_dir = mount_dir+'/v-kehanwu/nlm/checkpoints/bfm_llama/global_step11499'\n",
    "        # args.save_dir = \"/fastdata/peiran/nlm/checkpoints/stageB.prot/global_step1\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            args.llm_model_name_or_path,\n",
    "            model_max_length=args.model_max_length,\n",
    "            padding_side=\"right\",\n",
    "            use_fast=False,\n",
    "        )\n",
    "\n",
    "        special_tokens_dict = dict()\n",
    "        if tokenizer.pad_token is None:\n",
    "            special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "        if tokenizer.eos_token is None:\n",
    "            special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "        if tokenizer.bos_token is None:\n",
    "            special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "        if tokenizer.unk_token is None:\n",
    "            special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "        special_tokens_dict[\"additional_special_tokens\"] = SCIENCE_TAG_TOKENS\n",
    "        tokenizer.add_special_tokens(special_tokens_dict)\n",
    "        \n",
    "\n",
    "    return args, tokenizer\n",
    "\n",
    "args, tokenizer = get_args_and_tokenizer(use_llama=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_dict = {}\n",
    "\n",
    "model = ProGPTModel(args, len(tokenizer))\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer0.items():\n",
    "    new_k = \"pfm_encoder.\" + k\n",
    "    ckpt_dict[new_k] = v\n",
    "\n",
    "layer1 = torch.load(os.path.join(args.save_dir, \"layer_01-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict['decoder.model.embed_tokens.weight'] = layer1['embed_tokens.weight']\n",
    "\n",
    "layer2 = torch.load(os.path.join(args.save_dir, \"layer_02-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer2.items():\n",
    "    new_k = \"adaptor.\" + k\n",
    "    ckpt_dict[new_k] = v\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 3).zfill(2)\n",
    "    layer = torch.load(os.path.join(args.save_dir, f\"layer_{l_index}-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        ckpt_dict[f\"decoder.model.layers.{l}.{k}\"] = layer[k]\n",
    "    del layer\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_35-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_36-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "\n",
    "model_dict.update(ckpt_dict)\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))\n",
    "model.load_state_dict(model_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scigpt_vacab = {'L': 33874, 'A': 33875, 'G': 33878, 'V': 33877, 'S': 33876, 'E': 33879, 'R': 33880, 'T': 33881, 'I': 33882, 'D': 33884, 'P': 33886, 'K': 33883, 'Q': 33885, 'N': 33887, 'F': 33888, 'Y': 33890, 'M': 33873, 'H': 33889, 'W': 33891, 'C': 33892, 'X': 34276, 'B': 37965, 'U': 37967, 'Z': 37966, 'O': 0}\n",
    "\n",
    "vocab = {'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7, 'S': 8, 'E': 9, 'R': 10, 'T': 11, 'I': 12, 'D': 13, 'P': 14, 'K': 15, 'Q': 16, 'N': 17, 'F': 18, 'Y': 19, 'M': 20, 'H': 21, 'W': 22, 'C': 23, 'X': 24, 'B': 25, 'U': 26, 'Z': 27, 'O': 28, '.': 29, '-': 30, '<mask>': 31}\n",
    "\n",
    "def protein_process(protein):\n",
    "    protein_id = [vocab[tok] for tok in protein]\n",
    "    protein_bpe_id = [scigpt_vacab[tok] for tok in protein]\n",
    "\n",
    "    return protein_id, protein_bpe_id\n",
    "\n",
    "def process(text):\n",
    "    # find the part of protein seq that surrounded by <protein> and </protein> in text\n",
    "    protein = []\n",
    "    res = []\n",
    "    text1 = text.split(\"<protein>\")\n",
    "    res.append(text1[0])\n",
    "    for i in range(1, len(text1)):\n",
    "        text2 = text1[i].split(\"</protein>\")\n",
    "        protein.append(text2[0])\n",
    "        res.append(text2[1])\n",
    "\n",
    "    return res, protein\n",
    "\n",
    "def tokenize(text):\n",
    "    # split text with <protein> and </protein>\n",
    "    text_list, protein = process(text)\n",
    "    protein_id_list = []\n",
    "    protein_bpe_id_list = []\n",
    "\n",
    "    if len(protein) == 0:\n",
    "        return tokenizer.encode(text), protein_id_list, protein_bpe_id_list\n",
    "    else:\n",
    "        for p in protein:\n",
    "            protein_id, protein_bpe_id = protein_process(p)\n",
    "            protein_id_list.append(protein_id)\n",
    "            protein_bpe_id_list.append(protein_bpe_id)\n",
    "\n",
    "    input_ids = []\n",
    "    for i in range(len(text_list)):\n",
    "        if i == 0:\n",
    "            input_ids.extend(tokenizer.encode(text_list[i] + \" <protein>\"))\n",
    "        elif i != len(text_list) - 1:\n",
    "            input_ids.append(-1)\n",
    "            input_ids.extend(tokenizer.encode(\"</protein> \" + text_list[i] + \" <protein>\")[1:])\n",
    "        else:\n",
    "            input_ids.append(-1)\n",
    "            input_ids.extend(tokenizer.encode(\"</protein> \" + text_list[i])[1:])\n",
    "\n",
    "    return input_ids, protein_id_list, protein_bpe_id_list\n",
    "\n",
    "\n",
    "def collator(input_ids, protein_id_list, protein_bpe_id_list, device):\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int64)\n",
    "    for i in range(len(protein_bpe_id_list)):\n",
    "        protein_bpe_id_list[i] = torch.tensor(protein_bpe_id_list[i], dtype=torch.int64)\n",
    "\n",
    "    new_input_ids = []\n",
    "    original_input_ids_len = len(input_ids)\n",
    "    input_ids_len = len(input_ids)\n",
    "    mol_pos = torch.nonzero(input_ids < 0).squeeze(-1)\n",
    "    mol_pos = torch.cat(\n",
    "        [torch.tensor([0]), mol_pos, torch.tensor([len(input_ids)])]\n",
    "    )\n",
    "\n",
    "    for i in range(mol_pos.size(0) - 1):\n",
    "        if i == 0:\n",
    "            new_input_ids.extend(input_ids[mol_pos[i] : mol_pos[i + 1]])\n",
    "        else:\n",
    "            new_input_ids.extend(input_ids[mol_pos[i] + 1 : mol_pos[i + 1]])\n",
    "\n",
    "        if i < len(mol_pos) - 2:\n",
    "            len_protein = len(protein_id_list[i])\n",
    "            mol_idx = input_ids[mol_pos[i + 1]]\n",
    "            if len_protein > 1:\n",
    "                new_input_ids.extend(torch.ones([len_protein]) * mol_idx)\n",
    "            if mol_pos[i + 1] < original_input_ids_len:\n",
    "                input_ids_len += len_protein - 1\n",
    "\n",
    "\n",
    "    input_ids = torch.tensor(new_input_ids).to(dtype=torch.int64).unsqueeze(0)\n",
    "    if len(protein_id_list) == 0:\n",
    "        protein = torch.tensor([0, 2]).to(dtype=torch.int64).unsqueeze(0)\n",
    "    else:\n",
    "        protein = torch.tensor(protein_id_list[0]).to(dtype=torch.int64)\n",
    "        protein = torch.cat([torch.tensor([0]).to(dtype=torch.int64), protein, torch.tensor([2]).to(dtype=torch.int64)]).unsqueeze(0)\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids.to(device),\n",
    "        proteins=protein.long().to(device),\n",
    "        llm_mask=input_ids.ne(tokenizer.pad_token_id).to(device),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "from sfm.data.prot_data.util import bstr2obj\n",
    "import pickle as pkl\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "data_path = '/fastdata/peiran/nlm/progpt_valid_bpe.lmdb/'\n",
    "env = lmdb.open(\n",
    "    str(data_path), subdir=True, readonly=True, lock=False, readahead=False\n",
    ")\n",
    "txn = env.begin(write=False)\n",
    "metadata = bstr2obj(txn.get(\"metadata\".encode()))\n",
    "size, keys = metadata[\"size\"], metadata[\"keys\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 364])\n"
     ]
    }
   ],
   "source": [
    "key = keys[-1]\n",
    "value = txn.get(str(key).encode())\n",
    "input_ids, proteins, proteins_bpeid = pkl.loads(value)\n",
    "\n",
    "batched_data = collator(input_ids, proteins, proteins_bpeid, device)\n",
    "\n",
    "print(f\"input_ids: {batched_data['input_ids'].shape}\")\n",
    "# batched_data['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 20355,   915,   278, 32007,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1, 32008,  1919,   508,   445, 26823,   883, 21762,  9869,\n",
       "          29973]], device='cuda:0'),\n",
       " 'proteins': tensor([[ 0,  5,  5,  5,  6,  8,  6,  5,  6,  5,  6,  8,  6, 19,  6, 26,  6, 19,\n",
       "           6, 26,  6, 19,  6, 26,  6, 19,  6, 26,  2]], device='cuda:0'),\n",
       " 'llm_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True, True]], device='cuda:0')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "text = \"Describe the <protein>AAAGSGAGAGSGYGUGYGUGYGUGYGU</protein>, can this protein form beta sheet?\"\n",
    "# text = \"Hello, what to eat tonight?\"\n",
    "\n",
    "input_ids, protein_id_list, protein_bpe_id_list = tokenize(text)\n",
    "# input_ids = tokenizer.encode(text)\n",
    "batched_data = collator(input_ids, protein_id_list, protein_bpe_id_list, device)\n",
    "\n",
    "batched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input_ids: torch.Size([1, 41])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Describe the <protein>AAAGSGAGAGSGYGUGYGUGYGUGYGU</protein>, can this protein form beta sheet?,\n",
      " To check this we take the sequence, get residue pattern for beta formation and compare it with fibril database ( http://www.tugra.mrc-lmb.cam.ac.uk/gf/fibril_red ). The results show that the protein has more residues of fibrils formation than globular proteins, which means this protein is more prone to form beta sheets. Thus this protein can form fibrils. Figure </FREETEXT> </SECTION>   <DISCUSSION> <FREETEXT> DISCUSSIONS  </FREETEXT> </DISCUSSION>   <SECTION> <FREETEXT> SARS-CoV-2 protein's interaction with ACE2 receptors  The <protein> <a>MA <a>PLR <a>PLL <a>IL <a>ALLA <a>WV <a>ALA <a>DQE <a>SC <a>KGR <a>CT <a>EGF <a>NVD <a>KK <a>CQ <a>CD <a>EL <a>CS <a>YY <a>QS <a>CC <a>TD <a>YTA <a>EC <a>KP <a>QVT <a>RGD <a>VFT <a>MP <a>EDE <a>YTV <a>YDD <a>GEE <a>KNN <a>ATV <a>HE <a>QV <a>GGP <a>SLT <a>SDL <a>QAQ <a>SK <a>GNP <a>EQ <a>TPV <a>LKP <a>EEE <a>APAP <a>EV <a>GAS <a>KPE <a>GID <a>SRP <a>ETL <a>HP <a>GR <a>PQP <a>PA <a>EEE <a>LCS <a>GKP <a>FDA <a>FTD <a>LKN <a>GS <a>LFA <a>FRG <a>QY <a>CY <a>ELD <a>EKA <a>VRP <a>GYP <a>KLI <a>RDV <a>WG <a>IEG <a>PI <a>DAA <a>FTR <a>IN <a>CQ <a>GKT <a>YLF <a>KGS <a>QY <a>WR <a>FED <a>GVL <a>DPD <a>YP <a>RNI <a>SDG <a>FDG <a>IPD <a>NVD <a>AALA <a>LPA <a>HS <a>YSG <a>RER <a>VY <a>FF <a>KGK <a>QY <a>W <a>EY <a>QF <a>QHQ <a>PSQ <a>EE <a>CE <a>GSS <a>LSA <a>VFE <a>HF <a>A <a>M <a>MQ <a>RDS <a>WE <a>DIF <a>ELL <a>FW <a>GRT <a>SAG <a>TR <a>QPQ <a>FIS <a>RD\n"
     ]
    }
   ],
   "source": [
    "model = model.to(torch.float16).to(device)\n",
    "print(f\"shape of input_ids: {batched_data['input_ids'].shape}\")\n",
    "model.eval()\n",
    "output = model.generate(\n",
    "    batched_data,\n",
    "    num_beams=4,\n",
    "    max_new_tokens=300,\n",
    "    num_return_sequences=1,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.5\n",
    ")\n",
    "res = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"input: {text},\\n output: {res}\")\n",
    "\n",
    "# # output = model.generate(\n",
    "# #     input_ids=batched_data['input_ids'],\n",
    "# #     num_return_sequences=10,\n",
    "# #     num_beams=20,\n",
    "# # )\n",
    "# for i in range(10):\n",
    "#     print(tokenizer.decode(output[i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
