{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-18 08:40:51,389] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[\u001b[32m2024-04-18 08:40:51.862\u001b[0m][\u001b[36mINFO\u001b[0m]: flash_attn not installed, use default attn\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import transformers\n",
    "from accelerate import init_empty_weights\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sfm.models.progpt.progpt import ProGPTModel\n",
    "from sfm.models.progpt.progpt_config import ProGPTConfig\n",
    "from sfm.models.pfm.pfm_config import PFMConfig\n",
    "from sfm.data.sci_data.SFMDecTokenizer import SFMDecTokenizer\n",
    "from sfm.utils import arg_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_and_tokenizer(use_llama=False):\n",
    "    parser = ArgumentParser()\n",
    "    cfg_classes = [PFMConfig, ProGPTConfig]\n",
    "    parser = arg_utils.add_dataclass_to_parser(cfg_classes, parser)\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.load_ckpt = False\n",
    "    args.strategy = \"DDP\"\n",
    "    args.encoder_layers = 33\n",
    "    args.encoder_embed_dim = 1280\n",
    "    args.encoder_ffn_embed_dim = 5120\n",
    "    args.encoder_attention_heads = 20\n",
    "    \n",
    "    \n",
    "    if not use_llama:\n",
    "        args.llm_model_name_or_path = \"/home/v-wukehan/blob1.v2/v-kehanwu/SFM/scigpt/stageB.prot/global_step224655\"\n",
    "        args.tokenizer_path = \"/home/v-wukehan/blob1.v2/shufxi/data/scigpt\"\n",
    "        args.save_dir = '/home/v-wukehan/blob1.v2/v-kehanwu/nlm/checkpoints/bfm_scigpt_prot/global_step11499'\n",
    "        tokenizer = SFMDecTokenizer.from_pretrained(\n",
    "            args.llm_model_name_or_path,\n",
    "            prot_spm_path=os.path.join(args.tokenizer_path, \"ur50bpe/bpe\"),\n",
    "            dna_spm_path=os.path.join(args.tokenizer_path, \"dnabpe/bpe\"),\n",
    "            rna_spm_path=os.path.join(args.tokenizer_path, \"rnabpe/bpe\"),\n",
    "        )\n",
    "        args.vocab_size = len(tokenizer)  # now we have new tokens\n",
    "        args.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        args.llm_model_name_or_path = \"/home/v-wukehan/blob1.v2/v-kehanwu/SFM/scigpt/stageB.prot/global_step224655\"\n",
    "        args.tokenizer_path = \"/home/v-wukehan/blob1.v2/shufxi/data/scigpt\"\n",
    "        args.save_dir = '/home/v-wukehan/blob1.v2/v-kehanwu/nlm/checkpoints/bfm_llama/global_step11499'\n",
    "\n",
    "    return args, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[32m2024-04-18 08:40:52.860\u001b[0m][\u001b[36mWARNING\u001b[0m]: Duplicate config name: train_data_path, not added to parser\n",
      "[\u001b[32m2024-04-18 08:40:52.860\u001b[0m][\u001b[36mWARNING\u001b[0m]: Duplicate config name: valid_data_path, not added to parser\n",
      "[\u001b[32m2024-04-18 08:40:54.539\u001b[0m][\u001b[36mINFO\u001b[0m]: Loading protein sentencepiece model from /home/v-wukehan/blob1.v2/shufxi/data/scigpt/ur50bpe/bpe.model and /home/v-wukehan/blob1.v2/shufxi/data/scigpt/ur50bpe/bpe.vocab\n",
      "[\u001b[32m2024-04-18 08:40:55.027\u001b[0m][\u001b[36mINFO\u001b[0m]: Loading DNA sentencepiece model from /home/v-wukehan/blob1.v2/shufxi/data/scigpt/dnabpe/bpe.model and /home/v-wukehan/blob1.v2/shufxi/data/scigpt/dnabpe/bpe.vocab\n",
      "[\u001b[32m2024-04-18 08:40:55.234\u001b[0m][\u001b[36mINFO\u001b[0m]: Loading RNA sentencepiece model from /home/v-wukehan/blob1.v2/shufxi/data/scigpt/rnabpe/bpe.model and /home/v-wukehan/blob1.v2/shufxi/data/scigpt/rnabpe/bpe.vocab\n",
      "[\u001b[32m2024-04-18 08:40:55.716\u001b[0m][\u001b[36mINFO\u001b[0m]: Tokenizer has 40014 tokens\n",
      "[\u001b[32m2024-04-18 09:01:34.523\u001b[0m][\u001b[36mINFO\u001b[0m]: Trainer args: Namespace(num_classes=1, encoder_attention_heads=20, encoder_ffn_embed_dim=5120, encoder_embed_dim=1280, encoder_layers=33, num_pred_attn_layer=4, num_3d_bias_kernel=128, max_length=1024, pbc_expanded_token_cutoff=512, pbc_expanded_num_cell_per_direction=10, multi_hop_max_dist=20, droppath_prob=0.0, act_dropout=0.0, attn_dropout=0.0, dropout=0.0, sandwich_ln=True, noise_scale=0.2, mask_ratio=0.5, d_tilde=1.0, pbc_cutoff=40.0, data_path='', dataset_names='', loadcheck_path='', add_3d=False, no_2d=False, ft=False, infer=False, use_pbc=False, transformer_m_pretrain=True, mode_prob='0.6,0.2,0.2', num_timesteps=1000, ddpm_beta_start=0.0001, ddpm_beta_end=0.02, ddpm_schedule='linear', noise_mode='const', num_edges=1536, num_atom_features=5120, task_name='', data_basepath='', output_dim=1024, add_rope=True, flash_attn=False, stack_seq=False, num_residues=32, max_num_aa=1024, task='mae', mask_prob=0.15, train_data_path='', valid_data_path='', dataset_splits='', dataset_ratios='', pool_mode='full', embedding_length=20, model_max_length=512, smiles_dict_path='', loadbfmckpt_path='', llm_model_name_or_path='/home/v-wukehan/blob1.v2/v-kehanwu/SFM/scigpt/stageB.prot/global_step224655', mol_size_path='', tokenizer_path='/home/v-wukehan/blob1.v2/shufxi/data/scigpt', mfm_lora=False, llm_lora=False, btn_adaptor=False, fused_graphormer_llama=False, add_mol_attn_bias_in_llama=False, mol_attn_bias_in_llama_layerwise=False, path_edge_cutoff=0, skip_num_datasets='', num_data_loading_workers=16, use_global_padding=False, max_num_mol_per_sample=8, protein_max_size=1024, use_llama_tokenizer=False, save_dir='/home/v-wukehan/blob1.v2/v-kehanwu/nlm/checkpoints/bfm_scigpt_prot/global_step10999', load_ckpt=False, strategy='DDP', vocab_size=40014, pad_token_id=32000, attention_dropout=0.0, activation_fn='gelu', encoder_normalize_before=True, apply_graphormer_init=True, share_encoder_input_output_embed=False, no_token_positional_embeddings=False, pre_layernorm=False, encoder_learned_pos=False, num_segments=2, sentence_class_num=2, sent_loss=False, apply_bert_init=False, pooler_activation_fn='tanh', atom_loss_coeff=1.0, pos_loss_coeff=1.0, y_2d_loss_coeff=1.0, max_positions=512, num_atoms=4608, num_in_degree=512, num_out_degree=512, num_spatial=512, num_edge_dis=128, edge_type='multi_hop', layerdrop=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'SFMDecTokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'sfm.data.sci_data.SFMDecTokenizer.SFMDecTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "args, tokenizer = get_args_and_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scigpt_vacab = {'L': 33874, 'A': 33875, 'G': 33878, 'V': 33877, 'S': 33876, 'E': 33879, 'R': 33880, 'T': 33881, 'I': 33882, 'D': 33884, 'P': 33886, 'K': 33883, 'Q': 33885, 'N': 33887, 'F': 33888, 'Y': 33890, 'M': 33873, 'H': 33889, 'W': 33891, 'C': 33892, 'X': 34276, 'B': 37965, 'U': 37967, 'Z': 37966, 'O': 0}\n",
    "\n",
    "vocab = {'<cls>': 0, '<pad>': 1, '<eos>': 2, '<unk>': 3, 'L': 4, 'A': 5, 'G': 6, 'V': 7, 'S': 8, 'E': 9, 'R': 10, 'T': 11, 'I': 12, 'D': 13, 'P': 14, 'K': 15, 'Q': 16, 'N': 17, 'F': 18, 'Y': 19, 'M': 20, 'H': 21, 'W': 22, 'C': 23, 'X': 24, 'B': 25, 'U': 26, 'Z': 27, 'O': 28, '.': 29, '-': 30, '<mask>': 31}\n",
    "\n",
    "def protein_process(protein):\n",
    "    protein_id = [vocab[tok] for tok in protein]\n",
    "    protein_bpe_id = [scigpt_vacab[tok] for tok in protein]\n",
    "\n",
    "    return protein_id, protein_bpe_id\n",
    "\n",
    "def process(text):\n",
    "    # find the part of protein seq that surrounded by <protein> and </protein> in text\n",
    "    protein = []\n",
    "    res = []\n",
    "    text1 = text.split(\"<protein>\")\n",
    "    res.append(text1[0])\n",
    "    for i in range(1, len(text1)):\n",
    "        text2 = text1[i].split(\"</protein>\")\n",
    "        protein.append(text2[0])\n",
    "        res.append(text2[1])\n",
    "\n",
    "    return res, protein\n",
    "\n",
    "def tokenize(text):\n",
    "    # split text with <protein> and </protein>\n",
    "    text_list, protein = process(text)\n",
    "    protein_id_list = []\n",
    "    protein_bpe_id_list = []\n",
    "\n",
    "    if len(protein) == 0:\n",
    "        return tokenizer.encode(text), protein_id_list, protein_bpe_id_list\n",
    "    else:\n",
    "        for p in protein:\n",
    "            protein_id, protein_bpe_id = protein_process(p)\n",
    "            protein_id_list.append(protein_id)\n",
    "            protein_bpe_id_list.append(protein_bpe_id)\n",
    "\n",
    "    input_ids = []\n",
    "    for i in range(len(text_list)):\n",
    "        if i == 0:\n",
    "            input_ids.extend(tokenizer.encode(text_list[i] + \" <protein>\"))\n",
    "        elif i != len(text_list) - 1:\n",
    "            input_ids.append(-1)\n",
    "            input_ids.extend(tokenizer.encode(\"</protein> \" + text_list[i] + \" <protein>\")[1:])\n",
    "        else:\n",
    "            input_ids.append(-1)\n",
    "            input_ids.extend(tokenizer.encode(\"</protein> \" + text_list[i])[1:])\n",
    "\n",
    "    return input_ids, protein_id_list, protein_bpe_id_list\n",
    "\n",
    "\n",
    "def collator(input_ids, protein_id_list, protein_bpe_id_list, device):\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int64)\n",
    "    for i in range(len(protein_bpe_id_list)):\n",
    "        protein_bpe_id_list[i] = torch.tensor(protein_bpe_id_list[i], dtype=torch.int64)\n",
    "\n",
    "    new_input_ids = []\n",
    "    original_input_ids_len = len(input_ids)\n",
    "    input_ids_len = len(input_ids)\n",
    "    mol_pos = torch.nonzero(input_ids < 0).squeeze(-1)\n",
    "    mol_pos = torch.cat(\n",
    "        [torch.tensor([0]), mol_pos, torch.tensor([len(input_ids)])]\n",
    "    )\n",
    "\n",
    "    for i in range(mol_pos.size(0) - 1):\n",
    "        if i == 0:\n",
    "            new_input_ids.extend(input_ids[mol_pos[i] : mol_pos[i + 1]])\n",
    "        else:\n",
    "            new_input_ids.extend(input_ids[mol_pos[i] + 1 : mol_pos[i + 1]])\n",
    "\n",
    "        if i < len(mol_pos) - 2:\n",
    "            len_protein = len(protein_id_list[i])\n",
    "            mol_idx = input_ids[mol_pos[i + 1]]\n",
    "            if len_protein > 1:\n",
    "                new_input_ids.extend(torch.ones([len_protein]) * mol_idx)\n",
    "            if mol_pos[i + 1] < original_input_ids_len:\n",
    "                input_ids_len += len_protein - 1\n",
    "\n",
    "\n",
    "    input_ids = torch.tensor(new_input_ids).to(dtype=torch.int64).unsqueeze(0)\n",
    "    if len(protein_id_list) == 0:\n",
    "        protein = torch.tensor([0, 2]).to(dtype=torch.int64).unsqueeze(0)\n",
    "    else:\n",
    "        protein = torch.tensor(protein_id_list[0]).to(dtype=torch.int64)\n",
    "        protein = torch.cat([torch.tensor([0]).to(dtype=torch.int64), protein, torch.tensor([2]).to(dtype=torch.int64)]).unsqueeze(0)\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids.to(device),\n",
    "        proteins=protein.long().to(device),\n",
    "        llm_mask=input_ids.ne(tokenizer.pad_token_id).to(device),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "text = \"Describe the <protein>AAAGSGAGU</protein> .\"\n",
    "input_ids, protein_id_list, protein_bpe_id_list = tokenize(text)\n",
    "batched_data = collator(input_ids, protein_id_list, protein_bpe_id_list, device)\n",
    "residue_seq = batched_data[\"proteins\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_dict = {}\n",
    "\n",
    "model = ProGPTModel(args, len(tokenizer))\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "layer0 = torch.load(os.path.join(args.save_dir, \"layer_00-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer0.items():\n",
    "    new_k = \"pfm_encoder.\" + k\n",
    "    ckpt_dict[new_k] = v\n",
    "\n",
    "layer1 = torch.load(os.path.join(args.save_dir, \"layer_01-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict['decoder.model.embed_tokens.weight'] = layer1['embed_tokens.weight']\n",
    "\n",
    "layer2 = torch.load(os.path.join(args.save_dir, \"layer_02-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "for k, v in layer2.items():\n",
    "    ckpt_dict[k] = v\n",
    "\n",
    "for l in range(0, 32):\n",
    "    l_index = str(l + 3).zfill(2)\n",
    "    layer = torch.load(os.path.join(args.save_dir, f\"layer_{l_index}-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "    for k in layer:\n",
    "        if \"dummy\" in k or 'rotary_emb' in k:\n",
    "            continue\n",
    "        ckpt_dict[f\"decoder.model.layers.{l}.{k}\"] = layer[k]\n",
    "    del layer\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_35-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.model.norm.weight\"] = layer[\"norm.weight\"]\n",
    "\n",
    "layer = torch.load(os.path.join(args.save_dir, \"layer_36-model_states.pt\"), map_location=torch.device(\"cpu\"))\n",
    "ckpt_dict[\"decoder.lm_head.weight\"] = layer[\"lm_head.weight\"]\n",
    "\n",
    "# model_dict.update(ckpt_dict)\n",
    "model.load_state_dict(model_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 20355,   915,   278, 32007,    -1,    -1,    -1,    -1,    -1,\n",
       "             -1,    -1,    -1,    -1, 32008,   869]], device='cuda:0'),\n",
       " 'proteins': tensor([[ 0,  5,  5,  5,  6,  8,  6,  5,  6, 26,  2]], device='cuda:0'),\n",
       " 'llm_mask': tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "          True, True, True, True]], device='cuda:0')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.eval()\n",
    "output = model.generate(\n",
    "    batched_data,\n",
    "    num_return_sequences=10,\n",
    "    num_beams=20,\n",
    ")\n",
    "for i in range(10):\n",
    "    print(tokenizer.decode(output[i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
